<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-14T19:15:27+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Newbee</title><subtitle>Don't worry, be happy.</subtitle><author><name>Zhipeng Wang</name><email>wangzhipenghyc@163.com</email></author><entry><title type="html">Dns</title><link href="http://localhost:4000/dns/" rel="alternate" type="text/html" title="Dns" /><published>2023-11-14T00:00:00+08:00</published><updated>2023-11-14T00:00:00+08:00</updated><id>http://localhost:4000/dns</id><content type="html" xml:base="http://localhost:4000/dns/">&lt;style type=&quot;text/css&quot;&gt;
  @import url(&quot;/assets/css/simple.css&quot;)
&lt;/style&gt;

&lt;h1 id=&quot;dns域名解析系统&quot;&gt;DNS–域名解析系统&lt;/h1&gt;

&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;

&lt;p&gt;DNS（domain name system） 是互联网的重要基础设施，它的作用很简单–把域名（domain name）解析为对应的 IP 地址。在互联网初期时网络主机有限，大家互相记住 IP 地址就好了，但是随着网络主机的增多，纯靠 IP 地址记录对用户不太友好，使用字符串来标识服务器地址更加方便记忆，于是主机名就诞生了。最开始主机名和IP地址之间的映射关系是通过一个简单的 host.txt 文件维护，每天晚上所有主机都从一个维护此文件的站点取回 host.txt。然而随着主机数量不断的增加，这个方案就愈发显得简陋且脆弱：首先这个文件会变得越来越大，其次主机名称冲突会越来越频繁。&lt;/p&gt;

&lt;p&gt;为了解决这一问题，DNS 被发明了出来，在 RFC 1034、1035、2181 中给出了 DNS 的定义，后来又有其他文档对其进行阐述。但无论如何发展，它的核心功能没有变化：将域名转化为 ip 地址。&lt;/p&gt;

&lt;h2 id=&quot;dns-名字空间&quot;&gt;DNS 名字空间&lt;/h2&gt;

&lt;p&gt;每个城市都有一些常见的街道名如：春风路、南京路等等，如果只用街道名是无法区别的，但是前面加上所属行政区域和城市名称，就可以确定具体地址。域名就类似邮政系统中的地址，它是分层次的，在域基础上去分割子域名，在同一个域下每个子域名要保证唯一性，不同域下的子域名可以重复。对于 internet ，命名层次的顶级由一个专门的组织负责管理– ICANN (Internet Corporation for Assigned Names and Numbers, Internet 名字与数字地址分配机构)，以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.baidu.com&lt;/code&gt; 为例，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;com&lt;/code&gt; 就是最顶级的域名。&lt;/p&gt;

&lt;p&gt;顶级域名（TOP-Level）分为两种类型：通用的和国家或地区的，常见的通用域名如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;com: 企业、商业，这是我们最常见的顶级域名；&lt;/li&gt;
  &lt;li&gt;edu: 教育，各个高校站点常用的域名；&lt;/li&gt;
  &lt;li&gt;gov: 政府机关常用的域名；&lt;/li&gt;
  &lt;li&gt;mil: 军事机构常用的域名；&lt;/li&gt;
  &lt;li&gt;int: 国际组织常用的域名；&lt;/li&gt;
  &lt;li&gt;net: 网络提供商；&lt;/li&gt;
  &lt;li&gt;org: 非盈利组织；&lt;/li&gt;
  &lt;li&gt;aero: 航空运输；&lt;/li&gt;
  &lt;li&gt;biz: 公司；&lt;/li&gt;
  &lt;li&gt;coop: 合作；&lt;/li&gt;
  &lt;li&gt;info: 信息；&lt;/li&gt;
  &lt;li&gt;museum: 博物馆；&lt;/li&gt;
  &lt;li&gt;name: 人；&lt;/li&gt;
  &lt;li&gt;pro: 专业；&lt;/li&gt;
  &lt;li&gt;cat: 代表加泰罗尼亚语言的网站;&lt;/li&gt;
  &lt;li&gt;jobs: 就业；&lt;/li&gt;
  &lt;li&gt;mobi: 移动设备；&lt;/li&gt;
  &lt;li&gt;tel: 联络资料；&lt;/li&gt;
  &lt;li&gt;travel: 旅游业；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上并非所有的通用顶级域名，可以看出来包含面比较广，包括常见的各行各业，实际中最常见的是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;com&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edu&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gov&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;net&lt;/code&gt;。国家或地区的域名则可以很直观的反应站点所属的地域，在中文互联网比较常见：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cn: 中国&lt;/li&gt;
  &lt;li&gt;us: 美国&lt;/li&gt;
  &lt;li&gt;eu: 欧洲&lt;/li&gt;
  &lt;li&gt;jp: 日本&lt;/li&gt;
  &lt;li&gt;hk: 香港，表示所属地区&lt;/li&gt;
  &lt;li&gt;tw: 台湾，表示所属地区&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上两者也可以组合使用，通常是在国家或地区的域下划分，例如：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.com.cn&lt;/code&gt;，直观的表示为中国地区的商业网点。&lt;/p&gt;

&lt;h2 id=&quot;域名的资源记录&quot;&gt;域名的资源记录&lt;/h2&gt;

&lt;p&gt;无论是只有一台主机的域还是顶级域，每个域都有一组与它相关联的资源记录（resource record），这些记录组成了 DNS 数据库。一条资源记录有一个五元组构成：&lt;/p&gt;

&lt;div class=&quot;language-mkd highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Domain_name Time_to_live Class Type Value
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Domain_name: 域名，是搜索的主要关键字&lt;/li&gt;
  &lt;li&gt;Time_to_live: 该条记录的有效周期，比较稳定的记录会分配一个长周期（如 86400s, 即一天），不稳定的记录则会分配一个小周期（如 60s）&lt;/li&gt;
  &lt;li&gt;Class: 对于 Internet 信息，它总是 IN。其他 class 还有:
    &lt;ul&gt;
      &lt;li&gt;CH（Chaosnet）：该类别今天很少使用，但最初是为 Chaosnet 设计的网络协议。&lt;/li&gt;
      &lt;li&gt;HS（Hesiod）：该类别今天也很少使用，但最初是为 Hesiod 命名系统设计的，该命名系统在某些类 Unix 的系统上使用。&lt;/li&gt;
      &lt;li&gt;ANY：此类别用作通配符，匹配任何类别记录。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Type: 指出资源记录的类型。下面列出一些主要的 DNS 资源记录类型：
    &lt;ul&gt;
      &lt;li&gt;SOA&lt;/li&gt;
      &lt;li&gt;A：主机的 IPV4 地址&lt;/li&gt;
      &lt;li&gt;AAAA: 主机的 IPV6 地址&lt;/li&gt;
      &lt;li&gt;MX： 邮件交换，&lt;/li&gt;
      &lt;li&gt;NS： 域名服务器，本域的服务器名字&lt;/li&gt;
      &lt;li&gt;CNAME：规范名&lt;/li&gt;
      &lt;li&gt;PTR：指针，IP 地址的别名&lt;/li&gt;
      &lt;li&gt;SPF: 发送者的政策框架&lt;/li&gt;
      &lt;li&gt;SRV: 服务，提供服务的主机名&lt;/li&gt;
      &lt;li&gt;TXT: 文本，说明的 ASCII 文本&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面结合一个DNS数据库记录的示例，说明各个字段的含义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-DNS&quot;&gt;  ; cs.vu.nl的授权数据
  cs.vu.nl  86 400 IN SOA   star boss(9527, 7200, 7200, 241 920, 86 400)
  cs.vu.nl. 86 400 IN MX    1 zephyr
  cs.vu.nl. 86 400 IN MX    2 top
  cs.vu.nl. 86 400 IN NS    star
  
  star      86 400 IN A     130.27.56.205
  zephyr    86 400 IN A     130.37.20.10
  top       86 400 IN A     130.37.20.11
  www       86 400 IN CNAME star.cs.vu.nl
  ftp       86 400 IN CNAME zephyr.cs.vu.nl
  flits     86 400 IN A     130.37.16.112
  flits     86 400 IN A     192.31.231.165
  flits     86 400 IN MX    1 flits
  flits     86 400 IN MX    2 zephyr
  flits     86 400 IN MX    3 top
  
  rowboat          IN A     130.37.56.201
                   IN MX    1 rowboat
                   IN MX    2 zephyr
  
  little-sister    IN A     130.37.62.23
  
  laserjet         IN A     192.31.231.216                
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个非注释行 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SOA&lt;/code&gt; 给出了一些该域的基本信息。2~3 给出了的记录时 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MX&lt;/code&gt; 类型，按照上文所述这应该是一个邮箱类型的记录，它指明 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;personal@cs.vu.nl&lt;/code&gt; 邮箱可以投递给指定机器 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zephyr&lt;/code&gt; ，如果失败了则尝试投递给机器 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top&lt;/code&gt;。第 4 行指明 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cs.vu.nl&lt;/code&gt; 域下的名字服务器是主机 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;第 5~7 行都是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; 类地址，分别指明了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zephyr&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top&lt;/code&gt; 主机 ipv4 地址。第 8~9 行是两个别名，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.cs.vu.nl&lt;/code&gt; 对应 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;star.cs.vu.nl&lt;/code&gt;，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ftp.cs.vu.nl&lt;/code&gt; 对应 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zephyr.cs.vu.nl&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;第 10~11 行指定了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flits&lt;/code&gt; 的 主机 ipv4 地址，可以看到该域名绑定了两个物理主机地址。第 12~14 行，指定了向该域发送邮件的投递顺序，首先是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flits&lt;/code&gt; 本身，如果失效则依次尝试 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zephyr&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;第 15~17 行指定了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rowboat&lt;/code&gt; 的 主机 ipv4 地址，和该域邮件投递地址，与 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flits&lt;/code&gt; 类似。第 18~19 行则是指定了两个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; 类记录。&lt;/p&gt;

&lt;p&gt;我们通过上述记录可以看到，一个域名可以有多种类型的多条记录，一个域名同类型的记录可以有多条，例如：多个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; 地址，表明该域名绑定了多个主机；多个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MX&lt;/code&gt; 地址，表明该域下有多个邮件投递的选项，可以按照指定的优先顺序进行投递。&lt;/p&gt;

&lt;h2 id=&quot;域名服务&quot;&gt;域名服务&lt;/h2&gt;

&lt;p&gt;我们现在使用 nslookup 命令模拟一下域名解析的过程，nslookup 命令有两种模式：交互模式(Interactive mode) 和 非交互模式(non-interactive.)。为了模拟整个解析流程，我们使用交互模式来查询 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.baidu.com&lt;/code&gt; 的 ip 地址：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;默认 name server 切换为 root name server，查询 com 域名服务器：&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; nslookup
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; server a.root-servers.net
 Default server: a.root-servers.net
 Address: 198.41.0.4#53
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;all
 Default server: a.root-servers.net
 Address: 198.41.0.4#53

 Set options:
   novc			nodebug		nod2
   search		recurse
   &lt;span class=&quot;nb&quot;&gt;timeout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0		retry &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 3	port &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 53	ndots &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1
   querytype &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; A       	class &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; IN
   srchlist &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; PZ-L8

 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ns

 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; com.
 &lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt; Truncated, retrying &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;TCP mode.
 Server:		a.root-servers.net
 Address:	198.41.0.4#53

 Non-authoritative answer:
 &lt;span class=&quot;k&quot;&gt;***&lt;/span&gt; Can&lt;span class=&quot;s1&quot;&gt;'t find com.: No answer

 Authoritative answers can be found from:
 com	nameserver = e.gtld-servers.net.
 com	nameserver = b.gtld-servers.net.
 com	nameserver = j.gtld-servers.net.
 com	nameserver = m.gtld-servers.net.
 com	nameserver = i.gtld-servers.net.
 com	nameserver = f.gtld-servers.net.
 com	nameserver = a.gtld-servers.net.
 com	nameserver = g.gtld-servers.net.
 com	nameserver = h.gtld-servers.net.
 com	nameserver = l.gtld-servers.net.
 com	nameserver = k.gtld-servers.net.
 com	nameserver = c.gtld-servers.net.
 com	nameserver = d.gtld-servers.net.
 e.gtld-servers.net	internet address = 192.12.94.30
 e.gtld-servers.net	has AAAA address 2001:502:1ca1::30
 b.gtld-servers.net	internet address = 192.33.14.30
 b.gtld-servers.net	has AAAA address 2001:503:231d::2:30
 j.gtld-servers.net	internet address = 192.48.79.30
 j.gtld-servers.net	has AAAA address 2001:502:7094::30
 m.gtld-servers.net	internet address = 192.55.83.30
 m.gtld-servers.net	has AAAA address 2001:501:b1f9::30
 i.gtld-servers.net	internet address = 192.43.172.30
 i.gtld-servers.net	has AAAA address 2001:503:39c1::30
 f.gtld-servers.net	internet address = 192.35.51.30
 f.gtld-servers.net	has AAAA address 2001:503:d414::30
 a.gtld-servers.net	internet address = 192.5.6.30
 a.gtld-servers.net	has AAAA address 2001:503:a83e::2:30
 g.gtld-servers.net	internet address = 192.42.93.30
 g.gtld-servers.net	has AAAA address 2001:503:eea3::30
 h.gtld-servers.net	internet address = 192.54.112.30
 h.gtld-servers.net	has AAAA address 2001:502:8cc::30
 l.gtld-servers.net	internet address = 192.41.162.30
 l.gtld-servers.net	has AAAA address 2001:500:d937::30
 k.gtld-servers.net	internet address = 192.52.178.30
 k.gtld-servers.net	has AAAA address 2001:503:d2d::30
 c.gtld-servers.net	internet address = 192.26.92.30
 c.gtld-servers.net	has AAAA address 2001:503:83eb::30
 d.gtld-servers.net	internet address = 192.31.80.30
 d.gtld-servers.net	has AAAA address 2001:500:856e::30
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;默认 name server 切换到 com 的 name server，查询 baidu.com 域名服务器:&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; server j.gtld-servers.net
 Default server: j.gtld-servers.net
 Address: 192.48.79.30#53
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;all
 Default server: j.gtld-servers.net
 Address: 192.48.79.30#53

 Set options:
   novc			nodebug		nod2
   search		recurse
   &lt;span class=&quot;nb&quot;&gt;timeout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0		retry &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 3	port &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 53	ndots &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1
   querytype &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; ns      	class &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; IN
   srchlist &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; PZ-L8
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; baidu.com.
 Server:		j.gtld-servers.net
 Address:	192.48.79.30#53

 Non-authoritative answer:
 &lt;span class=&quot;k&quot;&gt;***&lt;/span&gt; Can&lt;span class=&quot;s1&quot;&gt;'t find baidu.com.: No answer

 Authoritative answers can be found from:
 baidu.com	nameserver = ns2.baidu.com.
 baidu.com	nameserver = ns3.baidu.com.
 baidu.com	nameserver = ns4.baidu.com.
 baidu.com	nameserver = ns1.baidu.com.
 baidu.com	nameserver = ns7.baidu.com.
 ns2.baidu.com	internet address = 220.181.33.31
 ns3.baidu.com	internet address = 112.80.248.64
 ns3.baidu.com	internet address = 36.152.45.193
 ns4.baidu.com	internet address = 111.45.3.226
 ns4.baidu.com	internet address = 14.215.178.80
 ns1.baidu.com	internet address = 110.242.68.134
 ns7.baidu.com	internet address = 180.76.76.92
 ns7.baidu.com	has AAAA address 240e:940:603:4:0:ff:b01b:589a
 ns7.baidu.com	has AAAA address 240e:bf:b801:1002:0:ff:b024:26de
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;默认 name server 切换到  baidu.com 的 name server，查询 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.baidu.com&lt;/code&gt; 的ip地址:&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; server ns3.baidu.com
 Default server: ns3.baidu.com
 Address: 36.152.45.193#53
 Default server: ns3.baidu.com
 Address: 112.80.248.64#53
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; www.baidu.com
 Server:		ns3.baidu.com
 Address:	36.152.45.193#53

 www.baidu.com	canonical name &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; www.a.shifen.com.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;此处发现 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.baidu.com&lt;/code&gt; 的规范名称为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.a.shifen.com&lt;/code&gt;，这才是我们真正要解析的域名。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;解析 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.a.shifen.com&lt;/code&gt; 的 ip 地址:&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; www.a.shifen.com.
 Server:		ns3.baidu.com
 Address:	36.152.45.193#53

 Non-authoritative answer:
 &lt;span class=&quot;k&quot;&gt;***&lt;/span&gt; Can&lt;span class=&quot;s1&quot;&gt;'t find www.a.shifen.com.: No answer

 Authoritative answers can be found from:
 a.shifen.com	nameserver = ns2.a.shifen.com.
 a.shifen.com	nameserver = ns3.a.shifen.com.
 a.shifen.com	nameserver = ns4.a.shifen.com.
 a.shifen.com	nameserver = ns5.a.shifen.com.
 a.shifen.com	nameserver = ns1.a.shifen.com.
 ns5.a.shifen.com	internet address = 180.76.76.95
 ns4.a.shifen.com	internet address = 14.215.177.229
 ns4.a.shifen.com	internet address = 111.20.4.28
 ns3.a.shifen.com	internet address = 36.152.45.198
 ns3.a.shifen.com	internet address = 112.80.255.253
 ns2.a.shifen.com	internet address = 220.181.33.32
 ns1.a.shifen.com	internet address = 110.242.68.42
 ns5.a.shifen.com	has AAAA address 240e:bf:b801:1006:0:ff:b04f:346b
 ns5.a.shifen.com	has AAAA address 240e:940:603:a:0:ff:b08d:239d
 &amp;gt; server ns5.a.shifen.com
 Default server: ns5.a.shifen.com
 Address: 180.76.76.95#53
 &amp;gt; set type=a
 &amp;gt; www.a.shifen.com
 Server:		ns5.a.shifen.com
 Address:	180.76.76.95#53

 Name:	www.a.shifen.com
 Address: 120.232.145.144
 Name:	www.a.shifen.com
 Address: 120.232.145.185
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;最后得出的 ipv4 地址结果是：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;120.232.145.144&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;120.232.145.185&lt;/code&gt;，我们使用非交互式的查询方式验证一下：&lt;/p&gt;

    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; nslookup www.baidu.com
 Server:		10.10.0.1
 Address:	10.10.0.1#53

 Non-authoritative answer:
 www.baidu.com	canonical name &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; www.a.shifen.com.
 Name:	www.a.shifen.com
 Address: 120.232.145.185
 Name:	www.a.shifen.com
 Address: 120.232.145.144
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;细心的人会发现，每一次查询 主机地址 或者 域名服务器，总会有两种类型的返回值：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Non-authoritative answer: 非权威性结果，它是 DNS 服务器中的缓存，为了加速查询；&lt;/li&gt;
  &lt;li&gt;Authoritative answer: 权威结果，这是由管理该服务器的权威部门提供的；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们最后验证答案的时候就是命中的本地域名服务器的 DNS 缓存。&lt;/p&gt;

&lt;p&gt;同时我们可以发现每个域下会有很多个 name server 记录，这是为了 dns 冗余设计的，允许每个域关联多个域名服务器，通常是设置一个主域名服务器和多个辅助域名服务器作为冗余备份。&lt;/p&gt;

&lt;p&gt;上述的例子有一点点特殊，它最后在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ns3.baidu.com&lt;/code&gt; 中发现其实 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.baidu.com&lt;/code&gt; 是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.a.shifen.com&lt;/code&gt; 的别名。正常的如果记录是 A 类型则可以直接返回 主机地址了。&lt;/p&gt;

&lt;p&gt;准确的讲，我们整个流程不是模仿客户端解析域名的过程，而是&lt;strong&gt;模仿本地域名服务器的解析域名过程&lt;/strong&gt;，完整的域名解析工作流程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dns/nslookup_dns_flow.svg&quot; alt=&quot;nslookup DNS flow&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a.root-servers.com: 根域名服务器，全球总共有 13 个根域名服务器（每个服务器对应多个计算机），命名从 a.root-server.com-m.root-server.com；&lt;/li&gt;
  &lt;li&gt;a.edu-server.net: TOP-Level 域名服务器，顶级域名的域名服务器；&lt;/li&gt;
  &lt;li&gt;marge.cac.washington.edu: 权威域名服务器；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如上图所示，描述了解析 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cs.washington.edu&lt;/code&gt; 的 ip 地址的整个流程，这里涉及如下几个概念：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;递归查询：当主机将 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cs.washington.edu&lt;/code&gt; 发送给本地域名服务器后，本地域名服务器就代替该主机处理域名解析工作，直到返回它所需的答案。这个答案必须是完整的，不能返回部分答案；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;迭代查询：根域名服务器（以及接下来的每一个域名服务器）并不是递归查询，而是只返回一个部分答案给本地域名服务器，本地域名移动到下一个 name server 继续剩余的查询；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缓存技术：所有的查询答案，包括部分答案都会被本地DNS缓存，这样下一个主机再查询 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cs.washington.edu&lt;/code&gt; 时可以直接返回 ip 地址；同时，当主机后续查询 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;phys.washington.edu&lt;/code&gt; 也可以直接发送给权威的域名服务器（washington.edu 域名服务器），直接从第 6 步开始执行；&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还记得我们上文提到的每一个 dns 记录都有一个 ttl 字段，对于那些不常变动的常用记录可以设置 ttl 为一天，对于那些高频变动的设置为几秒或者一分钟即可。在实际生产中，有很多企业的同城容灾系统，就是通过修改域名绑定的 ip 地址实现流量切换，但是由于缓存更新需要一定的时间（ttl过后才会失效），所以切换的速度通常在分钟级别。&lt;/p&gt;

&lt;p&gt;DNS 中常用 UDP 协议进行通信，报文格式简单，只有查询和响应，没有握手流程。如果很短时间内没有响应，DNS 客户端必须重复查询，在重试指定次数无效后，则去尝试域内的其他域名服务器，以此来保证高可用。每个查询携带 16 位的标识符，这些标识符会被添加到应道报文中，以此来匹配查询，避免查询结果混乱。&lt;/p&gt;

&lt;h2 id=&quot;gethostbyname&quot;&gt;gethostbyname&lt;/h2&gt;

&lt;p&gt;在实际工程中会调用系统或库提供的函数进行域名解析，以 C 语言为例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;netdb.h&amp;gt;
#include &amp;lt;arpa/inet.h&amp;gt;

int main() {
    struct hostent *he;
    char *hostname = &quot;www.baidu.com&quot;;
    he = gethostbyname(hostname);
    if (he == NULL) {
        printf(&quot;Couldn't resolve hostname\n&quot;);
        return 1;
    }
    printf(&quot;Hostname: %s\n&quot;, hostname);
    printf(&quot;IP address: %s\n&quot;, inet_ntoa(*((struct in_addr*)he-&amp;gt;h_addr)));
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译执行的结果如下：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gcc gethost.c &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; gethost &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ./gethost
Hostname: www.baidu.com
IP address: 120.232.145.144
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;它整个工作流程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;应用程序发起 DNS 查询请求；&lt;/li&gt;
  &lt;li&gt;操作系统内核首先检查本地缓存是否有该域名的 IP 地址信息；&lt;/li&gt;
  &lt;li&gt;如果本地缓存中不存在相应记录，则按照 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/hosts&lt;/code&gt; 中指定的顺序逐行读取其中的内容；&lt;/li&gt;
  &lt;li&gt;对于 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/hosts&lt;/code&gt; 中的每一行，如果包含了要查询的域名，则取出对应的 IP 地址并返回给应用程序；&lt;/li&gt;
  &lt;li&gt;如果 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/hosts&lt;/code&gt; 中没有匹配到相应的域名，则按照 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/resolv.conf&lt;/code&gt; 中指定的顺序依次向其中的 DNS 服务器发送查询请求；&lt;/li&gt;
  &lt;li&gt;如果某个 DNS 服务器返回了相应的解析结果，则操作系统内核将该结果保存在本地缓存中，并返回给应用程序；&lt;/li&gt;
  &lt;li&gt;如果所有 DNS 服务器都无法响应，则返回一个错误码给应用程序。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此，当使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gethostbyname()&lt;/code&gt; 函数进行 DNS 解析时，除了使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/resolv.conf&lt;/code&gt; 文件中配置的 DNS 服务器地址来解析域名，也会自动利用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/hosts&lt;/code&gt; 文件中的配置来解析域名。&lt;/p&gt;

&lt;h2 id=&quot;参考链接&quot;&gt;参考链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.rfc-editor.org/rfc/rfc1035&quot;&gt;DOMAIN NAMES - IMPLEMENTATION AND SPECIFICATION&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;《计算机网络（第五版）》. 7.1 节. DNS–域名系统&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zhipeng Wang</name><email>wangzhipenghyc@163.com</email></author><summary type="html"></summary></entry><entry><title type="html">Set_benchmark_analyse</title><link href="http://localhost:4000/set_benchmark_analyse/" rel="alternate" type="text/html" title="Set_benchmark_analyse" /><published>2022-05-10T00:00:00+08:00</published><updated>2022-05-10T00:00:00+08:00</updated><id>http://localhost:4000/set_benchmark_analyse</id><content type="html" xml:base="http://localhost:4000/set_benchmark_analyse/">&lt;style&gt;
  @import url(&quot;/assets/css/simple.css&quot;)
&lt;/style&gt;

# 记一次测试分析(开启 pipeline 时 slave 对 SET 耗时影响)

## 背景介绍

在工作过程中，测试同事反馈在如下 case 中 redis 的 `get` 指令性能要明显优于 `set` 指令性能：

``` shell
    redis-benchmark -h $IP -p $PORT -a $PASSWD -r 4000000 -n 5000000 -t set,get -d 500 -c 500 -P 16
    ......
    78143.31 requests per second // set
    ......
    175389.38 requests per second // get
```

在不指定 “-P” 时 `get` 的性能也要优于 `set`，但并不是如此明显：

``` shell
    redis-benchmark -h $IP -p $PORT -a $PASSWD -r 4000000 -n 5000000 -t set,get -d 500 -c 500 
    ... ...
    43798.95 requests per second // set
    ... ...
    63569.56 requests per second // get
```

下面进行简要分析，client 发送给 redis 的一次请求可以拆分为以下几个阶段：

1. T1: client 发送命令阶段；
2. T2: 物理层网络报文传输延迟；
3. T3: redis server 接收命令阶段；
4. **T4**: redis server 处理命令阶段；
5. T5: redis server 发送 reply 阶段；
6. T6: 物理层网络报文传输延迟；
7. T7: client 接收 reply 阶段；

tps 的计算公式如下：

```
    T = T1 + T2 + T3 + T4 + T5 + T6 + T7
    tps = requests/T
```

在并发客户端数量相同的情况下，指定 `-P 16` 的影响如下：

1. 减少 syscall 的调用数量: 在 redis server 内部实现中调用 [writeToClient](https://github.com/redis/redis/blob/5.0/src/networking.c#L979) 去向对端发送数据，会一次性将缓存区内所有数据调用 `write(2)` 函数写入 `socket`，在 server 内存充足（没有超过最大内存限制 或 未限制最大内存）的情况下，当发送超过 64 KB 数据后会停止发送（如果客户端是 slave则忽略此限制）；在 redis-benchmark 中也是一次性将调用`write(2)`函数写入所有数据，具体的处理过程见**延伸阅读**部分；

2. 提升网络利用率: redis server 与 redis-benchmark 中都关闭了 socket 的 `Nagle` 算法，对小包的传输性能会提升，但是会降低网络利用率。pipeline 将多个命令“合并”发送，可以大幅减少报文数量，减少应答次数，提高网络利用率；

但以上两点影响的是 T1、T3、T5、T7 阶段，T2与T6阶段为物理环境所决定，redis-benchmark 宿主机与 proxy 宿主机、数据库宿主机属于同一网段，网络延迟较低：

``` 
    # redis-benchmark 主机发起 ping
    ping -c 100 $proxy_host
    ... ...
    rtt min/avg/max/mdev = 0.058/0.092/0.172/0.035

    ping -c 100 $db_host
    ... ...
    rtt min/avg/max/mdev = 0.057/0.089/0.178/0.022
```

对于 **T4 阶段**(`redis server` 执行 `set` 和 `get` 命令的耗时)并不会因此而减少，因为总的请求数量不变。通过 redis server 内部的 statistics 信息，可以查看 `set` 与 `get` 命令执行的耗时差距：

```
    &gt; INFO commandstats
    cmdstat_set:calls=425583765,usec=1029939666,usec_per_call=2.42
    cmdstat_get:calls=438697614,usec=840824546,usec_per_call=1.92
```

可以看出 `get` 命令在执行时要明显优于 `set` 命令，但仍未达到测试结果的差距，比较接近不加 `-P` 时的性能差距：

```
    # 非 -P 情况
    set_tps/ get_tps = 43798.95/63569.56 = 0.689
    get_usec_per_call/set_usec_per_call= 1.92/2.42 = 0.79

    # -P 情况
    set_pipeline_tps/get_pipeline_tps = 78143.31/175389.38 = 0.446
```

通过上述的对比，明显可以发现在**T4 阶段**：除去执行命令本身， `get` 命令 case 其他过程要优于 `set` 命令 case。查看 redis server 的源代码可以发现，**在执行 `set` 命令时或其他会修改 `data set` 的命令时，需要将变化传递给 `slave` 和 `aof`[^1]，因此在执行过程中会拖慢 `processCommand` 的速度**。

为此进行了如下测试，对比在 `pipeline` 和 `slave` 两个影响因素下的 tps，结果如下：

```
    # 不开启 slave 时
    # 开启 pipeline
    get: 177405.62 request/s
    set: 118846.71 request/s

    # 不开启 pipeline
    get: 61101.54 request/s
    set: 46511.63 request/s

    # 开启 salve 时
    # 开启 pipeline
    get: 170520.42 request/s
    set: 81866.55 request/s

    # 不开启 pipeline
    get: 61282.02 request/s
    set: 42770.14 request/s

    # 测试指令
    pipeline: redis-benchmark -h $IP -p $PORT -a $PASSWD -r 4000000 -n 5000000 -t set,get -d 500 -c 500 -P 16
    no pipeline: redis-benchmark -h $IP -p $PORT -a $PASSWD -r 4000000 -n 5000000 -t set,get -d 500 -c 500
```

在不开启 slave 的场景（以下简称 no-slave）下：

+ 开启 pipeline 时的读写效率比 `set_rps/get_rps=0.67`；
+ 不开启 pipeline 时的读写效率比 `set_rps/get_rps=0.76`；

在开启 slave 的场景（以下简称 has-slave）下：

+ 开启 pipeline 时的读写效率比 `set_rps/get_rps=0.48`；
+ 不开启 pipeline 时的读写效率比 `set_rps/get_rps=0.69`；

可以清楚的发现在 no-slave 场景，是否开启 pipeline 对读写效率比无较大影响，在*has-slave 场景下，pipeline 对读写效率比影响较大*。对 set 命令 tps 进行横向对比可以发现：**在开启 pipeline 场景下，has-slave 时的 tps 较 no-slave 时下降 30.11%**，这是造成读写效率比下降的最重要原因。

## 根因分析

整个执行期间的时间开销可以分为两部分：on-cpu 和 off-cpu，其中：

+ on-cpu 部分：cpu 执行服务进程处理 command 的耗时，包括内核态、用户态；
+ off-cpu 部分：可以大致分两部分：1、主动触发：syscall 或 锁操作等 导致的 block、io-wait 的时间开销；2、被动触发：系统多任务抢占(例如：时间片耗尽)导致换入、换出 cpu 的延时开销，此外在多核 cpu 的环境下有可能发生 task migrate；

向 `slave` 发送数据在 `beforeSleep` 和 `eventLoop` 阶段完成，向 `aof` 刷写数据都是在 `beforeSleep` 或 `serverCron` 阶段中完成。在整个 `processCommand` 过程中除申请内存外无其他 syscall 调用，线上环境的 redis server 使用 `jemalloc` 进行管理内存。

### on-cpu 分析

redis server 的单事件处理线程模式大大方便了 profile 分析，此处使用 `perf` 工具进行 cpu profile 收集和分析。
has-slave 的结果如下(tps 73606.27)：

``` shell
    ## 此处指定 -t 标识只采集 thread id，避免 redis server 中其他线程的干扰。
    perf record --call-graph dwarf -t $redis_server_pid -- sleep 10
    Samples: 38K of event 'cycles', Event count(approx.): 17854460442
    Children Self  Command       Shared Object Symbol
    - 82.65% 0.00% redis-server  redis-server  [.] aeMain
     - aeMain
        - 80.60% aeProcessEvents
           - 78.69% processInputBuffer
              - 64.70% processCommand
                 - 62.82% call
                    - 24.54% replicationFeedSlaves
                       + 11.40% addReplyBulk
                       + 6.66% addReply
                       + 2.41% feeaReplicationBacklog
                       + 1.82% feedReplicationBacklogWithObject
                         0.50% stringObjectLen
                    + 21.86% setCommand
                    + 16.05% propagate
                 + 1.04% dictFetchValue
              + 12.53% processMultibulkBuffer
              + 1.09% resetClient
           + 1.15% writeToClient
             0.53% readQueryFromClient 
        + 2.04% beforeSleep
```

no-slave 的结果如下(tps 102724.5)：

``` shell
    perf record --call-graph dwarf -t $redis_server_pid -- sleep 10
    Samples: 39K of event 'cycles', Event count(approx.): 18063039903
    Children Self  Command       Shared Object Symbol
    - 77.02% 0.00% redis-server  redis-server  [.] aeMain
     - aeMain
        - 74.59% aeProcessEvents
           - 73.47% processInputBuffer
              - 56.40% processCommand
                 - 53.75% call
                    + 30.44% setCommand
                    + 22.84% propagate
                 + 1.51% dictFetchValue
              + 15.01% processMultibulkBuffer
              + 1.57% resetClient
             0.89% readQueryFromClient
        + 2.43% beforeSleep 
```

采集时间都为 10s，可以明显的看出在 has-slave 的场景下： 新增 replicationFeedSlaves 函数的 cpu 占比 24.54%，主要是在 addReply 和 addReplyBulk 函数，将数据添加到 slave client 的 output buffer 中。这里需要说明的是，perf 在统计中将 replicationFeedSlaves 与 propagate 放在同一层级是与实际源码不符的，实际源码情况 replicationFeedSlaves 是由 progate 函数调用的。

假设执行 `N` 个命令的 cpu 时间开销是不变的 `T`，在 has-slave 的情况下时间开销变为 `(1/0.755)T = 1.325T`，即 tps 下降率为 `24.5%`，而这与实际测试值得下降 `28.3%`仍有一些差距。说明在 has-slave 场景下仍有一些额外的开销，其中一个因素为*发送数据到 slave 的开销*。

这也可以从 `perf report` 的记录中观察出来：

在 has-slave 场景中：

```
    - 16.53% system_call_fastpath
       - 11.25% sys_write
        - 11.10% vfs_write
          - 10.88% do_sync_write
            + 7.17% sock_aio_write
            + 3.67% xfs_file_aio_write # 向 aof 日志写数据
       + 4.86% sys_read
```

在 no-slave 场景中：

```
    - 21.83% system_call_fastpath
       + 13.91% sys_write
       + 7.31% sys_read
       + 0.59% sys_epoll_wait
```

has-slave 的 w/r cpu 比例要高于 no-slave 场景，此处无法判断出有多少 cpu 开销是由于向 slave 同步数据造成的。

综上可以得出**结论1**：在 has-slave 场景下**replicationFeedSlaves 是主要的新增 cpu 开销**（占比 24.5%），此外还有向 slave 发送数据带来的 cpu 开销（占比较少）。

### off-cpu 分析

perf 中用于分析 off-cpu 的命令为 `perf sched`，使用 `perf sched record` 记录进程的调度信息，然后使用 `perf sched timehist` 去进行统计分析[^3]，对输出项的详细说明见[延伸阅读](#延伸阅读)部分

has-slave 的结果如下：

``` shell
    perf sched record -o redis_set_sched.data -- sleep 10

    # 统计 run time
    perf sched timehist -i redis_set_sched.data | grep &quot;\[$pid\]&quot; | awk '{sum += $6}END{print sum}'
    9752.96

    # 统计 wait time
    perf sched timehist -i redis_set_sched.data | grep &quot;\[$pid\]&quot; | awk '{sum += $4}END{print sum}'
    14.903

    # 统计 sch delay
    perf sched timehist -i redis_set_sched.data | grep &quot;\[$pid\]&quot; | awk '{sum += $5}END{print sum}'
    0
```

no-slave 的结果如下：

``` shell
   perf sched record -o redis_set_no_slave_sched.data -- sleep 10

   # 统计 run time
   perf sched timehist -i redis_set_no_slave_sched.data | grep &quot;\[$pid\]&quot; | awk '{sum += $6}END{print sum}' 
   9797.26

   # 统计 wait time
   perf sched timehist -i redis_set_no_slave_sched.data | grep &quot;\[$pid\]&quot; | awk '{sum += $4}END{print sum}' 
   14.546

   # 统计 sch delay
   perf sched timehist -i redis_set_sched.data | grep &quot;\[$pid\]&quot; | awk '{sum += $5}END{print sum}'
   0 
```

总计抓取的时间为 10s，has-slave 与 no-slave 统计出来 `run time` 的时间基本相等，且`wait time` 时间极短，说明进程 `off-cpu` 的时间占比很低。

需要说明的是使用 `perf sched record` 去记录时最好不要指定 pid，直接抓取全局的调度记录，这样才能准确的分析调度的状况。 perf 是基于采样的，难以抓取到所有的 sched 事件，统计结果会存在一定误差。

综上可以得出**结论2**：在 has-slave 和 no-slave 场景下，**off-cpu的时间占比很低**（has-slave 下 off-cpu占比 0.15 %，on-slave 下 off-cpu 占比 0.14%），两者基本相同。

统计结果并未像作者预先设想那样，会由于大量的 network io 交互导致很高的 off-cpu 占比。大部分换出 cpu 是因为多任务争用(retint_careful、sysret_careful)，少部分是因为 network(inet_sendmsg)、file io 读写(xfs_file_aio_write)导致，以及缺页中断(page_fault)和内存分配(sk_stream_alloc_skb)。

### function trace

上面通过 perf profile 和 perf sched 针对 on-cpu 和 off-cpu 两方面进行了分析。而真正的每个 function 的耗时统计，以及读写 slave 带来的时间开销统计，则需要 function trace 来完成。

systemtap 是一款内核调试的利器，详情请参见[延伸阅读](#延伸阅读)，可以通过它抓取 slave 与 master 建立的 socket fd，统计所有 rw 操作耗时。抓取 slave-master 的 socket fd 的脚本如下：

``` stap
#!/usr/bin/env stap
fuction addr_parse : string (fd : long) % {
    struct sockaddr_storage stPeer;
    char buff[128];
    struct sockaddr* sin4 = NULL;
    int err = 0;
    int addrLen = 0;
    struct socket* sock = NULL;

    sock = sockfd_lookup((int)STAP_ARG_fd, &amp;err);
    if (err != 0){
        sprintf(buff, &quot;lookup [%d] failure: %x&quot;, (int)STAP_ARG_fd, err);
    }else{
        err = kernel_getpeername(sock, (struct sockaddr *)&amp;stPeer, &amp;addrLen); 
        if (err &lt; 0) {
            sprintf(buff, &quot;getpeername [%d] failure: %x&quot;, (int)STAP_ARG_fd, err);
        } else {
            sin4 = (struct sockaddr *)&amp;stPeer;
            switch(sin4-&gt;sa_family){
            case AF_INET6:
                        sprintf(buff, &quot;%pISpc&quot;, sin4);
                break;
            case AF_INET:
                        sprintf(buff, &quot;%pISpc&quot;, sin4);
                break;
            }
        }
    }

    strlcat (STAP_RETVALUE, buff, MAXSTRINGLEN);
%}

probe syscall.accept*.return {
    if(pid()==target() &amp;&amp; retval &gt; 0){
        printf(&quot;accept %d: %s\n&quot;, retval, addr_parse(retval))
    }
}
```

统计 slave 相关操作耗时的脚本如下：

``` stap
#!/usr/bin/env stap
global process_event, before_sleep, call, slave_feed, slave_rw, all_rw, start
global ss = $slave_fd

probe process($redis_exec_path).function(&quot;call&quot;).return {
    if (pid()==target()) {
        delay = gettimeofday_us() - @entry(gettimeofday_us())
        call += delay
    }
}

probe process($redis_exec_path).function(&quot;replicationFeedSlaves&quot;).return {
    if (pid()==target()) {
        delay = gettimeofday_us() - @entry(gettimeofday_us())
        slave_feed += delay
    }
}

probe process($redis_exec_path).function(&quot;beforeSleep&quot;).return {
    if (pid()==target()) {
        delay = gettimeofday_us() - @entry(gettimeofday_us())
        before_sleep += delay
    }
}

probe process($redis_exec_path).function(&quot;aeProcessEvents&quot;).return {
    if (pid()==target()) {
        delay = gettimeofday_us() - @entry(gettimeofday_us())
        process_event += delay
    }
}

probe syscall.read.return {
   if (pid()==target()) {
        delay = gettimeofday_us() - @entry(gettimeofday_us())
        if(@entry($fd)==ss){
            slave_rw += delay
        }
        all_rw += delay
   } 
}

probe syscall.write.return {
   if (pid()==target()) {
        delay = gettimeofday_us() - @entry(gettimeofday_us())
        if(@entry($fd)==ss){
            slave_rw += delay
        }
        all_rw += delay
   } 
}

probe begin{
    start = gettimeofday_us() 
}

probe end{
    dur = gettimeofday_us() - start
    printf(&quot;total: %d, processEvent %d, before %d, call %d, slave_feed %d, all_rw %d, slave_rw %d\n&quot;, dur, process_event, before_sleep, call, slave_feed, all_rw, slave_rw); 
}
```

将 `$slave_fd` 和 `$redis_exec_path` 替换为实际值，运行统计的结果如下：

``` shell
    total: 10254152, processEvent 9407135, before 494402, call 6638171, slave_feed 2600933,  all_rw 933086, slave_rw 118100
```

其中 `processEvent+before = job_time = 9901537 us` 相当于整个实例处理命令的耗时，而且这里面包括 on-cpu 和 off-cpu 两个部分，`slave_feed/job_time = 26.26%` 与 上文中 `cpu 24.5%` 占比接近，`all_rw` 用于统计所有 sycall.write 和 syscall.read 的耗时，`all_rw/job_time = 9.4%` 与上文中的 cpu 采样 `16.53%` 有较大出入（这可能是由于 systemtap 引入的延迟导致，call 函数调用的次数高于 syscall，引入的延迟多于 syscall）。

slave_rw 是用于统计 slave 交互导致的耗时，`slave_rw/all_rw = 12.66% slave_rw/job_time = %1.19`， 按照比例推算，在 has-slave 场景中 slave 网络交互引入的 cpu 消耗为 `16.53%*12.66% = 2.06%`，slave 总计引入的 cpu 消耗为 `26.56%`，与上述测试的 tps 下降比例基本吻合。

### 结论

结合1、2 两个结论，最终可以汇总如下：

1. 在开启 pipeline 情况下，has-slave 相比 no-slave 下降的根因为向 slave 同步数据；

2. slave 数据同步大部由于向 slave 的发送缓冲区复制 set 命令（占 cpu 消耗 24.5%），少部分是由于 slave 网络交互（占 cpu 消耗 2.06% 推测值）。

3. 在本文 benchmark 的过程测试中，redis server 无论是否开启 slave，其大部分时间是 on-cpu 的，只有小部分时间是 off-cpu(0.14%~0.15%)，且 off-cpu 多因为系统多任务任务抢占。

由于 systemtap 的安装和入门有些门槛，可以通过 `perf trace` 进行 syscall 的耗时统计。
开启 slave 结果如下：

``` shell
    perf tarce -s -p $redis_server_pid -- sleep 10

    redis-server (289648), 275228 events, 100.0%
    syscall       calls   total       min     avg      max   stddev
                          (msec)    (msec)   (msec)  (msec)  (msec)
    ------------ ------ --------- --------- ------  ------  ------
    write         82497  1180.648    0.002   0.014   2.726   2.36%
    read          54612   626.546    0.002   0.011   0.138   0.16%
    epoll_wait      156    41.277    0.208   0.265   0.375   0.86%
    ... ...
```

不开启 slave 结果如下：

``` shell
    perf tarce -s -p $redis_server_pid -- sleep 10

    redis-server (289648), 303806 events, 100.0%
    syscall       calls   total       min     avg      max   stddev
                          (msec)    (msec)   (msec)  (msec)  (msec)
    ------------ ------ --------- --------- ------  ------  ------
    write         75673  1323.961    0.007   0.017   2.681   2.27%
    read          75644   886.095    0.002   0.012   0.299   0.17%
    epoll_wait      237    48.619    0.158   0.205   0.519   1.09%
    ... ...
```

可以明显看出开启 slave 之后，read 调用次数降低了 27.8%，write 调用次数上涨了 9.0%，read 下降的幅度接近于 tps 的降幅。

## 延伸阅读

### redis 的消息接收与发送

`redis-benchmark` 和 `redis server`中的消息处理体系，其读写过程如下：

+ receive:  内核从 NIC 中接收到消息，触发了 `readable` 事件，server 响应 `readable` 事件，调用 `read(2)`函数，将数据从内核空间复制到用户空间；
+ send: 调用 `write(2)` 函数发送消息，将数据从用户空间复制到内核空间，然后再传递到 NIC 进行发送。如果一次未完全发送，则为 `socket` 绑定 `writeHandle` 去响应 `writeable` 事件，继续发送剩余数据；

ps: redis 在 linux 中使用 `epoll(7)` 函数实现事件服务模型，其使用的事件类型为 `边沿触发`(edge-triggered) [^2]。

在调用 [createClient]() 创建 client 时，为 socket 设置了 `O_NONBLOCK` 和 `TCP_NODELAY` 两个 flag，即 socket 为非阻塞且关闭 *Nagle* 算法（主要优化small packet 的处理，提高网络利用率）提高请求的响应速度。

### redis 命令的耗时统计

redis 的命令耗时统计继承了它一贯简单易懂的风格，它为每一种 `command` 创建了一个全局对象，同种 `command` 请求都会复用这个全局对象，在 `command` 结构体内有 `microseconds` 和 `calls` 两个字段用于耗时统计：

``` C
    struct redisCommand {
        char *name;
        redisCommandProc *proc;
        int arity;
        char *sflags; /* Flags as string representation, one char per flag. */
        int flags;    /* The actual flags, obtained from the 'sflags' field. */
        /* Use a function to determine keys arguments in a command line.
        * Used for Redis Cluster redirect. */
        redisGetKeysProc *getkeys_proc;
        /* What keys should be loaded in background when calling this command? */
        int firstkey; /* The first argument that's a key (0 = no keys) */
        int lastkey;  /* The last argument that's a key */
        int keystep;  /* The step between first and last key */
        /* 用于耗时统计的字段
           microseconds 记录执行该命令的总耗时 
           calls 记录该命令的总调用次数 
        */
        long long microseconds, calls; 
    };

    /* If this function gets called we already read a whole
    * command, arguments are in the client argv/argc fields.
    * processCommand() execute the command or prepare the
    * server for a bulk read from the client.
    *
    * If C_OK is returned the client is still alive and valid and
    * other operations can be performed by the caller. Otherwise
    * if C_ERR is returned the client was destroyed (i.e. after QUIT). */
    int processCommand(client *c) {
        /* Now lookup the command and check ASAP about trivial error conditions
        * such as wrong arity, bad command name and so forth. */
        // 从command列表中找到要执行的command，
        // 同时赋值给 c-&gt;cmd 和 c-&gt;lastcmd.
        // lookupCommand 函数会返回待执行命令对应的全局对象 
        c-&gt;cmd = c-&gt;lastcmd = lookupCommand(c-&gt;argv[0]-&gt;ptr);

        /* Exec the command */
        if (c-&gt;flags &amp; CLIENT_MULTI &amp;&amp;
            c-&gt;cmd-&gt;proc != execCommand &amp;&amp; c-&gt;cmd-&gt;proc != discardCommand &amp;&amp;
            c-&gt;cmd-&gt;proc != multiCommand &amp;&amp; c-&gt;cmd-&gt;proc != watchCommand)
        {
            queueMultiCommand(c);
            addReply(c,shared.queued);
        } else {
            call(c,CMD_CALL_FULL);
            c-&gt;woff = server.master_repl_offset;
            if (listLength(server.ready_keys))
                handleClientsBlockedOnLists();
        }
        return C_OK;
    }

    void call(client *c, int flags) {
        ... ...
        /* Call the command. */
        dirty = server.dirty;
        // 记录起始时间，使用 us 时间戳
        start = ustime();
        c-&gt;cmd-&gt;proc(c);
        // 计算耗时
        duration = ustime()-start;
        dirty = server.dirty-dirty;
        if (dirty &lt; 0) dirty = 0;
        ... ...
        ... ...
        if (flags &amp; CMD_CALL_STATS) {
            // 更新总耗时
            c-&gt;lastcmd-&gt;microseconds += duration;
            // 更新总执行数量
            c-&gt;lastcmd-&gt;calls++;
        }
    }
```

需要额外补充的是，redis 在`multi-exec` 执行过程中会将所有包含的指令统计为 `execCommand`:

``` C
    void execCommand(client *c) {
        ... ...
        // 依次执行所有缓存的`mult-exec`指令
        for (j = 0; j &lt; c-&gt;mstate.count; j++) {
            c-&gt;argc = c-&gt;mstate.commands[j].argc;
            c-&gt;argv = c-&gt;mstate.commands[j].argv;
            c-&gt;cmd = c-&gt;mstate.commands[j].cmd;

            /* Propagate a MULTI request once we encounter the first command which
            * is not readonly nor an administrative one.
            * This way we'll deliver the MULTI/..../EXEC block as a whole and
            * both the AOF and the replication link will have the same consistency
            * and atomicity guarantees. */
            if (!must_propagate &amp;&amp; !(c-&gt;cmd-&gt;flags &amp; (CMD_READONLY|CMD_ADMIN))) {
                execCommandPropagateMulti(c);
                must_propagate = 1;
            }

            // 调用执行对应的命令
            call(c,server.loading ? CMD_CALL_NONE : CMD_CALL_FULL);

            /* Commands may alter argc/argv, restore mstate. */
            c-&gt;mstate.commands[j].argc = c-&gt;argc;
            c-&gt;mstate.commands[j].argv = c-&gt;argv;
            c-&gt;mstate.commands[j].cmd = c-&gt;cmd;
        }
        ... ...
    }
```

结合 `processCommand` 函数的代码，在执行 `exec`请求时已经赋值 `c-&gt;lastcmd=execCommand`，在调用`call` 函数执行 `execCommand` 时会在结束后累计一个执行次数，而在 `execCommand` 内部调用 `call` 也会累计 N （mulit-exec打包的命令个数）个执行次数，而这些执行都会累计到 `c-&gt;lastcmd` 即 `exec` 上，即会累计 `N+1` 个 `execCommand`调用次数:

``` mermaid! 
flowchart TB
    start([start])
    exit([end])

    process[proccessCommand: &lt;br&gt; c-&gt;lastcmd=execCommand]
    multi[proccessCommand:call:&lt;br&gt;execCommand start]
    next{proccessCommand:call:&lt;br&gt;execCommand:&lt;br&gt;next command?}
    run[proccessCommand:call:&lt;br&gt;execCommand:call:&lt;br&gt; run xx command, c-&gt;lastcmd-&gt;calls++]
    return[proccessCommand:call:&lt;br&gt;execCommand end, c-&gt;lastcmd-&gt;calls++]

    start--&gt;process--&gt;multi--&gt;next
    next--&gt;|No|return--&gt;exit
    next--&gt;|Yes|run--&gt;next
```

### perf sched

perf sched timehist 示例如下：

``` shell
    perf sched record -- sleep 1
    perf sched timehist

            time    cpu    task name             wait time  sch delay   run time
                           [tid/pid]              (msec)     (msec)     (msec)
    -------------- ------  --------------------  ---------  ---------  ---------
    79371.874569 [0011]  gcc[31949]                0.014      0.000      1.148
    79371.874591 [0010]  gcc[31951]                0.000      0.000      0.024
    79371.874603 [0010]  migration/10[59]          3.350      0.004      0.011
    79371.874604 [0011]  &lt;idle&gt;                    1.148      0.000      0.035
    79371.874723 [0005]  &lt;idle&gt;                    0.016      0.000      1.383
    79371.874746 [0005]  gcc[31949]                0.153      0.078      0.022           
```

输出项说明：

+ wait time：在 sched-out 和 下一个 sched-in 之间的间隔；
+ sch delay：在 wakeup 时间 和 实际 running 之间的间隔；
+ run time：task 实际处于 running 的时间；

可以通过 `perf list | grep sched` 查看系统中所有的 tracepoint，核心的 tracepoint 为：

+ sched_switch：记录 cpu 上 task 切换的事件，从 prev 切换到 next；
+ sched_wakeup：记录唤醒 task 的时间，记录唤醒 p 和 current；
+ sched_waking：3.10 中不支持此 tracepoint；
+ sched_migrate_task：记录 task 迁移到目的 cpu 事件，p 切换到 dest_cpu；

除此外还有 state 类型，用于统计 delay 时间：

+ sched_stat_blocked：任务不可中断时间；
+ sched_stat_iowait：由于等待 IO 完成，任务不可运行的时间；
+ sched_stat_runtime：记录在 cpu 上执行的时间；
+ sched_stat_sleep：任务不可运行的时间，包括 io_wait；
+ sched_stat_wait：由于调度程序争，任务可执行而未执行的延迟时间；

上述的 timehist 即是通过 state 进行统计：

+ t = time of current schedule out event
+ tprev = time of previous sched out event also time of schedule-in event for current task
+ last_time = time of last sched change event for current task(i.e, time process was last scheduled out)
+ ready_to_run = time of wakeup for current task
 
```
  -----|------------|------------|------------|------
      last         ready        tprev         t
      time         to run
       |-------- wait time ------|
                   |- sch delay -|- run time -|
```

### stap 使用

stap -L &quot;syscall.*&quot; 可以查看当前操作系统支持探测的 event，和 event 中携带的参数：

``` shell
stap -L &quot;syscall.*&quot; 
... ...
syscall.read name:string fd:long buf_uaddr:long count:long argstr:string
... ...
syscall.write name:string fd:long buf_uaddr:long count:long buf_str:string argstr:string
... ...
```

使用 stap 命令进行编译运行时会经过如下步骤[^7]：

1. 首先，SystemTap 根据现有的 Tapset 库（通常在 /usr/share/systemtap/tapset/ 中) 检查使用的任何 Tapset。SystemTap 将用它们在 Tapset 库中的相应定义替换任何找到的 Tapset;

2. SystemTap 然后将脚本转换为 C，运行系统 C 编译器以从中创建内核模块。执行此步骤的工具包含在 systemtap 包中（有关详细信息，请参阅第 2.1.1 节“安装 SystemTap”）;

3. SystemTap 加载模块，然后启用脚本中的所有探测器（事件和处理程序）。 systemtap-runtime 包中的 staprun（有关更多信息，请参阅第 2.1.1 节“安装 SystemTap”）提供此功能。

4. 随着事件的发生，它们相应的处理程序被执行。

5. 一旦 SystemTap 会话终止，探测器将被禁用，内核模块将被卸载。

编译和运行命令如下：

``` shell
# CONFIG_MODVERSIONS 指定内核检查时根据 symbol，跳过 magic 检查；
# -p4 指定stap 只执行到第 4 步即输出内核模块 xx.ko, 第 5 步为安装执行。 
sudo stap -vv -B CONFIG_MODVERSIONS=y -p4 -g xx.stp -m xx

# 指定 pid
sudo staprun xx.ko -x $pid

# 指定执行的命令，并监听 可以直接在 stap 脚本中通过 target() 获取 pid.
sudo staprun xx.ko -c $command 
```

## 参考引用

[^1]: redis_psync_protocol(1). Zhipeng Wang. Jan 06 2022, https://saffraan.github.io/redis_psync_protocol(1)/#%E5%91%BD%E4%BB%A4%E5%90%8C%E6%AD%A5
[^2]: man epoll(7). linux. Sep 15 2017, https://man7.org/linux/man-pages/man7/epoll.7.html

[^3]: perf sched.. https://zhuanlan.zhihu.com/p/143320517
[^4]: off-cpu. . https://www.brendangregg.com/offcpuanalysis.html
[^5]: Linux中如何保证数据安全落盘. 黑客画家. Jun 14 2019, https://my.oschina.net/fileoptions/blog/3061997
[^6]: Direct I/O tuning. IBM. Apr 07 2022, https://www.ibm.com/docs/en/aix/7.2?topic=tuning-direct-io
[^7]: SystemTap Beginners Guide. sourceware.org. May 09 2022, https://sourceware.org/systemtap/SystemTap_Beginners_Guide/understanding-how-systemtap-works.html#understanding-architecture-tools</content><author><name>Zhipeng Wang</name><email>wangzhipenghyc@163.com</email></author><summary type="html"></summary></entry><entry><title type="html">Redis_psync_protocol(1)</title><link href="http://localhost:4000/redis_psync_protocol(1)/" rel="alternate" type="text/html" title="Redis_psync_protocol(1)" /><published>2022-01-06T00:00:00+08:00</published><updated>2022-01-06T00:00:00+08:00</updated><id>http://localhost:4000/redis_psync_protocol(1)</id><content type="html" xml:base="http://localhost:4000/redis_psync_protocol(1)/">&lt;style&gt;
   @import url(&quot;/assets/css/simple.css&quot;)
&lt;/style&gt;

# Redis psync protocol(续)

在上一篇 [redis psync protocol](https://saffraan.github.io/redis_psync_protocol/) 中详细的阐述了 psync 协议的交互流程和实现细节，本文主要是针对命令同步的细节和生产实践中遇到的场景进行一些补充。文中代码部分可以忽略，直接看相关结论。

## 命令同步

在 redis 的源码中包含多种 command flag，利用这些 flag 来标识 command 的属性：

+ r(读)：读取数据，不会修改 key 数据；
+ w(写)：写入数据，可能会修改 key 数据；
+ m(内存)：可能会增长内存使用率，在 out of memory 时不允许使用；
+ a(管理)：管理命令，例如 SHUTDOWN、SAVE 命令；
+ p(发布订阅)：发布订阅相关的命令；
+ f(强制同步)：无论是否修改 data set 都需要同步给 slave；
+ s(非script)：在script中不支持的命令；
+ l(loading)：在数据库加载数据时允许执行的命令；
+ t(-)：当 slave 具有一些陈旧数据但是不允许使用该数据提供服务时，只有少数命令被允许执行；
+ M(屏蔽monitor)：不会被自动传播给 monitor 的命令；
+ k(ask)： 为此命令执行隐式 ASKING，因此如果 slot 标记为“importing”，则该命令将在集群模式下被接受；
+ F(Fast command)：在 kernel 分配足够执行时间时，时间复杂度为 O(1)、O(log(N)) 的命令执行很快，几乎无延迟。需要注意的是可能会触发 “DEL” 的命令不是 Fast command（例如 SET）；

每一个命令可能会包含多个 flag，例如 `get` 命令的 command flag 为 `rF`，表示该命令是一个只读的 fast command。对于主从同步来说**不会修改 data set 的命令是无需同步的（例如：带有 `r` flag 的命令），可能会修改 data set 的命令也只需要在实际修改了 data set 时才去同步**。带有 `f` flag 的命令则无论是否修改 data set 都需要被同步，但目前在 5.0 版本中未发现携带该 flag 的命令。

执行命令时调用的是[processCommand](https://github.com/redis/redis/blob/5.0/src/server.c#L2585)函数：

``` C
/* If this function gets called we already read a whole
 * command, arguments are in the client argv/argc fields.
 * processCommand() execute the command or prepare the
 * server for a bulk read from the client.
 *
 * If C_OK is returned the client is still alive and valid and
 * other operations can be performed by the caller. Otherwise
 * if C_ERR is returned the client was destroyed (i.e. after QUIT). */
int processCommand(client *c) {
    ... ...
    /* Exec the command */
    if (c-&gt;flags &amp; CLIENT_MULTI &amp;&amp;
        c-&gt;cmd-&gt;proc != execCommand &amp;&amp; c-&gt;cmd-&gt;proc != discardCommand &amp;&amp;
        c-&gt;cmd-&gt;proc != multiCommand &amp;&amp; c-&gt;cmd-&gt;proc != watchCommand)
    {
        queueMultiCommand(c);
        addReply(c,shared.queued);
    } else {
        call(c,CMD_CALL_FULL);
        c-&gt;woff = server.master_repl_offset;
        if (listLength(server.ready_keys))
            handleClientsBlockedOnKeys();
    }
}
```

在 processCommand 执行 command 前要经过一系列的检查：

1. 检查命令是否合法，如果命令不合法会将 transaction 标记为失败
2. 检查 client 认证信息
3. 如果开启了 cluster 检查是否需要重定向（master节点的无需重定向）
4. 检查内存是否充足
5. 如果当前节点为主节点，且存在磁盘持久化的问题，则拒绝写入命令
6. 如果配置了 min-slaves-to-write 选项，且当前slaves数量不满足，则拒绝写入命令
7. 当 slave-serve-stale-data 选项为 no，且节点为slave 且未与 master建立链接时，仅接受 INFO、SLAVEOF、PING、AUTH、replconf、replicaof、role、config、 等 flag 为 ‘t’ 的命令
8. 如果 server 处于 loading 状态，当前command 不包含 CMD_LOADING flag 则返回 loadingerr
9. 如果当前server处于 lua 超时状态，则只接受 auth、replconf、shutdown nosave、script kill命令
10. 执行命令，如果是 multibulk命令，则加入到 args 中等 exec 命令，否则调用 call 执行

在执行命令时主要是调用 [call](https://github.com/redis/redis/blob/5.0/src/server.c#L2451) 函数，在 redis 执行命令时不仅要同步给 slave 节点，（当aof日志开启时）也需要同步到 aof log，call 函数可以根据传入的 flag 来判断 `propagation` 的行为：

+ CMD_CALL_PROPAGATE_AOF: 如果修改了 data set 则将命令同步到 AOF 日志；
+ CMD_CALL_PROPAGATE_REPL: 如果修改了 data set 则将命令同步给 slave 节点；

同时 client 自身的 flag 也会影响到 `propagation` 的行为，具体逻辑如下：

``` C
/* Call() is the core of Redis execution of a command.
 *
 * The following flags can be passed:
 * CMD_CALL_NONE        No flags.
 * CMD_CALL_SLOWLOG     Check command speed and log in the slow log if needed.
 * CMD_CALL_STATS       Populate command stats.
 * CMD_CALL_PROPAGATE_AOF   Append command to AOF if it modified the dataset
 *                          or if the client flags are forcing propagation.
 * CMD_CALL_PROPAGATE_REPL  Send command to salves if it modified the dataset
 *                          or if the client flags are forcing propagation.
 * CMD_CALL_PROPAGATE   Alias for PROPAGATE_AOF|PROPAGATE_REPL.
 * CMD_CALL_FULL        Alias for SLOWLOG|STATS|PROPAGATE.
 *
 * The exact propagation behavior depends on the client flags.
 * Specifically:
 *
 * 1. If the client flags CLIENT_FORCE_AOF or CLIENT_FORCE_REPL are set
 *    and assuming the corresponding CMD_CALL_PROPAGATE_AOF/REPL is set
 *    in the call flags, then the command is propagated even if the
 *    dataset was not affected by the command.
 * 2. If the client flags CLIENT_PREVENT_REPL_PROP or CLIENT_PREVENT_AOF_PROP
 *    are set, the propagation into AOF or to slaves is not performed even
 *    if the command modified the dataset.
 *
 * Note that regardless of the client flags, if CMD_CALL_PROPAGATE_AOF
 * or CMD_CALL_PROPAGATE_REPL are not set, then respectively AOF or
 * slaves propagation will never occur.
 *
 * Client flags are modified by the implementation of a given command
 * using the following API:
 *
 * forceCommandPropagation(client *c, int flags);
 * preventCommandPropagation(client *c);
 * preventCommandAOF(client *c);
 * preventCommandReplication(client *c);
 *
 */
void call(client *c, int flags) {
     /* Propagate the command into the AOF and replication link */
    if (flags &amp; CMD_CALL_PROPAGATE &amp;&amp;
        (c-&gt;flags &amp; CLIENT_PREVENT_PROP) != CLIENT_PREVENT_PROP)
    {
        int propagate_flags = PROPAGATE_NONE;

        /* Check if the command operated changes in the data set. If so
         * set for replication / AOF propagation. */
        if (dirty) propagate_flags |= (PROPAGATE_AOF|PROPAGATE_REPL);

        /* If the client forced AOF / replication of the command, set
         * the flags regardless of the command effects on the data set. */
        if (c-&gt;flags &amp; CLIENT_FORCE_REPL) propagate_flags |= PROPAGATE_REPL;
        if (c-&gt;flags &amp; CLIENT_FORCE_AOF) propagate_flags |= PROPAGATE_AOF;

        /* However prevent AOF / replication propagation if the command
         * implementations called preventCommandPropagation() or similar,
         * or if we don't have the call() flags to do so. */
        if (c-&gt;flags &amp; CLIENT_PREVENT_REPL_PROP ||
            !(flags &amp; CMD_CALL_PROPAGATE_REPL))
                propagate_flags &amp;= ~PROPAGATE_REPL;
        if (c-&gt;flags &amp; CLIENT_PREVENT_AOF_PROP ||
            !(flags &amp; CMD_CALL_PROPAGATE_AOF))
                propagate_flags &amp;= ~PROPAGATE_AOF;

        /* Call propagate() only if at least one of AOF / replication
         * propagation is needed. Note that modules commands handle replication
         * in an explicit way, so we never replicate them automatically. */
        if (propagate_flags != PROPAGATE_NONE &amp;&amp; !(c-&gt;cmd-&gt;flags &amp; CMD_MODULE))
            propagate(c-&gt;cmd,c-&gt;db-&gt;id,c-&gt;argv,c-&gt;argc,propagate_flags);
    }
    ... ...
}
```

综上，如果要想将命令写入 aof 和 slave 则必须要满足两个条件：

1. 传入 `CMD_CALL_PROPAGATE_AOF|CMD_CALL_PROPAGATE_REPL` flag；
2. 命令修改了 data set；

processCommand 调用 call 函数传入的 flag 为 `CMD_CALL_FULL`，相当于  `SLOWLOG|STATS|PROPAGATE`，所以条件 1 是满足。条件 2 的关键在于如何判断 data set 是否被修改了？这主要依赖于 server.dirty 字段，**如果命令在执行的过程中有修改 data set 的操作，server.dirty 字段会被修改**。其核心逻辑实现也在 call 函数中：

``` C
void call(client *c, int flags) {
    ... ...
    /* Call the command. */
    dirty = server.dirty;
    updateCachedTime(0);
    start = server.ustime;
    c-&gt;cmd-&gt;proc(c);
    duration = ustime()-start;
    dirty = server.dirty-dirty;
    if (dirty &lt; 0) dirty = 0;
    
    ... ...

     /* Propagate the command into the AOF and replication link */
    if (flags &amp; CMD_CALL_PROPAGATE &amp;&amp;
        (c-&gt;flags &amp; CLIENT_PREVENT_PROP) != CLIENT_PREVENT_PROP)
    {
        int propagate_flags = PROPAGATE_NONE;

        /* Check if the command operated changes in the data set. If so
         * set for replication / AOF propagation. */
        if (dirty) propagate_flags |= (PROPAGATE_AOF|PROPAGATE_REPL);
        
        ... ...

        /* Call propagate() only if at least one of AOF / replication
         * propagation is needed. Note that modules commands handle replication
         * in an explicit way, so we never replicate them automatically. */
        if (propagate_flags != PROPAGATE_NONE &amp;&amp; !(c-&gt;cmd-&gt;flags &amp; CMD_MODULE))
            propagate(c-&gt;cmd,c-&gt;db-&gt;id,c-&gt;argv,c-&gt;argc,propagate_flags);
    }
}

/* Propagate the specified command (in the context of the specified database id)
 * to AOF and Slaves.
 *
 * flags are an xor between:
 * + PROPAGATE_NONE (no propagation of command at all)
 * + PROPAGATE_AOF (propagate into the AOF file if is enabled)
 * + PROPAGATE_REPL (propagate into the replication link)
 *
 * This should not be used inside commands implementation since it will not
 * wrap the resulting commands in MULTI/EXEC. Use instead alsoPropagate(),
 * preventCommandPropagation(), forceCommandPropagation().
 *
 * However for functions that need to (also) propagate out of the context of a
 * command execution, for example when serving a blocked client, you
 * want to use propagate().
 */
void propagate(struct redisCommand *cmd, int dbid, robj **argv, int argc,
               int flags)
{
    if (server.aof_state != AOF_OFF &amp;&amp; flags &amp; PROPAGATE_AOF)
        feedAppendOnlyFile(cmd,dbid,argv,argc);
    if (flags &amp; PROPAGATE_REPL)
        replicationFeedSlaves(server.slaves,dbid,argv,argc);
}
```

综上，扩散命令到 slave 节点的调用链为：processCommand-&gt;call-&gt;propagate-&gt;replicationFeedSlaves，针对 replicationFeedSlaves 的解析在下文会提到。下面以 [set](https://github.com/redis/redis/blob/5.0/src/t_string.c#L96) 命令为例展示一下 server.dirty 的作用：

``` C
/* SET key value [NX] [XX] [EX &lt;seconds&gt;] [PX &lt;milliseconds&gt;] */
void setCommand(client *c) {
    /* parse args */
    ... ...
    setGenericCommand(c,flags,c-&gt;argv[1],c-&gt;argv[2],expire,unit,NULL,NULL);
}

void setGenericCommand(client *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) {
    long long milliseconds = 0; /* initialized to avoid any harmness warning */

    if (expire) {
        if (getLongLongFromObjectOrReply(c, expire, &amp;milliseconds, NULL) != C_OK)
            return;
        if (milliseconds &lt;= 0) {
            addReplyErrorFormat(c,&quot;invalid expire time in %s&quot;,c-&gt;cmd-&gt;name);
            return;
        }
        if (unit == UNIT_SECONDS) milliseconds *= 1000;
    }

    if ((flags &amp; OBJ_SET_NX &amp;&amp; lookupKeyWrite(c-&gt;db,key) != NULL) ||
        (flags &amp; OBJ_SET_XX &amp;&amp; lookupKeyWrite(c-&gt;db,key) == NULL))
    {
        addReply(c, abort_reply ? abort_reply : shared.nullbulk);
        return;
    }
    setKey(c-&gt;db,key,val);
    server.dirty++;
    if (expire) setExpire(c,c-&gt;db,key,mstime()+milliseconds);
    notifyKeyspaceEvent(NOTIFY_STRING,&quot;set&quot;,key,c-&gt;db-&gt;id);
    if (expire) notifyKeyspaceEvent(NOTIFY_GENERIC,
        &quot;expire&quot;,key,c-&gt;db-&gt;id);
    addReply(c, ok_reply ? ok_reply : shared.ok);
}
```

在经过 expire time 和 NX/XX 的判断后，就会去修改内存中的数据，此时执行 `server.dirty++`，则会被同步到 slaves 和 aof，由此可见即使向 key 中写入同样的 value 也会被同步。

## 同步转化

在 redis 中大部分命令都会原封不动的转发给 slave 节点，也存在少部分命令需要会转化为其他命令再同步给 slave 节点。

1. eval 和 evalsha
    在 redis lua script 中支持很多内生函数，其中与复制相关的如下：

    + redis.replicate_commands()：启动脚本效果复制，开启后只复制脚本生成的写入命令，无需复制整个脚本。需要在监本执行任何操作之前调用，在 Redis 5.0 中默认开启。
    + redis.set_repl(int flag)：在开启脚本效果复制后，使用 flag 其控制复制行为：REPL_NONE、REPL_AOF、REPL_SLAVE、REPL_RELICA、REPL_ALL（缺省复制行为）。与上文中 call 函数中 flag 功能十分相似。

    所以在 5.0 中执行 script 会被自动转换为 multi-exec 事务命令；在 &gt;= 4.0.4 版本中 evalsha 命令会被转化为 eval；在 &lt; 4.0.4 版本中则不会转化，直接转发。

2. migrate
    slot 迁移属于数据库管理指令，在执行槽迁移时，会在源节点执行 `DUMP + DEL` 命令，在目的节点执行 `RESTOR` 命令。由于 `DUMP` 命令未修改 data set 所以不会被同步给 slave 节点， `DEL` 和 `RESTOR`命令会同步给 slave 节点。

## 增量同步数据传输和错误处理

master 发送数据时调用 [replicationFeedSlaves](https://github.com/redis/redis/blob/5.0/src/replication.c#L174) 函数会调用 [addReplyBulk](https://github.com/redis/redis/blob/5.0/src/networking.c#L562) 将 backlog 中缓存的数据加入到 slave list 中每个 slave client 的发送缓冲区。其中 addReplyBulk 的核心函数 [addReply](https://github.com/redis/redis/blob/5.0/src/networking.c#L536)  会调用 [prepareClientToWrite](https://github.com/redis/redis/blob/5.0/src/networking.c#L212) 会将 client 加入到 `server.clients_pending_write` 队列中，redis 主进程在每次进入 event loop 前调用 [handleClientsWithPendingWrites](https://github.com/redis/redis/blob/5.0/src/networking.c#L1082) 将 client 从 `server.clients_pending_write` 队列中取出，将 client buffer 中的数据发送对端：

``` C
/* Propagate write commands to slaves, and populate the replication backlog
 * as well. This function is used if the instance is a master: we use
 * the commands received by our clients in order to create the replication
 * stream. Instead if the instance is a slave and has sub-slaves attached,
 * we use replicationFeedSlavesFromMaster() */
void replicationFeedSlaves(list *slaves, int dictid, robj **argv, int argc) {
    
    /* Write the command to backlog. */
    ... ...

    /* Write the command to every slave. */
    listRewind(slaves,&amp;li);
    while((ln = listNext(&amp;li))) {
        client *slave = ln-&gt;value;

        /* Don't feed slaves that are still waiting for BGSAVE to start */
        if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) continue;

        /* Feed slaves that are waiting for the initial SYNC (so these commands
         * are queued in the output buffer until the initial SYNC completes),
         * or are already in sync with the master. */

        /* Add the multi bulk length. */
        addReplyMultiBulkLen(slave,argc);

        /* Finally any additional argument that was not stored inside the
         * static buffer if any (from j to argc). */
        for (j = 0; j &lt; argc; j++)
            addReplyBulk(slave,argv[j]);
    }
}

/* -----------------------------------------------------------------------------
 * Higher level functions to queue data on the client output buffer.
 * The following functions are the ones that commands implementations will call.
 * -------------------------------------------------------------------------- */

/* Add the object 'obj' string representation to the client output buffer. */
void addReply(client *c, robj *obj) {
    if (prepareClientToWrite(c) != C_OK) return;

    /* add the object into the client buffer*/
    ... ... 
}

/* This function is called every time we are going to transmit new data
 * to the client. The behavior is the following:
 *
 * If the client should receive new data (normal clients will) the function
 * returns C_OK, and make sure to install the write handler in our event
 * loop so that when the socket is writable new data gets written.
 *
 * If the client should not receive new data, because it is a fake client
 * (used to load AOF in memory), a master or because the setup of the write
 * handler failed, the function returns C_ERR.
 *
 * The function may return C_OK without actually installing the write
 * event handler in the following cases:
 *
 * 1) The event handler should already be installed since the output buffer
 *    already contains something.
 * 2) The client is a slave but not yet online, so we want to just accumulate
 *    writes in the buffer but not actually sending them yet.
 *
 * Typically gets called every time a reply is built, before adding more
 * data to the clients output buffers. If the function returns C_ERR no
 * data should be appended to the output buffers. */
int prepareClientToWrite(client *c) {
   ... ...

    /* Schedule the client to write the output buffers to the socket, unless
     * it should already be setup to do so (it has already pending data). */
    if (!clientHasPendingReplies(c)) clientInstallWriteHandler(c);

    /* Authorize the caller to queue in the output buffer of this client. */
    return C_OK;
}

/* This funciton puts the client in the queue of clients that should write
 * their output buffers to the socket. Note that it does not *yet* install
 * the write handler, to start clients are put in a queue of clients that need
 * to write, so we try to do that before returning in the event loop (see the
 * handleClientsWithPendingWrites() function).
 * If we fail and there is more data to write, compared to what the socket
 * buffers can hold, then we'll really install the handler. */
void clientInstallWriteHandler(client *c) {
    /* Schedule the client to write the output buffers to the socket only
     * if not already done and, for slaves, if the slave can actually receive
     * writes at this stage. */
    if (!(c-&gt;flags &amp; CLIENT_PENDING_WRITE) &amp;&amp;
        (c-&gt;replstate == REPL_STATE_NONE ||
         (c-&gt;replstate == SLAVE_STATE_ONLINE &amp;&amp; !c-&gt;repl_put_online_on_ack)))
    {
        /* Here instead of installing the write handler, we just flag the
         * client and put it into a list of clients that have something
         * to write to the socket. This way before re-entering the event
         * loop, we can try to directly write to the client sockets avoiding
         * a system call. We'll only really install the write handler if
         * we'll not be able to write the whole reply at once. */
        c-&gt;flags |= CLIENT_PENDING_WRITE;
        listAddNodeHead(server.clients_pending_write,c);
    }
}
```

着重分析一下 handleClientsWithPendingWrites 函数，它会将 client 从队列中依次取出，尝试调用 [writeToClient](https://github.com/redis/redis/blob/5.0/src/networking.c#L979) 将数据直接发送给对端，如果仍有残留数据需要发送，绑定 [sendReplyToClient](https://github.com/redis/redis/blob/5.0/src/networking.c#L1072) 到 slave fd 的可写入事件。当接收到写入事件，调用 POSIX write 向 slave fd 写入发送缓冲区的数据，write 如果返回小于 0，则停止写入。如果`errno == EAGAIN`，则忽略错误直接返回，否则返回错误，且会主动释放掉链接：

``` C
/* This function is called just before entering the event loop, in the hope
 * we can just write the replies to the client output buffer without any
 * need to use a syscall in order to install the writable event handler,
 * get it called, and so forth. */
int handleClientsWithPendingWrites(void) {
    listIter li;
    listNode *ln;
    int processed = listLength(server.clients_pending_write);

    listRewind(server.clients_pending_write,&amp;li);
    while((ln = listNext(&amp;li))) {
        client *c = listNodeValue(ln);
        c-&gt;flags &amp;= ~CLIENT_PENDING_WRITE;
        listDelNode(server.clients_pending_write,ln);

        /* If a client is protected, don't do anything,
         * that may trigger write error or recreate handler. */
        if (c-&gt;flags &amp; CLIENT_PROTECTED) continue;

        /* Try to write buffers to the client socket. */
        if (writeToClient(c-&gt;fd,c,0) == C_ERR) continue;

        /* If after the synchronous writes above we still have data to
         * output to the client, we need to install the writable handler. */
        if (clientHasPendingReplies(c)) {
            int ae_flags = AE_WRITABLE;
            /* For the fsync=always policy, we want that a given FD is never
             * served for reading and writing in the same event loop iteration,
             * so that in the middle of receiving the query, and serving it
             * to the client, we'll call beforeSleep() that will do the
             * actual fsync of AOF to disk. AE_BARRIER ensures that. */
            if (server.aof_state == AOF_ON &amp;&amp;
                server.aof_fsync == AOF_FSYNC_ALWAYS)
            {
                ae_flags |= AE_BARRIER;
            }
            if (aeCreateFileEvent(server.el, c-&gt;fd, ae_flags,
                sendReplyToClient, c) == AE_ERR)
            {
                    freeClientAsync(c);
            }
        }
    }
    return processed;
}

/* Write event handler. Just send data to the client. */
void sendReplyToClient(aeEventLoop *el, int fd, void *privdata, int mask) {
    UNUSED(el);
    UNUSED(mask);
    writeToClient(fd,privdata,1);
}

/* Write data in output buffers to client. Return C_OK if the client
 * is still valid after the call, C_ERR if it was freed. */
int writeToClient(int fd, client *c, int handler_installed) {
    ... ...
    while(clientHasPendingReplies(c)) {
        ... ...
        if (nwritten == -1) {
            if (errno == EAGAIN) {
                nwritten = 0;
            } else {
                serverLog(LL_VERBOSE,
                    &quot;Error writing to client: %s&quot;, strerror(errno));
                freeClient(c);
                return C_ERR;
            }
        }
        ... ..
    }
    return C_OK;
}

```

slave 在完成 psync shake 后会调用 [replicationResurrectCachedMaster](https://github.com/redis/redis/blob/5.0/src/replication.c#L2284) 将 [readQueryFromClient](https://github.com/redis/redis/blob/5.0/src/networking.c#L1522) 与 master fd 的读取 event 绑定。当接收到读取事件，调用 POSIX read 从 master fd 读取数据，read 如果返回小于 0，则停止读取。如果 `errno == EAGAIN`则忽略错误，否则返回错误，且主动释放掉链接：

```C
/* Turn the cached master into the current master, using the file descriptor
 * passed as argument as the socket for the new master.
 *
 * This function is called when successfully setup a partial resynchronization
 * so the stream of data that we'll receive will start from were this
 * master left. */
void replicationResurrectCachedMaster(int newfd) {
    server.master = server.cached_master;
    server.cached_master = NULL;
    server.master-&gt;fd = newfd;
    server.master-&gt;flags &amp;= ~(CLIENT_CLOSE_AFTER_REPLY|CLIENT_CLOSE_ASAP);
    server.master-&gt;authenticated = 1;
    server.master-&gt;lastinteraction = server.unixtime;
    server.repl_state = REPL_STATE_CONNECTED;
    server.repl_down_since = 0;

    /* Re-add to the list of clients. */
    linkClient(server.master);
    if (aeCreateFileEvent(server.el, newfd, AE_READABLE,
                          readQueryFromClient, server.master)) {
        serverLog(LL_WARNING,&quot;Error resurrecting the cached master, impossible to add the readable handler: %s&quot;, strerror(errno));
        freeClientAsync(server.master); /* Close ASAP. */
    }

    /* We may also need to install the write handler as well if there is
     * pending data in the write buffers. */
    if (clientHasPendingReplies(server.master)) {
        if (aeCreateFileEvent(server.el, newfd, AE_WRITABLE,
                          sendReplyToClient, server.master)) {
            serverLog(LL_WARNING,&quot;Error resurrecting the cached master, impossible to add the writable handler: %s&quot;, strerror(errno));
            freeClientAsync(server.master); /* Close ASAP. */
        }
    }
}

```

readQueryFromClient 函数中调用 [processInputBufferAndReplicate](https://github.com/redis/redis/blob/5.0/src/networking.c#L1507)函数，该函数会先调用 [processInputBuffer](https://github.com/redis/redis/blob/5.0/src/networking.c#L1428) 将数据应用到内存中，再调用 [replicationFeedSlavesFromMasterStream](https://github.com/redis/redis/blob/5.0/src/replication.c#L279) 将数据刷到 sub-slaves：

``` C

void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) {
   
    ... ...

    /* Time to process the buffer. If the client is a master we need to
     * compute the difference between the applied offset before and after
     * processing the buffer, to understand how much of the replication stream
     * was actually applied to the master state: this quantity, and its
     * corresponding part of the replication stream, will be propagated to
     * the sub-slaves and to the replication backlog. */
    processInputBufferAndReplicate(c);
}

/* This is a wrapper for processInputBuffer that also cares about handling
 * the replication forwarding to the sub-slaves, in case the client 'c'
 * is flagged as master. Usually you want to call this instead of the
 * raw processInputBuffer(). */
void processInputBufferAndReplicate(client *c) {
    if (!(c-&gt;flags &amp; CLIENT_MASTER)) {
        processInputBuffer(c);
    } else {
        size_t prev_offset = c-&gt;reploff;
        processInputBuffer(c);
        size_t applied = c-&gt;reploff - prev_offset;
        if (applied) {
            replicationFeedSlavesFromMasterStream(server.slaves,
                    c-&gt;pending_querybuf, applied);
            sdsrange(c-&gt;pending_querybuf,applied,-1);
        }
    }
}

/* This function is called every time, in the client structure 'c', there is
 * more query buffer to process, because we read more data from the socket
 * or because a client was blocked and later reactivated, so there could be
 * pending query buffer, already representing a full command, to process. */
void processInputBuffer(client *c) {
   ... ...
}

/* This function is used in order to proxy what we receive from our master
 * to our sub-slaves. */
#include &lt;ctype.h&gt;
void replicationFeedSlavesFromMasterStream(list *slaves, char *buf, size_t buflen) {
    listNode *ln;
    listIter li;

    /* Debugging: this is handy to see the stream sent from master
     * to slaves. Disabled with if(0). */
    if (0) {
        printf(&quot;%zu:&quot;,buflen);
        for (size_t j = 0; j &lt; buflen; j++) {
            printf(&quot;%c&quot;, isprint(buf[j]) ? buf[j] : '.');
        }
        printf(&quot;\n&quot;);
    }

    if (server.repl_backlog) feedReplicationBacklog(buf,buflen);
    listRewind(slaves,&amp;li);
    while((ln = listNext(&amp;li))) {
        client *slave = ln-&gt;value;

        /* Don't feed slaves that are still waiting for BGSAVE to start */
        if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) continue;
        addReplyString(slave,buf,buflen);
    }
}
```

在 replicationFeedSlavesFromMasterStream 中的 addReplyString 函数仍然调用的是 addReply，所以在发生网络错误时采取的逻辑相同。综上，**redis 在主从同步过程中遇到问题时的处理逻辑很简单，一旦发生错误就会直接调用 [freeClient](https://github.com/redis/redis/blob/5.0/src/networking.c#L847) 函数释放掉 client 对象**。

## Slave 增量同步过慢

在主从同步的过程中，可能发生从库同步太慢，重新触发全量同步的情况。repl backlog 的机制已经在上一篇中详细介绍了，那如何判断何时该主动放弃 slave 的异步同步呢？在 redis 中一切机制都按照从简的原则，如上文所说在接收到一个需要同步的命令时，会将其添加到 slave client 的发送缓冲区，当接收到缓冲区超过配置的阈值时，会主动放弃 (调用 free client 释放) slave client。具体实现在 [asyncCloseClientOnOutputBufferLimitReached](https://github.com/redis/redis/blob/5.0/src/networking.c#L2128) 函数，如下：

``` C
/* -----------------------------------------------------------------------------
 * Higher level functions to queue data on the client output buffer.
 * The following functions are the ones that commands implementations will call.
 * -------------------------------------------------------------------------- */

/* Add the object 'obj' string representation to the client output buffer. */
void addReply(client *c, robj *obj){
    ... ...
}

/* The function checks if the client reached output buffer soft or hard
 * limit, and also update the state needed to check the soft limit as
 * a side effect.
 *
 * Return value: non-zero if the client reached the soft or the hard limit.
 *               Otherwise zero is returned. */
int checkClientOutputBufferLimits(client *c) {   
}

/* Asynchronously close a client if soft or hard limit is reached on the
 * output buffer size. The caller can check if the client will be closed
 * checking if the client CLIENT_CLOSE_ASAP flag is set.
 *
 * Note: we need to close the client asynchronously because this function is
 * called from contexts where the client can't be freed safely, i.e. from the
 * lower level functions pushing data inside the client output buffers. */
void asyncCloseClientOnOutputBufferLimitReached(client *c) {
    if (c-&gt;fd == -1) return; /* It is unsafe to free fake clients. */
    serverAssert(c-&gt;reply_bytes &lt; SIZE_MAX-(1024*64));
    if (c-&gt;reply_bytes == 0 || c-&gt;flags &amp; CLIENT_CLOSE_ASAP) return;
    if (checkClientOutputBufferLimits(c)) {
        sds client = catClientInfoString(sdsempty(),c);

        freeClientAsync(c);
        serverLog(LL_WARNING,&quot;Client %s scheduled to be closed ASAP for overcoming of output buffer limits.&quot;, client);
        sdsfree(client);
    }
}
```

## FullResync 过程中 RDB 的生成和传输

本节先提出以下问题，下文会依次解答：

1. 什么情况下会触发 bgSave?
    PSYNC/SYNC command，replicationCron/serverCron，BgSave/BgRewrite command，RDB 或 aof-preamble 日志。
2. RDB 数据是如何生成的？
    首先 Fork 子进程去执行 bgSave 任务，然后根据不同场景决定文件头部的格式，将内存中的数据序列化后写入文件。
3. diskless 和 rdb file 两种模式有什么区别？
    diskless 模式不需要持久化到文件，直接在子进程中通过 socket 传输给 slaves；rdb file 在子进程中持久化到本地，在父进程中发送给 salves。另外，diskless 模式在传输完 rdb 数据后，要等待 slave 返回 replconf ack 后，才能开始增量同步。
4. 多个 slave 和 日志持久化 如何共享同一个 bgSave 任务？
    由 slave 同步请求（PSYNC/SYNC command，slave sync handshake）触发的 bgSave 可以有条件共享，其他情况触发的 bgSave 无法共享。
5. slave 如何加载 rdb file？
    slave 在将 rdb 数据持久化到本地，在完整接收后通过 [rdbLoad](https://github.com/redis/redis/blob/5.0/src/rdb.c#L2151) 的方式加载到内存。

### BgSave 的触发

当一个 slave client 进入 fullsync 流程后会经历如下四个阶段：

+ SLAVE_STATE_WAIT_BGSAVE_START：等待 bgsave 开始
+ SLAVE_STATE_WAIT_BGSAVE_END：等待 bgsave 结束
+ SLAVE_STATE_SEND_BULK：发送 rdb 数据
+ SLAVE_STATE_ONLINE：发送结束，标记为上线

在 redis 中有以下几个场景可能触发 Bgsave ：

+ 执行 PSYNC 和 SYNC command：当不满足部分同步条件时，可能会触发全量同步执行 bgSave；
+ [replicationCron](https://github.com/redis/redis/blob/5.0/src/replication.c#L2578)：每间隔 1s 执行一次，检查当前是否有处于 WAIT_BGSAVE_START 状态的 slave，如果有则开启 bgSave；
+ [serverCron](https://github.com/redis/redis/blob/5.0/src/server.c#L1111)：在 Bgsave 结束时，子进程通过 serverCron 检查执行完 bgSave的结果时可能会再次启动 bgSave 任务(下面会详细分析)；
+ BgSave/BgRewrite 命令：主动调用命令去生成 RDB file
+ 日志配置：配置了 RDB 日志或 aof-preamble 日志

在 PSYNC 和 SYNC command 中会即使不满足全量同步条件也不一定会立即触发 bgSave 命令:

1. 如果当前开启了一个 diskless 的 bgSave 任务，则等待下一轮 bgSave 任务；
2. 如果当前开启了一个 rdb file 的 bgSave 任务，检测当前 slaves 队列中是否有处于 SLAVE_STATE_WAIT_BGSAVE_END 状态的任务，且 slave 的 repl_capa 包含 调用 PSYNC/SYNC 的 client 的 repl_capa，此时 client 附加到当前任务，直接进入 SLAVE_STATE_WAIT_BGSAVE_END 状态；
3. 1 和 2 都不满的情况下，如果是 client 与 server 都支持 diskless，则等待下一轮 bgSave 任务，否则如果当前没有 aof_rewrite 任务则开启一个 bgSave 任务；

具体实现[syncCommand](https://github.com/redis/redis/blob/5.0/src/replication.c#L629)如下：

``` C
/* SYNC and PSYNC command implemenation. */
void syncCommand(client *c) {
   ... ... 

    /* CASE 1: BGSAVE is in progress, with disk target. */
    if (server.rdb_child_pid != -1 &amp;&amp;
        server.rdb_child_type == RDB_CHILD_TYPE_DISK)
    {
        /* Ok a background save is in progress. Let's check if it is a good
         * one for replication, i.e. if there is another slave that is
         * registering differences since the server forked to save. */
        client *slave;
        listNode *ln;
        listIter li;

        listRewind(server.slaves,&amp;li);
        while((ln = listNext(&amp;li))) {
            slave = ln-&gt;value;
            if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_END) break;
        }
        /* To attach this slave, we check that it has at least all the
         * capabilities of the slave that triggered the current BGSAVE. */
        if (ln &amp;&amp; ((c-&gt;slave_capa &amp; slave-&gt;slave_capa) == slave-&gt;slave_capa)) {
            /* Perfect, the server is already registering differences for
             * another slave. Set the right state, and copy the buffer. */
            copyClientOutputBuffer(c,slave);
            replicationSetupSlaveForFullResync(c,slave-&gt;psync_initial_offset);
            serverLog(LL_NOTICE,&quot;Waiting for end of BGSAVE for SYNC&quot;);
        } else {
            /* No way, we need to wait for the next BGSAVE in order to
             * register differences. */
            serverLog(LL_NOTICE,&quot;Can't attach the replica to the current BGSAVE. Waiting for next BGSAVE for SYNC&quot;);
        }

    /* CASE 2: BGSAVE is in progress, with socket target. */
    } else if (server.rdb_child_pid != -1 &amp;&amp;
               server.rdb_child_type == RDB_CHILD_TYPE_SOCKET)
    {
        /* There is an RDB child process but it is writing directly to
         * children sockets. We need to wait for the next BGSAVE
         * in order to synchronize. */
        serverLog(LL_NOTICE,&quot;Current BGSAVE has socket target. Waiting for next BGSAVE for SYNC&quot;);

    /* CASE 3: There is no BGSAVE is progress. */
    } else {
        if (server.repl_diskless_sync &amp;&amp; (c-&gt;slave_capa &amp; SLAVE_CAPA_EOF)) {
            /* Diskless replication RDB child is created inside
             * replicationCron() since we want to delay its start a
             * few seconds to wait for more slaves to arrive. */
            if (server.repl_diskless_sync_delay)
                serverLog(LL_NOTICE,&quot;Delay next BGSAVE for diskless SYNC&quot;);
        } else {
            /* Target is disk (or the slave is not capable of supporting
             * diskless replication) and we don't have a BGSAVE in progress,
             * let's start one. */
            if (server.aof_child_pid == -1) {
                startBgsaveForReplication(c-&gt;slave_capa);
            } else {
                serverLog(LL_NOTICE,
                    &quot;No BGSAVE in progress, but an AOF rewrite is active. &quot;
                    &quot;BGSAVE for replication delayed&quot;);
            }
        }
    }
    return;
}
```

这里讲解一下[replicationSetupSlaveForFullResync](https://github.com/redis/redis/blob/5.0/src/replication.c#L419)函数，该函数将 client-&gt;replstate 设置为 SLAVE_STATE_WAIT_BGSAVE_END 状态，然后会向 client 发送`+FULLRESYNC repli offset` 命令，在下文中的启动 bgSave 流程中也会调用该函数，具体实现如下：

``` C

/* Send a FULLRESYNC reply in the specific case of a full resynchronization,
 * as a side effect setup the slave for a full sync in different ways:
 *
 * 1) Remember, into the slave client structure, the replication offset
 *    we sent here, so that if new slaves will later attach to the same
 *    background RDB saving process (by duplicating this client output
 *    buffer), we can get the right offset from this slave.
 * 2) Set the replication state of the slave to WAIT_BGSAVE_END so that
 *    we start accumulating differences from this point.
 * 3) Force the replication stream to re-emit a SELECT statement so
 *    the new slave incremental differences will start selecting the
 *    right database number.
 *
 * Normally this function should be called immediately after a successful
 * BGSAVE for replication was started, or when there is one already in
 * progress that we attached our slave to. */
int replicationSetupSlaveForFullResync(client *slave, long long offset) {
    char buf[128];
    int buflen;

    slave-&gt;psync_initial_offset = offset;
    slave-&gt;replstate = SLAVE_STATE_WAIT_BGSAVE_END;
    /* We are going to accumulate the incremental changes for this
     * slave as well. Set slaveseldb to -1 in order to force to re-emit
     * a SELECT statement in the replication stream. */
    server.slaveseldb = -1;

    /* Don't send this reply to slaves that approached us with
     * the old SYNC command. */
    if (!(slave-&gt;flags &amp; CLIENT_PRE_PSYNC)) {
        buflen = snprintf(buf,sizeof(buf),&quot;+FULLRESYNC %s %lld\r\n&quot;,
                          server.replid,offset);
        if (write(slave-&gt;fd,buf,buflen) != buflen) {
            freeClientAsync(slave);
            return C_ERR;
        }
    }
    return C_OK;
```

replicationCron 作为处理主从复制的核心函数是在 serverCron 中被调用的，它里面包含了几乎所有处理主从复制的操作：

``` mermaid!
flowchart TB
    start([start])
    exit([end])
    cancelReplHandshake[cancel replication hand shake &lt;br&gt; if hand shake timeout.]
    cancelReplHandshake1[cancel replication hand shake &lt;br&gt; if bulk transfer I/O timeout.]
    freeMaster[free the master client &lt;br&gt; if the client is timeout.]
    connMaster[connect to master if the node is slave &lt;br&gt; in REPL_STATE_CONNECT state.]
    sendAck[send repl ack if the node is slave &lt;br&gt; and master supports PSYNC.]
    pingSubSlaves[ping slaves &lt;br&gt; if there are sub slaves.]
    sendNewLine[send newline char to no-diskless slaves &lt;br&gt; waiting bgsave.]
    disconnTimeoutSlaves[Disconnect timedout slaves.]
    cleanBacklog[&quot;free the back log after some (configured) time &lt;br&gt; if the node is master without slaves.&quot;]
    flushScriptCache[flush scripts cache if the node &lt;br&gt; disabled aof is without slaves.]
    startBgSave[Start a BGSAVE good for replication &lt;br&gt; if there are slaves in WAIT_BGSAVE_START state.]
    refreshSlaves[Refresh the number of slaves &lt;br&gt; with lag &lt;= min-slaves-max-lag]
    start--&gt;cancelReplHandshake--&gt;cancelReplHandshake1--&gt;freeMaster
    freeMaster--&gt;connMaster--&gt;sendAck--&gt;pingSubSlaves--&gt;sendNewLine
    sendNewLine--&gt;disconnTimeoutSlaves--&gt;cleanBacklog--&gt;flushScriptCache 
    flushScriptCache--&gt;startBgSave--&gt;refreshSlaves--&gt;exit
```

其中还有很多细节，不在这里展开了，本文主要关注 bgSave 的部分，它会在没有开启 bgSave 和 aofSave 子进程的前提下，检查当前所有 slave 的 capability，找到它们的 minicapa（取交集）。当开启 diskless 时会等待一段时间（`repl_diskless_sync_delay`），然后开启 bgSave 任务：

``` C
/* --------------------------- REPLICATION CRON  ---------------------------- */

/* Replication cron function, called 1 time per second. */
void replicationCron(void) {
    ... ...
      /* Start a BGSAVE good for replication if we have slaves in
     * WAIT_BGSAVE_START state.
     *
     * In case of diskless replication, we make sure to wait the specified
     * number of seconds (according to configuration) so that other slaves
     * have the time to arrive before we start streaming. */
    if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1) {
        time_t idle, max_idle = 0;
        int slaves_waiting = 0;
        int mincapa = -1;
        listNode *ln;
        listIter li;

        listRewind(server.slaves,&amp;li);
        while((ln = listNext(&amp;li))) {
            client *slave = ln-&gt;value;
            if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
                idle = server.unixtime - slave-&gt;lastinteraction;
                if (idle &gt; max_idle) max_idle = idle;
                slaves_waiting++;
                mincapa = (mincapa == -1) ? slave-&gt;slave_capa :
                                            (mincapa &amp; slave-&gt;slave_capa);
            }
        }

        if (slaves_waiting &amp;&amp;
            (!server.repl_diskless_sync ||
             max_idle &gt; server.repl_diskless_sync_delay))
        {
            /* Start the BGSAVE. The called function may start a
             * BGSAVE with socket target or disk target depending on the
             * configuration and slaves capabilities. */
            startBgsaveForReplication(mincapa);
        }
    }
    ... ...
}
```

serverCron 中调用的链路比较复杂，下面会讲到，简单的来讲是在检查 bgSave 子进程返回结果时，查看当前是否有需要 bgSave 的 slave，如果有则会开启新一轮 bgSave。至此回答了第一个问题。

### RDB 数据的生成

RDB 数据的字段格式依次如下：

+ MAGIC(9 byte): REDIS$RDB_VESION，RDB_VERSION format %04d，例如 0009
+ InfoAUXFileds: 包含字段依次如下
    - redis-ver[all]: REDIS version，例如 5.0.12
    - redis-bits[all]: redis 所在的主机位数，例如 32
    - ctime[all]: rdb 创建的时间
    - used-mem[all]: 当前所存储数据使用内存的大小，在restore的时候可以依据该字段提前分配内存
    - repl-stream-db[bgSave]: 当前的 select db，保证在全量同步完成后都可以切换到同一个db
    - repl-id[bgSave]: 当前的 repl-id
    - repl-offset[bgSave]: 当前的 repl-offset
    - aof-preamble[aof]: 在开启 aof preamble 功能时，会重写 aof 日志到 rdb file，此时会携带该标致
+ []ModuleAUX: redis 允许用户加载自定义数据模块，这些支持aux_save 方法且开启 aux_save_tiggers 的模块信息（名字、版本号、模块自定义AUX信息等）也要保存在 RDB file中，这样才能保证在加载自定义数据时找到对应的 module
+ []DBdata: 包含字段依次如下
    - DB_NUM: 数据库id
    - DB_SIZE: 数据大小
    - EXPIRES_SIZE: 数据过期时间集合大小
    - []KEY: key数据，包括 key、value、expire
+ []Script: Script数据，与AuxFileds格式相同，key 为 lua、filed 为 script data，即会有多个 lua 字段
+ []ModuleAUX: 支持 aux_save 方法但没有开启 aux_save_tiggers 的模块信息
+ CheckSum(8 byte): 校验码

RDB 中还有很多种类的 OPCODE 用来标识数据类型，除了MAGIC 字段外，每一个字段都是由：`OPCODE + LENGTH + DATA` 格式组成的，这里不详细展开，其核心函数为[rdbSaveRio]()：

``` C
/* Produces a dump of the database in RDB format sending it to the specified
 * Redis I/O channel. On success C_OK is returned, otherwise C_ERR
 * is returned and part of the output, or all the output, can be
 * missing because of I/O errors.
 *
 * When the function returns C_ERR and if 'error' is not NULL, the
 * integer pointed by 'error' is set to the value of errno just after the I/O
 * error. */
int rdbSaveRio(rio *rdb, int *error, int flags, rdbSaveInfo *rsi) {
    dictIterator *di = NULL;
    dictEntry *de;
    char magic[10];
    int j;
    uint64_t cksum;
    size_t processed = 0;

    if (server.rdb_checksum)
        rdb-&gt;update_cksum = rioGenericUpdateChecksum;
    snprintf(magic,sizeof(magic),&quot;REDIS%04d&quot;,RDB_VERSION);
    if (rdbWriteRaw(rdb,magic,9) == -1) goto werr;
    if (rdbSaveInfoAuxFields(rdb,flags,rsi) == -1) goto werr;
    if (rdbSaveModulesAux(rdb, REDISMODULE_AUX_BEFORE_RDB) == -1) goto werr;

    for (j = 0; j &lt; server.dbnum; j++) {
        redisDb *db = server.db+j;
        dict *d = db-&gt;dict;
        if (dictSize(d) == 0) continue;
        di = dictGetSafeIterator(d);

        /* Write the SELECT DB opcode */
        if (rdbSaveType(rdb,RDB_OPCODE_SELECTDB) == -1) goto werr;
        if (rdbSaveLen(rdb,j) == -1) goto werr;

        /* Write the RESIZE DB opcode. We trim the size to UINT32_MAX, which
         * is currently the largest type we are able to represent in RDB sizes.
         * However this does not limit the actual size of the DB to load since
         * these sizes are just hints to resize the hash tables. */
        uint64_t db_size, expires_size;
        db_size = dictSize(db-&gt;dict);
        expires_size = dictSize(db-&gt;expires);
        if (rdbSaveType(rdb,RDB_OPCODE_RESIZEDB) == -1) goto werr;
        if (rdbSaveLen(rdb,db_size) == -1) goto werr;
        if (rdbSaveLen(rdb,expires_size) == -1) goto werr;

        /* Iterate this DB writing every entry */
        while((de = dictNext(di)) != NULL) {
            sds keystr = dictGetKey(de);
            robj key, *o = dictGetVal(de);
            long long expire;

            initStaticStringObject(key,keystr);
            expire = getExpire(db,&amp;key);
            if (rdbSaveKeyValuePair(rdb,&amp;key,o,expire) == -1) goto werr;

            /* When this RDB is produced as part of an AOF rewrite, move
             * accumulated diff from parent to child while rewriting in
             * order to have a smaller final write. */
            if (flags &amp; RDB_SAVE_AOF_PREAMBLE &amp;&amp;
                rdb-&gt;processed_bytes &gt; processed+AOF_READ_DIFF_INTERVAL_BYTES)
            {
                processed = rdb-&gt;processed_bytes;
                aofReadDiffFromParent();
            }
        }
        dictReleaseIterator(di);
        di = NULL; /* So that we don't release it again on error. */
    }

    /* If we are storing the replication information on disk, persist
     * the script cache as well: on successful PSYNC after a restart, we need
     * to be able to process any EVALSHA inside the replication backlog the
     * master will send us. */
    if (rsi &amp;&amp; dictSize(server.lua_scripts)) {
        di = dictGetIterator(server.lua_scripts);
        while((de = dictNext(di)) != NULL) {
            robj *body = dictGetVal(de);
            if (rdbSaveAuxField(rdb,&quot;lua&quot;,3,body-&gt;ptr,sdslen(body-&gt;ptr)) == -1)
                goto werr;
        }
        dictReleaseIterator(di);
        di = NULL; /* So that we don't release it again on error. */
    }

    if (rdbSaveModulesAux(rdb, REDISMODULE_AUX_AFTER_RDB) == -1) goto werr;

    /* EOF opcode */
    if (rdbSaveType(rdb,RDB_OPCODE_EOF) == -1) goto werr;

    /* CRC64 checksum. It will be zero if checksum computation is disabled, the
     * loading code skips the check in this case. */
    cksum = rdb-&gt;cksum;
    memrev64ifbe(&amp;cksum);
    if (rioWrite(rdb,&amp;cksum,8) == 0) goto werr;
    return C_OK;

werr:
    if (error) *error = errno;
    if (di) dictReleaseIterator(di);
    return C_ERR;
}
```

至此解答了上述的第二个问题。

### Diskless 和 RDB file

在 2.6.0 版本后 redis 引入了 diskless（详情见[上一篇文章](https://saffraan.github.io/redis_psync_protocol/))后，支持两种 RDB 传输模式：

+ diskless: 先将 eofmark（40个字节的HEX char随机字符串） 作为 replpreamble configuration 发送给 slave，然后传输 RDB data，当再次收到 eofmark 时意味着传输终止；
    +FULLRESYNC replid offset
    \$EOF: \$eofmark\r\n
    RDB data
    $eofmark
+ rdbfile: 先将 file length 作为 replpreamble configuration 发送给 slave，然后传输 RDB data，当接收到最够的数据后传输终止；
    +FULLRESYNC replid offset
    $\&lt;length\&gt;\r\n
    RDB data

[startBgsaveForReplication](https://github.com/redis/redis/blob/5.0/src/replication.c#L564) 是 bgsave 开启的核心函数，在开启后fork() 创建一个子进程去执行 bgsave 的任务，父进程通过 pipe 来接收子进程输出的消息，实现进程间通信。针对于满足 diskless 情况会调用[rdbSaveToSlavesSockets](https://github.com/redis/redis/blob/5.0/src/rdb.c#L2312) 函数，否则使用[rdbSaveBackground](https://github.com/redis/redis/blob/5.0/src/rdb.c#L1328) 函数：

``` mermaid!
graph TB
    start([begin])
    exit([end])

    diskless{save to sockets?}
    saveToss[rdbSaveToSlavesSockets]
    saveTof[rdbSaveBackground]
    OK{return ok?}
    TerminalSync[set replstate REPL_STATE_NONE,&lt;br&gt; close client after reply.]

    isNotDiskless{save to file?}
    setupFullsync[setup the salves for a full resync.]
    flushScript[flush the script cache.]

    start--&gt;diskless
    diskless--&gt;|Yes|saveToss--&gt;OK
    diskless--&gt;|No|saveTof--&gt;OK
    OK--&gt;|No|TerminalSync--&gt;exit
    OK--&gt;|Yes|isNotDiskless
    isNotDiskless--&gt;|Yes|setupFullsync--&gt;flushScript
    isNotDiskless--&gt;|No|flushScript
    flushScript--&gt;exit 
```


具体实现如下：

``` C
/* Start a BGSAVE for replication goals, which is, selecting the disk or
 * socket target depending on the configuration, and making sure that
 * the script cache is flushed before to start.
 *
 * The mincapa argument is the bitwise AND among all the slaves capabilities
 * of the slaves waiting for this BGSAVE, so represents the slave capabilities
 * all the slaves support. Can be tested via SLAVE_CAPA_* macros.
 *
 * Side effects, other than starting a BGSAVE:
 *
 * 1) Handle the slaves in WAIT_START state, by preparing them for a full
 *    sync if the BGSAVE was successfully started, or sending them an error
 *    and dropping them from the list of slaves.
 *
 * 2) Flush the Lua scripting script cache if the BGSAVE was actually
 *    started.
 *
 * Returns C_OK on success or C_ERR otherwise. */
int startBgsaveForReplication(int mincapa) {
    int retval;
    int socket_target = server.repl_diskless_sync &amp;&amp; (mincapa &amp; SLAVE_CAPA_EOF);
    listIter li;
    listNode *ln;

    serverLog(LL_NOTICE,&quot;Starting BGSAVE for SYNC with target: %s&quot;,
        socket_target ? &quot;replicas sockets&quot; : &quot;disk&quot;);

    rdbSaveInfo rsi, *rsiptr;
    rsiptr = rdbPopulateSaveInfo(&amp;rsi);
    /* Only do rdbSave* when rsiptr is not NULL,
     * otherwise slave will miss repl-stream-db. */
    if (rsiptr) {
        if (socket_target)
            retval = rdbSaveToSlavesSockets(rsiptr);
        else
            retval = rdbSaveBackground(server.rdb_filename,rsiptr);
    } else {
        serverLog(LL_WARNING,&quot;BGSAVE for replication: replication information not available, can't generate the RDB file right now. Try later.&quot;);
        retval = C_ERR;
    }

    /* If we failed to BGSAVE, remove the slaves waiting for a full
     * resynchorinization from the list of salves, inform them with
     * an error about what happened, close the connection ASAP. */
    if (retval == C_ERR) {
        serverLog(LL_WARNING,&quot;BGSAVE for replication failed&quot;);
        listRewind(server.slaves,&amp;li);
        while((ln = listNext(&amp;li))) {
            client *slave = ln-&gt;value;

            if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
                slave-&gt;replstate = REPL_STATE_NONE;
                slave-&gt;flags &amp;= ~CLIENT_SLAVE;
                listDelNode(server.slaves,ln);
                addReplyError(slave,
                    &quot;BGSAVE failed, replication can't continue&quot;);
                slave-&gt;flags |= CLIENT_CLOSE_AFTER_REPLY;
            }
        }
        return retval;
    }

    /* If the target is socket, rdbSaveToSlavesSockets() already setup
     * the salves for a full resync. Otherwise for disk target do it now.*/
    if (!socket_target) {
        listRewind(server.slaves,&amp;li);
        while((ln = listNext(&amp;li))) {
            client *slave = ln-&gt;value;

            if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
                    replicationSetupSlaveForFullResync(slave,
                            getPsyncInitialOffset());
            }
        }
    }

    /* Flush the script cache, since we need that slave differences are
     * accumulated without requiring slaves to match our cached scripts. */
    if (retval == C_OK) replicationScriptCacheFlush();
    return retval;
}
```

rdbSaveToSlavesSockets 里面会调用 [replicationSetupSlaveForFullResync](https://github.com/redis/redis/blob/5.0/src/replication.c#L419) 函数，所以无需在外部再次调用，在 fork 子进程后，父进程函数会立即返回，不会阻塞父进程的执行：

``` mermaid!
graph TB
    start([begin]);exit([end]);
    createPipe[create pipes.];
    setupFullsync[setup the salves for a full resync.]
    fork[fork a child process.]
    saveDataToss[[save data to sockets.]]
    return[write results into pipe.]
    start--&gt;createPipe--&gt;setupFullsync--&gt;fork
    fork-.-&gt;|Child|saveDataToss--&gt;return-.-&gt;exit
    fork--&gt;|Parent|exit
```

具体实现如下：

``` C
/* Spawn an RDB child that writes the RDB to the sockets of the slaves
 * that are currently in SLAVE_STATE_WAIT_BGSAVE_START state. */
int rdbSaveToSlavesSockets(rdbSaveInfo *rsi) {
    int *fds;
    uint64_t *clientids;
    int numfds;
    listNode *ln;
    listIter li;
    pid_t childpid;
    long long start;
    int pipefds[2];

    if (server.aof_child_pid != -1 || server.rdb_child_pid != -1) return C_ERR;

    /* Before to fork, create a pipe that will be used in order to
     * send back to the parent the IDs of the slaves that successfully
     * received all the writes. */
    if (pipe(pipefds) == -1) return C_ERR;
    server.rdb_pipe_read_result_from_child = pipefds[0];
    server.rdb_pipe_write_result_to_parent = pipefds[1];

    /* Collect the file descriptors of the slaves we want to transfer
     * the RDB to, which are i WAIT_BGSAVE_START state. */
    fds = zmalloc(sizeof(int)*listLength(server.slaves));
    /* We also allocate an array of corresponding client IDs. This will
     * be useful for the child process in order to build the report
     * (sent via unix pipe) that will be sent to the parent. */
    clientids = zmalloc(sizeof(uint64_t)*listLength(server.slaves));
    numfds = 0;

    listRewind(server.slaves,&amp;li);
    while((ln = listNext(&amp;li))) {
        client *slave = ln-&gt;value;

        if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
            clientids[numfds] = slave-&gt;id;
            fds[numfds++] = slave-&gt;fd;
            replicationSetupSlaveForFullResync(slave,getPsyncInitialOffset());
            /* Put the socket in blocking mode to simplify RDB transfer.
             * We'll restore it when the children returns (since duped socket
             * will share the O_NONBLOCK attribute with the parent). */
            anetBlock(NULL,slave-&gt;fd);
            anetSendTimeout(NULL,slave-&gt;fd,server.repl_timeout*1000);
        }
    }

    /* Create the child process. */
    openChildInfoPipe();
    start = ustime();
    if ((childpid = fork()) == 0) {
        /* Child */
        int retval;
        rio slave_sockets;

        rioInitWithFdset(&amp;slave_sockets,fds,numfds);
        zfree(fds);

        closeClildUnusedResourceAfterFork();
        redisSetProcTitle(&quot;redis-rdb-to-slaves&quot;);

        retval = rdbSaveRioWithEOFMark(&amp;slave_sockets,NULL,rsi);
        if (retval == C_OK &amp;&amp; rioFlush(&amp;slave_sockets) == 0)
            retval = C_ERR;

        if (retval == C_OK) {
            size_t private_dirty = zmalloc_get_private_dirty(-1);

            if (private_dirty) {
                serverLog(LL_NOTICE,
                    &quot;RDB: %zu MB of memory used by copy-on-write&quot;,
                    private_dirty/(1024*1024));
            }

            server.child_info_data.cow_size = private_dirty;
            sendChildInfo(CHILD_INFO_TYPE_RDB);

            /* If we are returning OK, at least one slave was served
             * with the RDB file as expected, so we need to send a report
             * to the parent via the pipe. The format of the message is:
             *
             * &lt;len&gt; &lt;slave[0].id&gt; &lt;slave[0].error&gt; ...
             *
             * len, slave IDs, and slave errors, are all uint64_t integers,
             * so basically the reply is composed of 64 bits for the len field
             * plus 2 additional 64 bit integers for each entry, for a total
             * of 'len' entries.
             *
             * The 'id' represents the slave's client ID, so that the master
             * can match the report with a specific slave, and 'error' is
             * set to 0 if the replication process terminated with a success
             * or the error code if an error occurred. */
            void *msg = zmalloc(sizeof(uint64_t)*(1+2*numfds));
            uint64_t *len = msg;
            uint64_t *ids = len+1;
            int j, msglen;

            *len = numfds;
            for (j = 0; j &lt; numfds; j++) {
                *ids++ = clientids[j];
                *ids++ = slave_sockets.io.fdset.state[j];
            }

            /* Write the message to the parent. If we have no good slaves or
             * we are unable to transfer the message to the parent, we exit
             * with an error so that the parent will abort the replication
             * process with all the childre that were waiting. */
            msglen = sizeof(uint64_t)*(1+2*numfds);
            if (*len == 0 ||
                write(server.rdb_pipe_write_result_to_parent,msg,msglen)
                != msglen)
            {
                retval = C_ERR;
            }
            zfree(msg);
        }
        zfree(clientids);
        rioFreeFdset(&amp;slave_sockets);
        exitFromChild((retval == C_OK) ? 0 : 1);
    } else {
        /* Parent */
        if (childpid == -1) {
            serverLog(LL_WARNING,&quot;Can't save in background: fork: %s&quot;,
                strerror(errno));

            /* Undo the state change. The caller will perform cleanup on
             * all the slaves in BGSAVE_START state, but an early call to
             * replicationSetupSlaveForFullResync() turned it into BGSAVE_END */
            listRewind(server.slaves,&amp;li);
            while((ln = listNext(&amp;li))) {
                client *slave = ln-&gt;value;
                int j;

                for (j = 0; j &lt; numfds; j++) {
                    if (slave-&gt;id == clientids[j]) {
                        slave-&gt;replstate = SLAVE_STATE_WAIT_BGSAVE_START;
                        break;
                    }
                }
            }
            close(pipefds[0]);
            close(pipefds[1]);
            closeChildInfoPipe();
        } else {
            server.stat_fork_time = ustime()-start;
            server.stat_fork_rate = (double) zmalloc_used_memory() * 1000000 / server.stat_fork_time / (1024*1024*1024); /* GB per second. */
            latencyAddSampleIfNeeded(&quot;fork&quot;,server.stat_fork_time/1000);

            serverLog(LL_NOTICE,&quot;Background RDB transfer started by pid %d&quot;,
                childpid);
            server.rdb_save_time_start = time(NULL);
            server.rdb_child_pid = childpid;
            server.rdb_child_type = RDB_CHILD_TYPE_SOCKET;
            updateDictResizePolicy();
        }
        zfree(clientids);
        zfree(fds);
        return (childpid == -1) ? C_ERR : C_OK;
    }
    return C_OK; /* Unreached. */
}


/* This is just a wrapper to rdbSaveRio() that additionally adds a prefix
 * and a suffix to the generated RDB dump. The prefix is:
 *
 * $EOF:&lt;40 bytes unguessable hex string&gt;\r\n
 *
 * While the suffix is the 40 bytes hex string we announced in the prefix.
 * This way processes receiving the payload can understand when it ends
 * without doing any processing of the content. */
int rdbSaveRioWithEOFMark(rio *rdb, int *error, rdbSaveInfo *rsi) {
    char eofmark[RDB_EOF_MARK_SIZE];

    getRandomHexChars(eofmark,RDB_EOF_MARK_SIZE);
    if (error) *error = 0;
    if (rioWrite(rdb,&quot;$EOF:&quot;,5) == 0) goto werr;
    if (rioWrite(rdb,eofmark,RDB_EOF_MARK_SIZE) == 0) goto werr;
    if (rioWrite(rdb,&quot;\r\n&quot;,2) == 0) goto werr;
    if (rdbSaveRio(rdb,error,RDB_SAVE_NONE,rsi) == C_ERR) goto werr;
    if (rioWrite(rdb,eofmark,RDB_EOF_MARK_SIZE) == 0) goto werr;
    return C_OK;

werr: /* Write error. */
    /* Set 'error' only if not already set by rdbSaveRio() call. */
    if (error &amp;&amp; *error == 0) *error = errno;
    return C_ERR;
}

/* Produces a dump of the database in RDB format sending it to the specified
 * Redis I/O channel. On success C_OK is returned, otherwise C_ERR
 * is returned and part of the output, or all the output, can be
 * missing because of I/O errors.
 *
 * When the function returns C_ERR and if 'error' is not NULL, the
 * integer pointed by 'error' is set to the value of errno just after the I/O
 * error. */
int rdbSaveRio(rio *rdb, int *error, int flags, rdbSaveInfo *rsi) {
    ... ...
}
```

rdbSaveBackground 也会 fork 一个子进程，在子进程中执行写 RDB 文件的操作，主进程的函数也会立即返回，不过与rdbSaveToSlavesSockets 不同的是数据的传输不是在子进程内：

``` mermaid!
graph TB
    start([start]);exit([end])
    fork[fork a child process.]
    return[write results into pipe]
    savefile[save data into rdb file.]
    start--&gt;fork--&gt;|Parent|exit
    fork-.-&gt;|Child|savefile--&gt;return-.-&gt;exit
```

具体实现如下：

``` C
int rdbSaveBackground(char *filename, rdbSaveInfo *rsi) {
    pid_t childpid;
    long long start;

    if (server.aof_child_pid != -1 || server.rdb_child_pid != -1) return C_ERR;

    server.dirty_before_bgsave = server.dirty;
    server.lastbgsave_try = time(NULL);
    openChildInfoPipe();

    start = ustime();
    if ((childpid = fork()) == 0) {
        int retval;

        /* Child */
        closeClildUnusedResourceAfterFork();
        redisSetProcTitle(&quot;redis-rdb-bgsave&quot;);
        retval = rdbSave(filename,rsi);
        if (retval == C_OK) {
            size_t private_dirty = zmalloc_get_private_dirty(-1);

            if (private_dirty) {
                serverLog(LL_NOTICE,
                    &quot;RDB: %zu MB of memory used by copy-on-write&quot;,
                    private_dirty/(1024*1024));
            }

            server.child_info_data.cow_size = private_dirty;
            sendChildInfo(CHILD_INFO_TYPE_RDB);
        }
        exitFromChild((retval == C_OK) ? 0 : 1);
    } else {
        /* Parent */
        server.stat_fork_time = ustime()-start;
        server.stat_fork_rate = (double) zmalloc_used_memory() * 1000000 / server.stat_fork_time / (1024*1024*1024); /* GB per second. */
        latencyAddSampleIfNeeded(&quot;fork&quot;,server.stat_fork_time/1000);
        if (childpid == -1) {
            closeChildInfoPipe();
            server.lastbgsave_status = C_ERR;
            serverLog(LL_WARNING,&quot;Can't save in background: fork: %s&quot;,
                strerror(errno));
            return C_ERR;
        }
        serverLog(LL_NOTICE,&quot;Background saving started by pid %d&quot;,childpid);
        server.rdb_save_time_start = time(NULL);
        server.rdb_child_pid = childpid;
        server.rdb_child_type = RDB_CHILD_TYPE_DISK;
        updateDictResizePolicy();
        return C_OK;
    }
    return C_OK; /* unreached */
}
```

在 `save to file` 的情况下，数据最后发送的操作是在主进程完成的，它的触发时机是在 [serverCron](https://github.com/redis/redis/blob/5.0/src/server.c#L1111) 处理 bgSaveDone 的时候，主要实现函数为 [backgroundSaveDoneHandlerDisk](https://github.com/redis/redis/blob/5.0/src/rdb.c#L2167) 和 [backgroundSaveDoneHandlerSocket](https://github.com/redis/redis/blob/5.0/src/rdb.c#L2203)，其中  backgroundSaveDoneHandlerDisk 包含 RDB file 数据发送的核心逻辑，它调用 [updateSlavesWaitingBgsave](https://github.com/redis/redis/blob/5.0/src/replication.c#L946) 函数将 [sendBulkToSlave](https://github.com/redis/redis/blob/5.0/src/replication.c#L876)函数绑定到 `slave-&gt;fd`的可写入事件上。

``` C
/* This is our timer interrupt, called server.hz times per second.
 * Here is where we do a number of things that need to be done asynchronously.
 * For instance:
 *
 * - Active expired keys collection (it is also performed in a lazy way on
 *   lookup).
 * - Software watchdog.
 * - Update some statistic.
 * - Incremental rehashing of the DBs hash tables.
 * - Triggering BGSAVE / AOF rewrite, and handling of terminated children.
 * - Clients timeout of different kinds.
 * - Replication reconnection.
 * - Many more...
 *
 * Everything directly called here will be called server.hz times per second,
 * so in order to throttle execution of things we want to do less frequently
 * a macro is used: run_with_period(milliseconds) { .... }
 */

int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {
    ... ...

    /* Check if a background saving or AOF rewrite in progress terminated. */
    if (server.rdb_child_pid != -1 || server.aof_child_pid != -1 ||
        ldbPendingChildren())
    {
        int statloc;
        pid_t pid;

        if ((pid = wait3(&amp;statloc,WNOHANG,NULL)) != 0) {
            int exitcode = WEXITSTATUS(statloc);
            int bysignal = 0;

            if (WIFSIGNALED(statloc)) bysignal = WTERMSIG(statloc);

            if (pid == -1) {
                serverLog(LL_WARNING,&quot;wait3() returned an error: %s. &quot;
                    &quot;rdb_child_pid = %d, aof_child_pid = %d&quot;,
                    strerror(errno),
                    (int) server.rdb_child_pid,
                    (int) server.aof_child_pid);
            } else if (pid == server.rdb_child_pid) {
                backgroundSaveDoneHandler(exitcode,bysignal);
                if (!bysignal &amp;&amp; exitcode == 0) receiveChildInfo();
            } else if (pid == server.aof_child_pid) {
                backgroundRewriteDoneHandler(exitcode,bysignal);
                if (!bysignal &amp;&amp; exitcode == 0) receiveChildInfo();
            } else {
                if (!ldbRemoveChild(pid)) {
                    serverLog(LL_WARNING,
                        &quot;Warning, detected child with unmatched pid: %ld&quot;,
                        (long)pid);
                }
            }
            updateDictResizePolicy();
            closeChildInfoPipe();
        }
    } else {
        /* If there is not a background saving/rewrite in progress check if
         * we have to save/rewrite now. */
        ... ... 
        /* Trigger an AOF rewrite if needed. */
        ... ...
    }
  ... ... 
}


/* A background saving child (BGSAVE) terminated its work. Handle this.
 * This function covers the case of actual BGSAVEs. */
void backgroundSaveDoneHandlerDisk(int exitcode, int bysignal) {
    ... ... 
    /* Possibly there are slaves waiting for a BGSAVE in order to be served
     * (the first stage of SYNC is a bulk transfer of dump.rdb) */
    updateSlavesWaitingBgsave((!bysignal &amp;&amp; exitcode == 0) ? C_OK : C_ERR, RDB_CHILD_TYPE_DISK);
}

/* When a background RDB saving/transfer terminates, call the right handler. */
void backgroundSaveDoneHandler(int exitcode, int bysignal) {
    switch(server.rdb_child_type) {
    case RDB_CHILD_TYPE_DISK:
        backgroundSaveDoneHandlerDisk(exitcode,bysignal);
        break;
    case RDB_CHILD_TYPE_SOCKET:
        backgroundSaveDoneHandlerSocket(exitcode,bysignal);
        break;
    default:
        serverPanic(&quot;Unknown RDB child type.&quot;);
        break;
    }
}


/* This function is called at the end of every background saving,
 * or when the replication RDB transfer strategy is modified from
 * disk to socket or the other way around.
 *
 * The goal of this function is to handle slaves waiting for a successful
 * background saving in order to perform non-blocking synchronization, and
 * to schedule a new BGSAVE if there are slaves that attached while a
 * BGSAVE was in progress, but it was not a good one for replication (no
 * other slave was accumulating differences).
 *
 * The argument bgsaveerr is C_OK if the background saving succeeded
 * otherwise C_ERR is passed to the function.
 * The 'type' argument is the type of the child that terminated
 * (if it had a disk or socket target). */
void updateSlavesWaitingBgsave(int bgsaveerr, int type) {
    listNode *ln;
    int startbgsave = 0;
    int mincapa = -1;
    listIter li;

    listRewind(server.slaves,&amp;li);
    while((ln = listNext(&amp;li))) {
        client *slave = ln-&gt;value;

        if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
            ... ...
        } else if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_END) {
           ... ...
        } else {
            if (bgsaveerr != C_OK) {
                freeClient(slave);
                serverLog(LL_WARNING,&quot;SYNC failed. BGSAVE child returned an error&quot;);
                continue;
            }
            if ((slave-&gt;repldbfd = open(server.rdb_filename,O_RDONLY)) == -1 ||
                redis_fstat(slave-&gt;repldbfd,&amp;buf) == -1) {
                freeClient(slave);
                serverLog(LL_WARNING,&quot;SYNC failed. Can't open/stat DB after BGSAVE: %s&quot;, strerror(errno));
                continue;
            }
            slave-&gt;repldboff = 0;
            slave-&gt;repldbsize = buf.st_size;
            slave-&gt;replstate = SLAVE_STATE_SEND_BULK;
            slave-&gt;replpreamble = sdscatprintf(sdsempty(),&quot;$%lld\r\n&quot;,
                (unsigned long long) slave-&gt;repldbsize);

            aeDeleteFileEvent(server.el,slave-&gt;fd,AE_WRITABLE);
            if (aeCreateFileEvent(server.el, slave-&gt;fd, AE_WRITABLE, sendBulkToSlave, slave) == AE_ERR) {
                freeClient(slave);
                continue;
            }
        }
    }
    ... ...
}


void sendBulkToSlave(aeEventLoop *el, int fd, void *privdata, int mask) {
    client *slave = privdata;
    UNUSED(el);
    UNUSED(mask);
    char buf[PROTO_IOBUF_LEN];
    ssize_t nwritten, buflen;

    /* Before sending the RDB file, we send the preamble as configured by the
     * replication process. Currently the preamble is just the bulk count of
     * the file in the form &quot;$&lt;length&gt;\r\n&quot;. */
    if (slave-&gt;replpreamble) {
        nwritten = write(fd,slave-&gt;replpreamble,sdslen(slave-&gt;replpreamble));
        if (nwritten == -1) {
            serverLog(LL_VERBOSE,&quot;Write error sending RDB preamble to replica: %s&quot;,
                strerror(errno));
            freeClient(slave);
            return;
        }
        server.stat_net_output_bytes += nwritten;
        sdsrange(slave-&gt;replpreamble,nwritten,-1);
        if (sdslen(slave-&gt;replpreamble) == 0) {
            sdsfree(slave-&gt;replpreamble);
            slave-&gt;replpreamble = NULL;
            /* fall through sending data. */
        } else {
            return;
        }
    }

    /* If the preamble was already transferred, send the RDB bulk data. */
    lseek(slave-&gt;repldbfd,slave-&gt;repldboff,SEEK_SET);
    buflen = read(slave-&gt;repldbfd,buf,PROTO_IOBUF_LEN);
    if (buflen &lt;= 0) {
        serverLog(LL_WARNING,&quot;Read error sending DB to replica: %s&quot;,
            (buflen == 0) ? &quot;premature EOF&quot; : strerror(errno));
        freeClient(slave);
        return;
    }
    if ((nwritten = write(fd,buf,buflen)) == -1) {
        if (errno != EAGAIN) {
            serverLog(LL_WARNING,&quot;Write error sending DB to replica: %s&quot;,
                strerror(errno));
            freeClient(slave);
        }
        return;
    }
    slave-&gt;repldboff += nwritten;
    server.stat_net_output_bytes += nwritten;
    if (slave-&gt;repldboff == slave-&gt;repldbsize) {
        close(slave-&gt;repldbfd);
        slave-&gt;repldbfd = -1;
        aeDeleteFileEvent(server.el,slave-&gt;fd,AE_WRITABLE);
        putSlaveOnline(slave);
    }
}
```

在 updateSlavesWaitingBgsave 函数中与 replicationCron 会统计当前是否有 slave 处于 SLAVE_STATE_WAIT_BGSAVE_START 状态，如果有则会开启一个 bgSave 任务。

``` C
void updateSlavesWaitingBgsave(int bgsaveerr, int type) {
    listNode *ln;
    int startbgsave = 0;
    int mincapa = -1;
    listIter li;

    listRewind(server.slaves,&amp;li);
    while((ln = listNext(&amp;li))) {
        client *slave = ln-&gt;value;

        if (slave-&gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
            startbgsave = 1;
            mincapa = (mincapa == -1) ? slave-&gt;slave_capa :
                                        (mincapa &amp; slave-&gt;slave_capa);
        } else {
            ... ...
        }
    }
    if (startbgsave) startBgsaveForReplication(mincapa);
}


```

至此为止，可以部分回答第 4 个问题，即多个 slave 如何共享一个 bgSave 任务？

总结上文不难发现在，serverCron 和 replicationCron 两个入口内，都会统计多个处于 SLAVE_STATE_WAIT_BGSAVE_START 状态的 slave 的 mini capa，这样即使当前同时存在支持 EOF 和不支持 EOF 协议的 slave 也可以共享同一个 （rdb file）bgSave任务。在 PSYNC/SYNC command 入口中也会判断当前的 client 能否附加到正在执行的 （rdb file）bgSave 任务上。

此处总结上述函数的调用依赖关系：

``` mermaid!
stateDiagram-v2
    syncCommand--&gt;replicationSetupSlaveForFullResync
    note right of replicationSetupSlaveForFullResync: from WAIT_BGSAVE_START to WAIT_BGSAVE_END.
    

    syncCommand--&gt;startBgsaveForReplication
    note left of syncCommand: client into WAIT_BGSAVE_START in fullresync mode\nclient into ONLINE mode in psync mode. 
    replicationCron--&gt;startBgsaveForReplication

    serverCron--&gt;backgroundSaveDoneHandler
    backgroundSaveDoneHandler--&gt;backgroundSaveDoneHandlerDisk: rdb_file mode
    backgroundSaveDoneHandler--&gt;backgroundSaveDoneHandlerSocket: diskless mode

    backgroundSaveDoneHandlerDisk--&gt;updateSlavesWaitingBgsave
    note left of updateSlavesWaitingBgsave: from WAIT_BGSAVE_END to SEND_BULK in rdb_file mode\nfrom WAIT_BGSAVE_END to ONLINE in diskless mode.
    backgroundSaveDoneHandlerSocket--&gt;updateSlavesWaitingBgsave

    updateSlavesWaitingBgsave--&gt;startBgsaveForReplication
    updateSlavesWaitingBgsave--&gt;sendBulkToSlave: bind writeable event in rdb_file mode.
    

    startBgsaveForReplication--&gt;replicationSetupSlaveForFullResync
    startBgsaveForReplication--&gt;rdbSaveToSlavesSockets 
    startBgsaveForReplication--&gt;rdbSaveBackground
   

    rdbSaveBackground--&gt;rdbSave: fork child
    rdbSave--&gt;rdbSaveRio
    rdbSaveToSlavesSockets--&gt;replicationSetupSlaveForFullResync
    rdbSaveToSlavesSockets--&gt;rdbSaveRioWithEOFMark: fork child
    rdbSaveRioWithEOFMark--&gt;rdbSaveRio

    sendBulkToSlave--&gt;putSlaveOnline
    note right of sendBulkToSlave: from SEND_BULK to ONLINE. 
    
```

### BgSave/Save 命令与 Rdb/Aof-rewrite 日志

细心的读者会发现，在上一节中没有讲到日志和 bgSave/bgRewrite 命令。主要是因为两者非主从复制触发，并非本文的重点，以下只做简单的阐述。
在 redisServer 结构体中有如下字段：

``` C
struct redisServer {
    pid_t aof_child_pid;            /* PID if rewriting process */
    ...
    pid_t rdb_child_pid;            /* PID of RDB saving child */
}
```

rdb_child_pid 用于记录执行 bgSave 任务的子进程 pid，aof_child_pid 用于记录执行 aof_rewrite 任务的子进程 pid。从字段上显而易见，**同时只能执行一个 bgSave 任务，也只能执行一个 aof_rewrite 任务**。

执行`BGSAVE` 命令时会直接去调用 rdbSaveBackground 函数去开启一个 bgSave 任务，如果此时已经开启了一个 bgSave 任务 或 aof-rewrite 任务，则会报错。如果执行`BGSAVE SCHEDULE`命令，则会开启 rdb_bgsave_scheduled（`server.rdb_bgsave_scheduled=1`），等待 serverCron （不需要执行 bgSave 任务 和 aof-rewrite 任务时）启动执行。其**优先级低于 replicationCron，但是一定会执行，即便在其上一轮已经执行过了 bgSave 任务**。

那么 slave 能 attach 到这个 bgSave 任务上嘛？结合 [bgsaveCommand](https://github.com/redis/redis/blob/5.0/src/rdb.c#L2486) 实现和上一节的 syncCommand 解析可以看出来， 在开启了 bgSave 任务时并未向 server-&gt;slaves 队列中添加 slave，所以新加入的 slave 无法找到匹配的处于 WAIT_BGSAVE_END 状态的 slave，无法 attach 到该 bgSave 任务中。

这里顺便讲一下 `save` 命令，[saveCommand](https://github.com/redis/redis/blob/5.0/src/rdb.c#L2471) 直接在主进程调用 rdbSave 函数，没有 fork 子进程，所以会阻塞住全局的事件循环的执行，要慎用。

`save` 和 `bgSave` 命令核心都是依赖 rdbSave 函数，会先生成一个临时文件，然后用临时文件替换（可配置，缺省为 dump.rdb）的旧文件。
具体实现如下：

``` C
void saveCommand(client *c) {
    if (server.rdb_child_pid != -1) {
        addReplyError(c,&quot;Background save already in progress&quot;);
        return;
    }
    rdbSaveInfo rsi, *rsiptr;
    rsiptr = rdbPopulateSaveInfo(&amp;rsi);
    if (rdbSave(server.rdb_filename,rsiptr) == C_OK) {
        addReply(c,shared.ok);
    } else {
        addReply(c,shared.err);
    }
}

/* BGSAVE [SCHEDULE] */
void bgsaveCommand(client *c) {
    int schedule = 0;

    /* The SCHEDULE option changes the behavior of BGSAVE when an AOF rewrite
     * is in progress. Instead of returning an error a BGSAVE gets scheduled. */
    if (c-&gt;argc &gt; 1) {
        if (c-&gt;argc == 2 &amp;&amp; !strcasecmp(c-&gt;argv[1]-&gt;ptr,&quot;schedule&quot;)) {
            schedule = 1;
        } else {
            addReply(c,shared.syntaxerr);
            return;
        }
    }

    rdbSaveInfo rsi, *rsiptr;
    rsiptr = rdbPopulateSaveInfo(&amp;rsi);

    if (server.rdb_child_pid != -1) {
        addReplyError(c,&quot;Background save already in progress&quot;);
    } else if (server.aof_child_pid != -1) {
        if (schedule) {
            server.rdb_bgsave_scheduled = 1;
            addReplyStatus(c,&quot;Background saving scheduled&quot;);
        } else {
            addReplyError(c,
                &quot;An AOF log rewriting in progress: can't BGSAVE right now. &quot;
                &quot;Use BGSAVE SCHEDULE in order to schedule a BGSAVE whenever &quot;
                &quot;possible.&quot;);
        }
    } else if (rdbSaveBackground(server.rdb_filename,rsiptr) == C_OK) {
        addReplyStatus(c,&quot;Background saving started&quot;);
    } else {
        addReply(c,shared.err);
    }
}

int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {
    ... ...
     /* Start a scheduled BGSAVE if the corresponding flag is set. This is
     * useful when we are forced to postpone a BGSAVE because an AOF
     * rewrite is in progress.
     *
     * Note: this code must be after the replicationCron() call above so
     * make sure when refactoring this file to keep this order. This is useful
     * because we want to give priority to RDB savings for replication. */
    if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1 &amp;&amp;
        server.rdb_bgsave_scheduled &amp;&amp;
        (server.unixtime-server.lastbgsave_try &gt; CONFIG_BGSAVE_RETRY_DELAY ||
         server.lastbgsave_status == C_OK))
    {
        rdbSaveInfo rsi, *rsiptr;
        rsiptr = rdbPopulateSaveInfo(&amp;rsi);
        if (rdbSaveBackground(server.rdb_filename,rsiptr) == C_OK)
            server.rdb_bgsave_scheduled = 0;
    }
}
```

由日志触发的情况有以下三种：

1. 按照 'save xxx yy' 配置，在至少修改了 yy 个key后 xxx 秒，触发 rdb save；
2. aof 日志内部机制触发 rewrite；
3. client 调用 BGREWRITEAOF 命令；

在调用 `BGREWRITEAOF` 命令时，会判断当前是否满足开启一个 aof-rewrite 的条件：

+ 当前如果有一个 bgSave 任务在执行，则立即返回。等任务结束后，由 serverCron 启动 aof-rewrite 任务
+ 当前如果有一个 aof-rewrite 任务在执行，则直接返回错误
+ 当前没有 bgSave 任务 和 aof-rewrite 任务，则立即启动一个 aof-rewrite 任务。

整个 aof 任务执行流程如下：

``` mermaid!
graph TB
    start([start])
    exit([end])
    openPipe[open pipes.]
    
    preamble{enable rdb preamble?}
    rdbSave[save data into rdb file.]
    rewriteAof[rewrite the aof file.]
    
    readDiff[save diff data from parent.]
    timeout{save_diff_time &gt;= 1s or &lt;br&gt; no_data_time &gt;= 20ms}
    stopSendingDiff[ask the master stop &lt;br&gt; to sending diffs]
    ackFromParent[read ack from parent.]
    readDiff1[save diff data from parent.]
    scriptFlush[Empty the script cache.]
    start--&gt;openPipe--&gt;fork-.-&gt;preamble
    fork--&gt;scriptFlush--&gt;exit
    preamble--&gt;|Yes|rdbSave--&gt;readDiff--&gt;timeout
    preamble--&gt;|No|rewriteAof--&gt;readDiff
    timeout--&gt;|No|readDiff
    timeout--&gt;|Yes|stopSendingDiff--&gt;ackFromParent
    ackFromParent--&gt;readDiff1-.-&gt;exit
   
```

启动的任务子进程会将数据先保存在临时文件，当写入完成后再去替换旧的日志文件。当开启 preamble 选项时在 aof-rewrite 时会生成一个 rdb 格式文件（调用 rdbSaveRio 函数），然后会将 diff 内容追加到后面。在上一节讲解 rdbSaveRio 时有提到 `aof-preamble` 字段，该字段标识了当前 rdb 文件是否用于 `aof-rewrite`，即对应上述的情况。aof-rewrite 核心函数为 [rewriteAppendOnlyFileBackground](https://github.com/redis/redis/blob/5.0/src/aof.c#L1569)。

对于以上的 1、2 种情况，都是在开启了持久化由 redis 内部机制自动触发的，其判断触发的逻辑在 serverCron 中，下图中的每个步骤都有一个前提：当前没有 bgSave 和 aof-rewrite 任务在执行：

``` mermaid!
graph LR
    start([Start]) 
    exit([End])

    bgTask{No aof-rewrite and &lt;br&gt; bgSave in progress?}
    schedule{Aof-rewrite has &lt;br&gt; been scheduled?}
    save{change keys &gt; limit_keys &lt;br&gt; and  &lt;br&gt; now - last_save_time &gt; limit_time}
    scheduleAofRewrite[Start a aof-rewrite]
    start--&gt;bgTask--&gt;|No|exit
    bgTask--&gt;|Yes|schedule--&gt;|Yes|scheduleAofRewrite
    scheduleAofRewrite--&gt;exit

    schedule--&gt;|No|save--&gt;|Yes|bgSave[Start a bgSave]--&gt;exit
    rewrite{Growth &gt;= Rewrite-perc &lt;br&gt; and &lt;br&gt; Aof-current-size &gt; Rewrite-min-size}

    save--&gt;|No|rewrite--&gt;|No|exit
    rewrite--&gt;|Yes|aof-rewrite[Start a aof-rewrite]--&gt;exit
```

具体实现如下：

``` C
int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {
    ... ...
    /* Start a scheduled AOF rewrite if this was requested by the user while
     * a BGSAVE was in progress. */
    if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1 &amp;&amp;
        server.aof_rewrite_scheduled)
    {
        rewriteAppendOnlyFileBackground();
    }
    ... ...
      /* Check if a background saving or AOF rewrite in progress terminated. */
    if (server.rdb_child_pid != -1 || server.aof_child_pid != -1 ||
        ldbPendingChildren())
    {
       ... ... 
    } else {
        /* If there is not a background saving/rewrite in progress check if
         * we have to save/rewrite now. */
        for (j = 0; j &lt; server.saveparamslen; j++) {
            struct saveparam *sp = server.saveparams+j;

            /* Save if we reached the given amount of changes,
             * the given amount of seconds, and if the latest bgsave was
             * successful or if, in case of an error, at least
             * CONFIG_BGSAVE_RETRY_DELAY seconds already elapsed. */
            if (server.dirty &gt;= sp-&gt;changes &amp;&amp;
                server.unixtime-server.lastsave &gt; sp-&gt;seconds &amp;&amp;
                (server.unixtime-server.lastbgsave_try &gt;
                 CONFIG_BGSAVE_RETRY_DELAY ||
                 server.lastbgsave_status == C_OK))
            {
                serverLog(LL_NOTICE,&quot;%d changes in %d seconds. Saving...&quot;,
                    sp-&gt;changes, (int)sp-&gt;seconds);
                rdbSaveInfo rsi, *rsiptr;
                rsiptr = rdbPopulateSaveInfo(&amp;rsi);
                rdbSaveBackground(server.rdb_filename,rsiptr);
                break;
            }
        }

        /* Trigger an AOF rewrite if needed. */
        if (server.aof_state == AOF_ON &amp;&amp;
            server.rdb_child_pid == -1 &amp;&amp;
            server.aof_child_pid == -1 &amp;&amp;
            server.aof_rewrite_perc &amp;&amp;
            server.aof_current_size &gt; server.aof_rewrite_min_size)
        {
            long long base = server.aof_rewrite_base_size ?
                server.aof_rewrite_base_size : 1;
            long long growth = (server.aof_current_size*100/base) - 100;
            if (growth &gt;= server.aof_rewrite_perc) {
                serverLog(LL_NOTICE,&quot;Starting automatic rewriting of AOF on %lld%% growth&quot;,growth);
                rewriteAppendOnlyFileBackground();
            }
        }
    }
    ... ...
}
```

那么上述由日志触发的 bgSave 能否与 slaves 共享？从代码来看，是无法与 slave 共享的，原因与手动触发一致。至此就解答了第 4 个问题，即由 slave 触发的 bgSave 有条件共享，其他情况 无法共享。

以上所有触发 bgSave 情况在 serverCron 的优先级如下：

1. BGREWRITEAOF command
2. backgroundSaveDoneHandler
3. save config
4. aof-rewrite config
5. replicationCron
6. BGSAVE SCHEDULE command

### Slave 加载 RDB 数据

slave 在将 rdb 数据持久化到本地临时文件中，在完整接收重命名为指定的（缺省为 dump.rdb） rdb file，然后通过 [rdbLoad](https://github.com/redis/redis/blob/5.0/src/rdb.c#L2151) 的方式加载到内存。在加载 rdb file 的过程中，redis 无法执行 flag 不包含 `l` 的命令，此时如果 sub-slave 向其发起 psync handshake 会返回错误：“-LOADING Redis is loading the dataset in memory”。

## Replconf ack 的作用

0. 保证 slave 和 master 之间的链接活性；
1. diskless rdb 数据传输结束后，会等待一个 ack 才开启增量同步；
2. 使用 wait 命令会等待 ack；

## CLIENT PAUSE

master 向 slave 提供主从同步请求的端口，与对普通 client 提供数据服务的端口相同。当开启了`client pause` 时，slave client 也会受到影响：已经完成 hand shake 的 slave client 同步数据过程不受影响，但是未完成 hand shake 的 slave client 会被阻塞，所以该命令要慎重使用。

## Master client

所谓 master client 是指 socket 对端为 master 节点的 client，但该 socket 建立的发起者仍然为 slave 节点。slave 与 master 节点进行握手时会交换 ip 和 port（详情见上一篇），但是 slave 提供的 ip 和 port 对于 master 而言仅仅起到标识作用，在除 failover 情况外，master 不会向 slave 提供的 ip 和 port 主动发起链接。

## FailOver

redis 5.0 的服务模型为**事件驱动模型**，事件的监听和响应在单线程中，这种模型可以天然的实现无锁编程，但是也会带来一些弊端。由于其串行处理模式，当执行某个“耗时” 命令时，可能会导致 redis master 被判定为 `offline` 。

在 redis 初始化的时候，会将 acceptTcpHandler 绑定到所有监听的 （listen port 为server_port） socket fd 的 readable 事件上，会将 acceptUnixHandler 绑定到 unix socket fd 的 readbable 事件上，用于接收 client 链接请求。然后执行 clusterInit，同样将 clusterAcceptHandler 绑定到所有监听的（listen port 为cluster_port = server_port + 10000）socket fd 的 readable 事件上，用于接收其他节点的链接请求。

在接收到链接请求后会将 readQueryFromClient 绑定到 accepted socket fd 上，用于处理对端发送的数据。在执行所有初始化操作后，在 [main](https://github.com/redis/redis/blob/5.0/src/server.c#L4222) 函数中启动事件处理循环，具体实现如下：

``` C
int main(int argc, char **argv) {
    ... ...
    initServer();
    ... ...
    aeMain(server.el);
}


void initServer(void) {
    ... ...
     /* Create the timer callback, this is our way to process many background
     * operations incrementally, like clients timeout, eviction of unaccessed
     * expired keys and so forth. */
    if (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) {
        serverPanic(&quot;Can't create event loop timers.&quot;);
        exit(1);
    }

    /* Create an event handler for accepting new connections in TCP and Unix
     * domain sockets. */
    for (j = 0; j &lt; server.ipfd_count; j++) {
        if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,
            acceptTcpHandler,NULL) == AE_ERR)
            {
                serverPanic(
                    &quot;Unrecoverable error creating server.ipfd file event.&quot;);
            }
    }
    if (server.sofd &gt; 0 &amp;&amp; aeCreateFileEvent(server.el,server.sofd,AE_READABLE,
        acceptUnixHandler,NULL) == AE_ERR) serverPanic(&quot;Unrecoverable error creating server.sofd file event.&quot;);

    ... ...
    if (server.cluster_enabled) clusterInit();
    ... ...
}

void aeMain(aeEventLoop *eventLoop) {
    eventLoop-&gt;stop = 0;
    while (!eventLoop-&gt;stop) {
        if (eventLoop-&gt;beforesleep != NULL)
            eventLoop-&gt;beforesleep(eventLoop);
        aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP);
    }
}


void clusterInit(void) {
    ... ...
    if (listenToPort(server.port+CLUSTER_PORT_INCR,
        server.cfd,&amp;server.cfd_count) == C_ERR)
    {
        exit(1);
    } else {
        int j;

        for (j = 0; j &lt; server.cfd_count; j++) {
            if (aeCreateFileEvent(server.el, server.cfd[j], AE_READABLE,
                clusterAcceptHandler, NULL) == AE_ERR)
                    serverPanic(&quot;Unrecoverable error creating Redis Cluster &quot;
                                &quot;file event.&quot;);
        }
    }
    ... ...
}
```

事件循环处理的流程如下：

``` mermaid!
graph TB
    start([start])
    stop{loop stop?}
    beforeSleep[run proc before sleep]
    exit([end])
    nextTimer[find neartest timer]
    apollWait[apoll wait file events &lt;br&gt; until next timer fired]
    process[process file events]
    processTime[process time events]
    afterSleep[run proc after sleep]
    
    start--&gt;stop--&gt;|Yes|exit
    stop--&gt;|No|beforeSleep--&gt;nextTimer--&gt;apollWait
    apollWait--&gt;afterSleep--&gt;process--&gt;processTime--&gt;stop 
```

事件处理的核心函数是 [aeProcessEvents](https://github.com/redis/redis/blob/5.0/src/ae.c#L358) 函数，采用事件边沿触发机制。通常会先处理 socket fd 可读事件，然后再处理可写事件，即处理客户端请求然后立即应答的模式；也有一些情况需要先处理写事件，再处理读事件，例如：cluster模式下向其他节点发送一个 gossip 消息然后再接收应答。具体实现如下：

``` C
/* Process every pending time event, then every pending file event
 * (that may be registered by time event callbacks just processed).
 * Without special flags the function sleeps until some file event
 * fires, or when the next time event occurs (if any).
 *
 * If flags is 0, the function does nothing and returns.
 * if flags has AE_ALL_EVENTS set, all the kind of events are processed.
 * if flags has AE_FILE_EVENTS set, file events are processed.
 * if flags has AE_TIME_EVENTS set, time events are processed.
 * if flags has AE_DONT_WAIT set the function returns ASAP until all
 * if flags has AE_CALL_AFTER_SLEEP set, the aftersleep callback is called.
 * the events that's possible to process without to wait are processed.
 *
 * The function returns the number of events processed. */
int aeProcessEvents(aeEventLoop *eventLoop, int flags)
{
    int processed = 0, numevents;

    /* Nothing to do? return ASAP */
    if (!(flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_FILE_EVENTS)) return 0;

    /* Note that we want call select() even if there are no
     * file events to process as long as we want to process time
     * events, in order to sleep until the next time event is ready
     * to fire. */
    if (eventLoop-&gt;maxfd != -1 ||
        ((flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_DONT_WAIT))) {
        int j;
        aeTimeEvent *shortest = NULL;
        struct timeval tv, *tvp;

        if (flags &amp; AE_TIME_EVENTS &amp;&amp; !(flags &amp; AE_DONT_WAIT))
            shortest = aeSearchNearestTimer(eventLoop);
        if (shortest) {
            long now_sec, now_ms;

            aeGetTime(&amp;now_sec, &amp;now_ms);
            tvp = &amp;tv;

            /* How many milliseconds we need to wait for the next
             * time event to fire? */
            long long ms =
                (shortest-&gt;when_sec - now_sec)*1000 +
                shortest-&gt;when_ms - now_ms;

            if (ms &gt; 0) {
                tvp-&gt;tv_sec = ms/1000;
                tvp-&gt;tv_usec = (ms % 1000)*1000;
            } else {
                tvp-&gt;tv_sec = 0;
                tvp-&gt;tv_usec = 0;
            }
        } else {
            /* If we have to check for events but need to return
             * ASAP because of AE_DONT_WAIT we need to set the timeout
             * to zero */
            if (flags &amp; AE_DONT_WAIT) {
                tv.tv_sec = tv.tv_usec = 0;
                tvp = &amp;tv;
            } else {
                /* Otherwise we can block */
                tvp = NULL; /* wait forever */
            }
        }

        /* Call the multiplexing API, will return only on timeout or when
         * some event fires. */
        numevents = aeApiPoll(eventLoop, tvp);

        /* After sleep callback. */
        if (eventLoop-&gt;aftersleep != NULL &amp;&amp; flags &amp; AE_CALL_AFTER_SLEEP)
            eventLoop-&gt;aftersleep(eventLoop);

        for (j = 0; j &lt; numevents; j++) {
            aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd];
            int mask = eventLoop-&gt;fired[j].mask;
            int fd = eventLoop-&gt;fired[j].fd;
            int fired = 0; /* Number of events fired for current fd. */

            /* Normally we execute the readable event first, and the writable
             * event laster. This is useful as sometimes we may be able
             * to serve the reply of a query immediately after processing the
             * query.
             *
             * However if AE_BARRIER is set in the mask, our application is
             * asking us to do the reverse: never fire the writable event
             * after the readable. In such a case, we invert the calls.
             * This is useful when, for instance, we want to do things
             * in the beforeSleep() hook, like fsynching a file to disk,
             * before replying to a client. */
            int invert = fe-&gt;mask &amp; AE_BARRIER;

            /* Note the &quot;fe-&gt;mask &amp; mask &amp; ...&quot; code: maybe an already
             * processed event removed an element that fired and we still
             * didn't processed, so we check if the event is still valid.
             *
             * Fire the readable event if the call sequence is not
             * inverted. */
            if (!invert &amp;&amp; fe-&gt;mask &amp; mask &amp; AE_READABLE) {
                fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask);
                fired++;
            }

            /* Fire the writable event. */
            if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) {
                if (!fired || fe-&gt;wfileProc != fe-&gt;rfileProc) {
                    fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask);
                    fired++;
                }
            }

            /* If we have to invert the call, fire the readable event now
             * after the writable one. */
            if (invert &amp;&amp; fe-&gt;mask &amp; mask &amp; AE_READABLE) {
                if (!fired || fe-&gt;wfileProc != fe-&gt;rfileProc) {
                    fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask);
                    fired++;
                }
            }

            processed++;
        }
    }
    /* Check time events */
    if (flags &amp; AE_TIME_EVENTS)
        processed += processTimeEvents(eventLoop);

    return processed; /* return the number of processed file/time events */
}
```

当 server 在接收到 client 发送的完整命令后，**会同步调用对应的 command 实现函数，如果耗时过长会阻塞住其他事件的处理**。例如：在包含大量数据的 redis 数据库中执行 `flushdb` 命令，导致无法响应 cluster 的 gossip 信息，如果阻塞时间超过 cluster-timeout，会被其他节点判定为 FAIL 状态，从而触发主从切换。

flushdbCommand 的主要执行任务就是释放 记录 key-value、key-expire、key-expire-slaves 的 dict 和 记录 key 在 slots 上分布情况 的 struct 占用的内存，具体实现如下：

``` C

/* Remove all keys from all the databases in a Redis server.
 * If callback is given the function is called from time to time to
 * signal that work is in progress.
 *
 * The dbnum can be -1 if all the DBs should be flushed, or the specified
 * DB number if we want to flush only a single Redis database number.
 *
 * Flags are be EMPTYDB_NO_FLAGS if no special flags are specified or
 * EMPTYDB_ASYNC if we want the memory to be freed in a different thread
 * and the function to return ASAP.
 *
 * On success the fuction returns the number of keys removed from the
 * database(s). Otherwise -1 is returned in the specific case the
 * DB number is out of range, and errno is set to EINVAL. */
long long emptyDb(int dbnum, int flags, void(callback)(void*)) {
    int async = (flags &amp; EMPTYDB_ASYNC);
    long long removed = 0;

    if (dbnum &lt; -1 || dbnum &gt;= server.dbnum) {
        errno = EINVAL;
        return -1;
    }

    int startdb, enddb;
    if (dbnum == -1) {
        startdb = 0;
        enddb = server.dbnum-1;
    } else {
        startdb = enddb = dbnum;
    }

    for (int j = startdb; j &lt;= enddb; j++) {
        removed += dictSize(server.db[j].dict);
        if (async) {
            emptyDbAsync(&amp;server.db[j]);
        } else {
            dictEmpty(server.db[j].dict,callback);
            dictEmpty(server.db[j].expires,callback);
        }
    }
    if (server.cluster_enabled) {
        if (async) {
            slotToKeyFlushAsync();
        } else {
            slotToKeyFlush();
        }
    }
    if (dbnum == -1) flushSlaveKeysWithExpireList();
    return removed;
}
```

模拟key-value size 平均为 100 bytes 的场景，在总数据量为 2 GB 场景，分析完全释放的耗时时间：

从 `perf record --call-graph dwarf` 生成的 profil e图上来看，主要的耗时是在执行 emptyDb 函数上：

``` 
Samples: 75K of event 'cycles', Event count (approx.): 45874076225
    main
    aeMain
    aeProcessEvents
        - 89.32% processInputBuffer
            call
            flushdbCommand
          - emptyDb
            - 65.44% dictEmpty
            - 22.05% slotToKeyFlush    
```

其中 `dictEmpty` 是最耗时的部分：

```
    dictEmpty
        - _dictClear
            - 29.04% decrRefCount
                + 10.35% je_free
                   8.83% sdsfree
                + 4.67% zfree
            - 13.65% je_fee
                + 13.06% je_tcache_bin_flush_small
            - 7.61% zfree
                6.74% je_malloc_usable_size
              7.52% sdsfree 
```

其中函数耗时排名中，`sdsfree` 时间占比最高 15.47%，其次是 `je_malloc_usable_size` 函数占比为 11.32%，都是源自 jemalloc package 内部的函数。

简单补充一下关于 perf 的知识，它是基于 hw/sw events、trace point、probe point 采样的性能分析工具，其默认使用 cycles event 作为采样 event，这是由内核映射到特定于硬件的 PMU 事件的通用硬件事件。 对于 Intel，它映射到 UNHALTED_CORE_CYCLES。 在存在 CPU 频率缩放的情况下，此事件不会与时间保持恒定的相关性。Intel 提供了另一个事件，称为 UNHALTED_REFERENCE_CYCLES，但该事件当前不适用于 perf_events。在 AMD 系统上，该事件被映射到 CPU_CLK_UNHALTED 并且该事件也受频率缩放的影响。 在任何 Intel 或 AMD 处理器上，当处理器空闲时，即调用 mwait() 时，循环事件不计算在内。[^1]

## 总结

以上就是在生产过程中遇到的一些问题和原因的分析，redis 5.0 无论代码总量还是模型设计都趋向于就简原则，充分发挥了简洁之美和数据结构之美。阅读 redis 的源码可以发现即使用 C 语言这种低级语言，也可以写出复杂而又精巧的工程，对于软件工程上的设计思路也是极大的启发，例如其事件驱动模型基础上构建的无锁编程设计，（在一定条件下）可以获得不亚于甚至超过多线程服务模型的性能。也对日常生产开发的注意事项有一些帮助，比如 script 的使用和同步，在不同版本下有不同的实现机制；触发 bgSave 的条件和优先级；Failover 的根因分析等等。

## 参考引用

[^1]: perf. Linux kernel profiling with perf. May 5 2015, https://perf.wiki.kernel.org/index.php/Tutorial#Sampling_with_perf_record</content><author><name>Zhipeng Wang</name><email>wangzhipenghyc@163.com</email></author><summary type="html"></summary></entry><entry><title type="html">Redis_psync_protocol</title><link href="http://localhost:4000/redis_psync_protocol/" rel="alternate" type="text/html" title="Redis_psync_protocol" /><published>2022-01-05T00:00:00+08:00</published><updated>2022-01-05T00:00:00+08:00</updated><id>http://localhost:4000/redis_psync_protocol</id><content type="html" xml:base="http://localhost:4000/redis_psync_protocol/">&lt;style&gt;
  @import url(&quot;/assets/css/simple.css&quot;)
&lt;/style&gt;

# Redis psync protocol

redis老版本的同步协议是 `SYNC`，因为它不支持部分同步所以被`PSYNC`代替，发展出了 `psync1`协议。后续为优化由 `failover` 带来的不必要`full Resynchronization`，发展出了 `psync2` 协议。下面的内容是基于 `redis 5.0` 版本，剖析一下 `psync2` 协议的实现。

## replication handshake

slave 与 master 之前发起同步的过程称为 **replication  handshake**， 在 `slave node` 的 [replicationCron](https://github.com/redis/redis/blob/5.0/src/replication.c#L2578) 任务（每秒调用一次）中会调用 `connectWithMaster -&gt; registry file event[syncWithMaster -&gt; slaveTryPartialResynchronization]` 函数与 `master node` 完成 `replication  handshake` 过程，具体的握手流程实现在[syncWithMaster](https://github.com/redis/redis/blob/5.0/src/replication.c#L1643)函数中。下面展示的是 `slave node`进入 `REPL_STATE_SEND_PSYNC`状态后的交互流程，在此之前，`slave` 和 `master`已经依次执行了如下流程：

1. slave 向 master 发起 tcp 链接；
2. slave 向 master 发送 `PING` 命令，master 响应是否需要AUTH，即返回 `-NOAUTH`，如果需要执行 3，否则跳转到 4；
3. slave 向 master 发送 `AUTH masterauth`，master 响应是否认证成功；
4. slave 发送 `REPLCONF listening-port port` （如果 `slave_announce_port` 不存在，则发送`tcp listening port`）；如果`slave_announce_ip` 不为 NULL 则发送 `REPLCONF ip-address slave_announce_ip`
5. slave 向 master 同步当前支持的能力，发送 `REPLCONF capa eof capa psync2`。

从 1~5 过程中，slave 状态依次经过如下变迁： `REPL_STATE_NONE -&gt; REPL_STATE_CONNECT -&gt; REPL_STATE_CONNECTING -&gt; REPL_STATE_RECEIVE_PONG -&gt; REPL_STATE_SEND_AUTH -&gt; REPL_STATE_RECEIVE_AUTH -&gt; REPL_STATE_SEND_PORT -&gt; REPL_STATE_RECEIVE_PORT -&gt; REPL_STATE_SEND_IP -&gt; REPL_STATE_RECEIVE_IP -&gt; REPL_STATE_SEND_CAPA -&gt; REPL_STATE_RECEIVE_CAPA -&gt; REPL_STATE_RECEIVE_PSYNC`

``` mermaid!
sequenceDiagram
    participant Slave
    participant Master
    Note left of Slave: REPL_STATE_SEND_PSYNC

    alt replid + offset is exists
        Slave -&gt;&gt; Master: PSYNC replid offset + 1
    else
        Slave -&gt;&gt; Master: PSYNC ? -1
    end
    
    Note left of Slave: REPL_STATE_RECEIVE_PSYNC
    alt FULL RESYNC
        Master -&gt;&gt; Slave: +FULLRESYNC replid offset
        Slave -&gt;&gt; Slave: update master_replid and master_initial_offset
        
        Note over Slave, Master: FULL SYNC
        
    else PSYNC
        Master -&gt;&gt; Slave: +CONTINUE replid
        Note left of Slave: REPL_STATE_CONNECTED
    end

```

当 `master` 返回`+CONTINUE replid`时，`slave node` 进入 `REPL_STATE_CONNECTED`状态，开始接收 `master node` 同步过来的命令。

执行`Full Resynchronization` 时 `Master node` 会调用 `fork()` 创建一个`Backend Process` 与 `Slave Nodes` 进行同步通信，避免阻塞`Master node`对外提供服务：

``` mermaid!
sequenceDiagram
    participant Slave
    participant BackendMaster
    Note left of Slave: REPL_STATE_TRANSFER
    opt not diskless sync
        BackendMaster --&gt;&gt; BackendMaster: BGSAVE
    end

    loop until eof
        BackendMaster -&gt;&gt; Slave: send RDB data
        Slave -&gt;&gt; Slave: save in the temp file
    end

    Slave-&gt;&gt;Slave: load the temp file 

    Note left of Slave: REPL_STATE_CONNECTED
```


在全量同步时 `backend save process` 会先生成一个 `RDB file` 保存在磁盘里，然后再从磁盘中将文件加载到内存同步给`slave nodes`。与此同时它会缓冲当前收到的 `client command`，当 `RDB file` 数据传输完成后，`master` 会把缓冲的 `command` 全部发送给 `slave` 。**如果并发有多个全量同步请求，`master` 只会 `fork` 一个 `backend process` 去服务所有请求**，当无法附加到当前的 `BGSAVE` 流程时（`slave capability`与开启当前`BGSAVE`流程的 `slave node` 不同），需要去等待下一个 `BGSAVE` 或 `SYNC`(详情见[syncCommand](https://github.com/redis/redis/blob/5.0/src/replication.c#L629)函数)。

当 `master node` 所在宿主机的磁盘读写速度较慢时，会对`master node`带来额外的压力，为了解决这个问题，在2.8.18以后的版本支持不使用磁盘作为中间存储的介质，直接在`backend process`中通过网络发送到给其他的`slave nodes`（前提是在 `slave nodes` 支持 `eof capability` 的情况下，在 `replication shake`过程中通过 `REPLCONF`命令同步 `slave capability`），可以通过`repl-diskless-sync`开启这个选项。**在diskless模式下，为了延迟几秒等待更多的slave nodes 全量同步请求到达，backend process不会立即创建， 而是放在 replicationCron()中被创建**，如果已经被启动，则需要等待下一个 `BGSAVE` 或 `SYNC`(详情见[syncCommand](https://github.com/redis/redis/blob/5.0/src/replication.c#L629)函数)。

`backend save process`执行同步的结果（与slave全量同步是否成功）会通过`pipe`被父进程（服务进程）接收，服务进程根据返回结果来决定对每个slave同步的后续处理流程（是否进入增量同步流程）[^3]。

`master` 判断是否直接通过 `socket` 发送 `RDB` 数据的代码 [startBgsaveForReplication](https://github.com/redis/redis/blob/5.0/src/replication.c#L564) 如下：

``` C

int startBgsaveForReplication(int mincapa) {
    int retval;
    // 需要 master 开启 repl_diskless_sync 选项，同时 slave 支持 SLAVE_CAPA_EOF
    int socket_target = server.repl_diskless_sync &amp;&amp; (mincapa &amp; SLAVE_CAPA_EOF); 
    listIter li;
    listNode *ln;

    serverLog(LL_NOTICE,&quot;Starting BGSAVE for SYNC with target: %s&quot;,
        socket_target ? &quot;replicas sockets&quot; : &quot;disk&quot;);

    rdbSaveInfo rsi, *rsiptr;
    rsiptr = rdbPopulateSaveInfo(&amp;rsi);
    /* Only do rdbSave* when rsiptr is not NULL,
     * otherwise slave will miss repl-stream-db. */
    if (rsiptr) {
        // 需要支持 socket_target;
        if (socket_target) 
            retval = rdbSaveToSlavesSockets(rsiptr);
        else
            retval = rdbSaveBackground(server.rdb_filename,rsiptr);
    } else {
        serverLog(LL_WARNING,&quot;BGSAVE for replication: replication information not available, can't generate the RDB file right now. Try later.&quot;);
        retval = C_ERR;
    }
}

```

`slave` 接收并加载`RDB`数据的具体实现在[readSyncBulkPayload](https://github.com/redis/redis/blob/5.0/src/replication.c#L1141)中。需要注意的是，当完成上面的 `RDB Full Resynchronization` 流程后，`slave`会变更为`REPL_STATE_CONNECTED`状态，完成 `replication  handshake`。

从上述的过程可以看出 `replid + offset` 是整个复制流程最核心的要素，下面会围绕它们去展开，来对实现细节进一步发掘。

在进入下一节前在此提出几个问题：

1. replid 是如何产生的？它是如何在节点之间同步的？

2. offset时如何统计的？failover后，新的 master 节点是否会继承 failover 前的 offset 记录向继续向上递增？

3. backlog的作用是什么？节点在 slave 角色下是否会开启 backlog？

4. 在发生 failover 后如何继续 psync？

## Replication ID

每一个 `master node` 都拥有一个 `replication id`（用一个巨大的随机数来标记指定的 `dataset` 的 `story` ），`master node`向 `slave node` 每发送一个byte都会增加`offset`，简单的说就是用`replication id + offset`构成一个同步状态的标识和记录。如上节所述当 `slave node` 连接到 `master node` 进行 `replication handshake` 时，使用 `PSYNC` 命令将 `replication id + offset` 发送给 `master node`，`master node`可以根据该信息，只将更新的部分发送给 `slave node` 。

下面给出[redisServer](https://github.com/redis/redis/blob/5.0/src/server.h#L942)中与数据同步相关的部分字段：

``` C
#define CONFIG_RUN_ID_SIZE 40

struct redisServer{
    ... ...
    /* Replication (master) */
    char replid[CONFIG_RUN_ID_SIZE+1];  /* 当前的 replication ID. */
    char replid2[CONFIG_RUN_ID_SIZE+1]; /* 继承自 failover 前的 master*/
    long long master_repl_offset;   /* 当前的 replication offset */
    long long second_replid_offset; /* 对于 replid2 可以接受的最大偏移量上限. */
    ... ...
    char *repl_backlog;             /* 用于部分同步的 backlog  */
    long long repl_backlog_size;    /* backlog ringbuffer size */
    long long repl_backlog_histlen; /* backlog 实际数据长度 */
    long long repl_backlog_idx;     /* backlog ringbuffer 下一个写入位置.*/
    long long repl_backlog_off;     /* backlog buffer的第一个字节对应 复制集“master offset”*/
    ... ...
    client *master;     /* 对应master的Client对象 */
    client *cached_master; /* 为了用于 PSYNC 而缓存的 master client对象.*/
    ... ...
    // 下面两个字段是用于记录 PSYNC 过程中 master node 的 replid/offset
    // 最终会把它们记录到 server-&gt;master 对象中
    char master_replid[CONFIG_RUN_ID_SIZE+1];  /* Master PSYNC runid. */
    long long master_initial_offset;           /* Master PSYNC offset. */
}

```

`master node` 对于是否满足`PSYNC`的判断条件具体实现在[masterTryPartialResynchronization](https://github.com/redis/redis/blob/5.0/src/replication.c#L448)函数：

``` C
/* This function handles the PSYNC command from the point of view of a
 * master receiving a request for partial resynchronization.
 *
 * On success return C_OK, otherwise C_ERR is returned and we proceed
 * with the usual full resync. */
int masterTryPartialResynchronization(client *c) {
    ... ...
     /* Is the replication ID of this master the same advertised by the wannabe
     * slave via PSYNC? If the replication ID changed this master has a
     * different replication history, and there is no way to continue.
     *
     * Note that there are two potentially valid replication IDs: the ID1
     * and the ID2. The ID2 however is only valid up to a specific offset. */
    if (strcasecmp(master_replid, server.replid) &amp;&amp;
        (strcasecmp(master_replid, server.replid2) ||
         psync_offset &gt; server.second_replid_offset))
    {
        /* Run id &quot;?&quot; is used by slaves that want to force a full resync. */
        if (master_replid[0] != '?') {
            if (strcasecmp(master_replid, server.replid) &amp;&amp;
                strcasecmp(master_replid, server.replid2))
            {
                serverLog(LL_NOTICE,&quot;Partial resynchronization not accepted: &quot;
                    &quot;Replication ID mismatch (Replica asked for '%s', my &quot;
                    &quot;replication IDs are '%s' and '%s')&quot;,
                    master_replid, server.replid, server.replid2);
            } else {
                serverLog(LL_NOTICE,&quot;Partial resynchronization not accepted: &quot;
                    &quot;Requested offset for second ID was %lld, but I can reply &quot;
                    &quot;up to %lld&quot;, psync_offset, server.second_replid_offset);
            }
        } else {
            serverLog(LL_NOTICE,&quot;Full resync requested by replica %s&quot;,
                replicationGetSlaveName(c));
        }
        goto need_full_resync;
    }
    ... ...
}
```

在 redis 节点中通过 `info replication` 命令查询可能会看到两个 `replication id` ：`master` 和 `secondary` ，两者作用分别如下：

+ master : 当前的`replication id`，用于标识目前状态最新的 `replicas`；
+ sencondary : `failover` 前的 `replication id`，是节点在变更为主节点之前标识`replicas`的`id`；

如果 `slave node` 的 `replicationID` 与 `replid` 不同，同时与 `replid2` 不同或 `psync_offset &gt; second_replid_offset`，则执行全量同步。如果 `master node` 是刚刚完成 `failover`由 `slave node` 切换而来，此时 `second_replid_offset = master_repl_offset + 1`（下文有详细阐述）。在redis集群中，当网络情况正常时（未发生集群网络分裂）`offset` 最大的 `slave node` 会被其他 `slave nodes` 选为新的 `master node`。此时其他的从节点发送 `PSYNC replicationID offset` 过来，可以避免跳转到全量同步进入下面的流程：

``` C
    /* We still have the data our slave is asking for? */
    if (!server.repl_backlog ||
        psync_offset &lt; server.repl_backlog_off ||
        psync_offset &gt; (server.repl_backlog_off + server.repl_backlog_histlen))
    {
        serverLog(LL_NOTICE,
            &quot;Unable to partial resync with replica %s for lack of backlog (Replica request was: %lld).&quot;, replicationGetSlaveName(c), psync_offset);
        if (psync_offset &gt; server.master_repl_offset) {
            serverLog(LL_WARNING,
                &quot;Warning: replica %s tried to PSYNC with an offset that is greater than the master replication offset.&quot;, replicationGetSlaveName(c));
        }
        goto need_full_resync;
    }
```

当 `master node` 没有开启 `backlog` 或 `psync_offset` 不在 `backlog` 范围内时，会直接触发全量同步。**如果任意两个节点的`replication id + offset`相等，说明两个节点之间的数据相同**。整体的流程如下：

``` mermaid!
graph TB
    start([master try to PSYNC])
    exit([end])
    parser[/parse master_replid and &lt;br&gt; psync_offset from slave./]
    fullsync[[full resync]]
    check_replid{server.replid == &lt;br&gt; master_replid ?}
    check_replid2{server.replid2 != master_replid Or &lt;br&gt; psync_offset &gt; &lt;br&gt; server.second_replid_offset ?}
    backlog{enable backlog?}
    backlog_match{psync_offset &lt; server.repl_backlog_off Or &lt;br&gt; psync_offset &gt; server.repl_backlog_off + &lt;br&gt; server.repl_backlog_histlen ?}

    start--&gt;parser--&gt;check_replid
    check_replid--&gt;|No|check_replid2
    check_replid--&gt;|Yes|backlog
    check_replid2--&gt;|Yes|fullsync
    check_replid2--&gt;|No|backlog
    backlog--&gt;|No| fullsync
    backlog--&gt;|Yes| backlog_match
    backlog_match--&gt;|No| fullsync
    backlog_match--&gt;|Yes| exit
    fullsync--&gt; exit
```


在发生主从切换后，`slave nodes` 会向新的`master`发送旧的`replication id + offset`，此时新的`master`节点收到后会与`master relication id` 和 `secondary replication id`进行对比，如果对比发现`slave node`发送的`replication id + offset`与 `secondary replication id + offset`匹配且偏差在一定安全范围，则不需要进行全量同步。

上面给出的是`master node`视角的处理流程，下面讲一下`slave node`视角的处理流程。

在 `PSYNC` 初始化的阶段（`PSYNC handshark`） `slave node` 会调用 [slaveTryPartialResynchronization(master_fd, 0)](https://github.com/redis/redis/blob/5.0/src/replication.c#L1483) 函数将自己的 `replicationID + offset` 通过 `PSYNC` 命令发送给 `master node`，如果 `cached_master` 无效，则发送 `PSYNC ? -1` 给 `master node` 会直接触发全量同步：

``` C
int slaveTryPartialResynchronization(int fd, int read_reply) {
    ... ...
    if (!read_reply) {
    /* Initially set master_initial_offset to -1 to mark the current
        * master run_id and offset as not valid. Later if we'll be able to do
        * a FULL resync using the PSYNC command we'll set the offset at the
        * right value, so that this information will be propagated to the
        * client structure representing the master into server.master. */
        server.master_initial_offset = -1;

        if (server.cached_master) {
            psync_replid = server.cached_master-&gt;replid;
            snprintf(psync_offset,sizeof(psync_offset),&quot;%lld&quot;, server.cached_master-&gt;reploff+1);
            serverLog(LL_NOTICE,&quot;Trying a partial resynchronization (request %s:%s).&quot;, psync_replid, psync_offset);
        } else {
            serverLog(LL_NOTICE,&quot;Partial resynchronization not possible (no cached master)&quot;);
            psync_replid = &quot;?&quot;;
            memcpy(psync_offset,&quot;-1&quot;,3);
        }

        /* Issue the PSYNC command */
        reply = sendSynchronousCommand(SYNC_CMD_WRITE,fd,&quot;PSYNC&quot;,psync_replid,psync_offset,NULL);
        if (reply != NULL) {
            serverLog(LL_WARNING,&quot;Unable to send PSYNC to master: %s&quot;,reply);
            sdsfree(reply);
            aeDeleteFileEvent(server.el,fd,AE_READABLE);
            return PSYNC_WRITE_ERROR;
        }
        return PSYNC_WAIT_REPLY;
    }
    ... ...
}
```

在接收`master node` 应答时，会调用`slaveTryPartialResynchronization(master_fd, 1)`去解析服务端的响应：

``` C
int slaveTryPartialResynchronization(int fd, int read_reply) {
    ... ...
    if (!strncmp(reply,&quot;+FULLRESYNC&quot;,11)) { 
        // 全量同步 
        char *replid = NULL, *offset = NULL;

        /* FULL RESYNC, parse the reply in order to extract the run id
         * and the replication offset. */
        ... ...
        // 保留 master 发送的 replid
        // server.master_replid = replid
        memcpy(server.master_replid, replid, offset-replid-1);
        server.master_replid[CONFIG_RUN_ID_SIZE] = '\0';
        // 保留 master 发送的 offset
        // server.master_initial_offset = offset
        server.master_initial_offset = strtoll(offset,NULL,10);
        ... ...
        return PSYNC_FULLRESYNC;
    }

    if (!strncmp(reply,&quot;+CONTINUE&quot;,9)) {
        // 开启增量同步
        ... ... 
        // 比较 master 返回的 replid 与 slave 当前保留的 replid
        // 在发生 failover 或 master 更新 replid 时会触发
        if (strcmp(new,server.cached_master-&gt;replid)) {
            
            /* Master ID changed. */
            serverLog(LL_WARNING,&quot;Master replication ID changed to %s&quot;,new);

            /* Set the old ID as our ID2, up to the current offset+1. */
            // 保留 上一个周期的 replid
            // server.replid2 = cached_master-&gt;replid
            memcpy(server.replid2,server.cached_master-&gt;replid,
                sizeof(server.replid2));
            // 保留 上一个周期的 offset
            server.second_replid_offset = server.master_repl_offset+1;

            /* Update the cached master ID and our own primary ID to the
                * new one. */
            // 更新 replid
            // server.replid = new replid
            // server.cached_master-&gt;replid = new replid
            memcpy(server.replid,new,sizeof(server.replid));
            memcpy(server.cached_master-&gt;replid,new,sizeof(server.replid));

            /* Disconnect all the sub-slaves: they need to be notified. */
            disconnectSlaves();
        }
        ... ... 
    }
    ... ...
}
```

如果返回结果为 `+FULLRESYNC` 则直接更新`master_replid` 和 `master_initial_offset`，后续进入全量同步流程；如果返回 `+CONTINUE` 说明可以进入增量同步流程，同时将 `master node` 返回的 `replid` 与当前的 `cached_master-&gt;replid` 进行比较，来决定是否更新 `server.replid` 和 `cached_master-&gt;replid`。整体流程如下：

``` mermaid!
graph TB
    start([slave try to PSYNC])
    exit([end])

    cached{cached_master is NULL ?}
    sync[/PSYNC ? -1/]
    psync[/PSYNC cached_master-&gt;replid cached_master-&gt;reploff+1/]
    response[[receive master respone]]
    sync_res{ +FULLRESYNC replid offset ?}
    update_full[server.master_initial_offset = offset&lt;br&gt;server.master_replid = replid]
    psync_res{ +CONTINUED replid ?}
    new_psync{ cached_master-&gt;replid&lt;br&gt;== replid ?}
    update_psync[server.replid2=cached_master-&gt;replid&lt;br&gt;server.second_replid_offset = server.master_repl_offset+1&lt;br&gt;server.replid = replid&lt;br&gt;cached_master-&gt;replid = replid&lt;br&gt;]

    start--&gt;cached
    cached--&gt;|Yes|psync-.-&gt;response
    cached--&gt;|No|sync-.-&gt;response
    response--&gt;sync_res

    sync_res--&gt;|Yes|update_full--&gt;|full resync|exit
    sync_res--&gt;|No|psync_res

    psync_res--&gt;|Yes|new_psync
    psync_res--&gt;|No, retry or not supported|exit

    new_psync--&gt;|Yes, psync|exit
    new_psync--&gt;|No|update_psync--&gt;|psync|exit
```

`replid2` 来源除上面提到的情况，还有一种情况是在发生`failover`时，候选的`slave node`主动保留上一轮的`replicas id + offset`，其具体实现在[replicationUnsetMaster](https://github.com/redis/redis/blob/5.0/src/replication.c#L448)函数，下面会阐述`failover`时与`psync`有关的细节。

在上述的流程中，参与`PSYNC`判断的 `replid`如下：
master 视角：

+ server.replid
+ server.replid2

slave 视角：

+ cached_master-&gt;replid

`server.replid` 赋值来源：

+ master 视角：
    1. 开启新的 `replication story`，生成新`replid`（详情见`changeReplicationId` 函数）；
    2. 节点可以在重启时使用 [loadDataFromDisk](https://github.com/redis/redis/blob/5.0/src/server.c#L4067)函数从 `RDB` 文件中恢复同步的 `metadata`，对 `replid` 进行初始化（将 `rsi.repl_id,` 赋值给 `replid`）；
+ slave 视角：
    1. 执行 `replication shake`，当`master node`开启新的 `replication story`时，更新`replid`为`new replid`（详情见`slaveTryPartialResynchronization`函数）；
    2. 执行完 `full resync` 后将 `master-&gt;replid` 赋值给 `replid` (详情见 `readSyncBulkPayload`)；
    3. 节点可以在重启时使用 [loadDataFromDisk](https://github.com/redis/redis/blob/5.0/src/server.c#L4067)函数从 `RDB` 文件中恢复同步的 `metadata`，对 `replid` 进行初始化（将 `rsi.repl_id,` 赋值给 `replid`）；

`server.replid2` 赋值来源：

+ master 视角：发生`failover`，提升为`master node`，将上一轮的 `server.replid`赋值给`server.replid2`（详情见`shiftReplicationId`函数）；
+ slave 视角：执行 `replication shake`，当`master node`开启新的 `replication story`时，将上一轮的 `cached_master-&gt;replid`赋值给`server.replid2`（详情见`slaveTryPartialResynchronization`函数）；

`cached_master-&gt;replid` 赋值来源：

+ master 视角：发生 failover 后 `master node` 切换为 `slave node`，将 `service.replid` 赋值给 `cached_master-&gt;replid`（此时为了与 `new master node` 执行 `replication handshake`，以自身为蓝本创建一个 `cached_master`，详情见`replicationCacheMasterUsingMyself`函数）；

+ slave 视角：
    1. 执行完 `full resync` 将 `master_replid`（`master node` 通过 `+FULLRESYNC` 命令发送的`replid`） 赋值给 `master-&gt;replid`，当发生 `failover` 时 `master client` 会被 cache 为`cached_master client`（详情见`replicationCreateMasterClient`函数）；
    2. 执行 `replication shake`，当`master node`开启新的 `replication story`时，更新`cached_master-&gt;replid`为`new replid`（详情见`slaveTryPartialResynchronization`函数）；

 ![replid 状态图](/assets/images/redis_psync_protocol/replid_trans.svg) 

`redisServer`作为一个通用结构体，即会在 `slave` 角色下使用也会在 `master` 角色下使用。在 failover 的情况下会发生角色的转换，所以要分两个视角阐述赋值来源。

同时要考虑另一种情况，即一个`slave node`也可能存在依附于它的`slave node`，所以弄清楚上面涉及的 `replid` 来源，也对下面针对这种情况的分析有好处。

到此可以回答上面提出的第一个问题：

1. replid 是如何产生的？它是如何在节点之间同步的？

    replid是由随机函数生成的长达40bytes的id，它在replication shake 期间， `master node` 通过 `+FULLRESYNC` 或 `+CONTINUE` 命令发送给 `slave node`，`slave node` 通过 `PSYNC` 命令向 `master node` 出示自己当前对应的 replicas id。

本节讲述了`replid`的产生、流转和作用，下面讲述 `replicas` 中`offset` 如何维持全局统计。

## offset

参与 `PSYNC` 流程判断的 `offset` 如下：

master 视角：

+ server.second_replid_offset
+ server.repl_backlog_off

slave 视角：

+ cached_master-&gt;reploff

cached_master-&gt;reploff 赋值来源：

+ slave 视角：
    1. 更新从 master 接收并成功执行的数据偏移量（详情见 `processInputBuffer` 函数）；
    2. 执行完 `fullresync` 后将 `server.master_initial_offset` 赋值给 `master-&gt;reploff`（详情见 `replicationCreateMasterClient` 函数）；

    注意：以上都是在 `master client` 中得到的赋值，而后 `master client` 转化为 `cached_master client`。

server.second_replid_offset：

+ master 视角：
    在 failover 过程中，节点的角色由 `slave` 切换到 `master` ，将 `server.master_repl_offset+1`赋值给`server.second_replid_offset`（详情见 `shiftReplicationId` 函数）；
+ slave 视角：
   执行 `replication shake`，当`master node`开启新的 `replication story`时，将 `server.master_repl_offset+1`赋值给`server.second_replid_offset` (详情见`slaveTryPartialResynchronization`函数）；

注意：将`master_repl_offset+1`赋值给 `second_replid_offset` 是由于 `slave` 会将它期望收到数据的首字节 `offset` 发给依附的`master node`。假设两个节点相同的数据为 50 bytes，则下次 `slave` 在 `PSYNC`中发送的 `offset` 为 51。

server.repl_backlog_off：

+ master 视角：
    1. 开启了 `backlog` 时，会将接收到 `client` 端下发的命令缓冲在 `backlog` 中，将 `backlog` 中**首字节的偏移量**赋值给 `server.repl_backlog_off`（详情见`feedReplicationBacklog`函数）；

    2. 在创建 `backlog` 和 重置 `backlog size` 时，将`master_repl_offset+1` 赋值给 `server.repl_backlog_off`（详情见`createReplicationBacklog` 和 `resizeReplicationBacklog` 函数）；
+ slave 视角：
    1. 开启了 `backlog` 时，会将接收到的`master` 端同步的数据缓冲在 `backlog` 中，将 `backlog` 中**首字节的偏移量**赋值给 `server.repl_backlog_off`（详情见`feedReplicationBacklog`函数）；

    2. 在创建 `backlog` 和 重置 `backlog size` 时，将`master_repl_offset+1` 赋值给 `server.repl_backlog_off`（详情见`createReplicationBacklog` 和 `resizeReplicationBacklog` 函数）；

上述的字段中依赖 `server.master_repl_offset`，其赋值来源如下：

+ master 视角：
    1.开启了 `backlog` 时，会将接收到 `client` 端下发的命令缓冲在 `backlog` 中，将接收到数据的最末端byte对应的 `offset` 赋值给 `server.master_repl_offset`(详情见`feedReplicationBacklog`函数）；
+ slave 视角：
    1.开启了 `backlog` 时，会将接收到的`master` 端同步的数据缓冲在 `backlog` 中，将接收到数据的最末端byte对应的 `offset` 赋值给 `server.master_repl_offset`(详情见`feedReplicationBacklog`函数）；
    2. 完成 `full resync` 后将 `server.master-&gt;reploff` 赋值给 `server.master_repl_offset`(详情见 `readSyncBulkPayload` 函数)；
    3.节点可以在重启时使用 [loadDataFromDisk](https://github.com/redis/redis/blob/5.0/src/server.c#L4067)函数从 `RDB` 文件中恢复同步的 `metadata`，对 `master_repl_offset` 进行初始化（将 `repl_offset` 赋值给 `master_repl_offset`）
    注意：redis节点重启后，replication meta 数据可以从 RDB 文件中恢复，当使用 Aof 格式的日志格式时无法支持此功能。可以使用`SHUT DOWN`命令去关闭节点同时生成一个 RDB 文件，这在节点更新的时候很有用，在重启恢复内存后可以使用部分同步继续更新数据（在条件满足的情况下）。

当发生 `failover` 时，提升 `slave` 到 `master` 的过程中 `master_repl_offset` 不会被修改，即`new master` 会延续之前的 `offset` 继续递增。

![reploff 状态图](/assets/images/redis_psync_protocol/reploff_trans.svg) 

至此可以回答上面第二个问题：

2. offset是如何统计的？failover后，新的 master 节点是否会继承 failover 前的 offset 记录向继续向上递增？

    redisServer 中有多处 `offset`，对于 `master` 而言最重要的是`master_repl_offset`，这是用于记录当前同步数据的最高水位，在向 `backlog`中写入数据时会按 data_bytes_len 向上累加 ；对于 `slave` 而言最重要的是 `master-&gt;reploff`，这是用于记录同步完成（即已经被成功执行）数据的最高水位，会按同步成功数据的 data_bytes_len 向上累加。

    在 failover 后，`new master` 会继承上一轮的 `offset` 继续向上累加。简单的说 **PSYNC replid offset 发送的是当前的 replid 和 期待数据的起始偏移(已经处理的数据偏移量 + 1)**。

## backlog

`backlog` 的是一个基于连续内存的`ringbuff`，下面简单讲解一下它的实现。

1. 初始化一个 size 为 10 的`backlog`，如下图所示：

    ![backlog_init 图](/assets/images/redis_psync_protocol/backlog_init.svg)

2. 向其中写入 5 bytes 数据，如下图所示：

    ![backlog_write 图](/assets/images/redis_psync_protocol/backlog_write.svg)

    此时`backlog`尚未写满，`backlog_off`不会发生变动。

3. 继续向其中写入 8 bytes 数据，如下图所示：

    ![backlog_write1 图](/assets/images/redis_psync_protocol/backlog_write_1.svg)

其具体实现如下：

``` C
/* Add data to the replication backlog.
 * This function also increments the global replication offset stored at
 * server.master_repl_offset, because there is no case where we want to feed
 * the backlog without incrementing the offset. */
void feedReplicationBacklog(void *ptr, size_t len) {
    unsigned char *p = ptr;

    server.master_repl_offset += len;

    /* This is a circular buffer, so write as much data we can at every
     * iteration and rewind the &quot;idx&quot; index if we reach the limit. */
    while(len) {
        size_t thislen = server.repl_backlog_size - server.repl_backlog_idx;
        if (thislen &gt; len) thislen = len;
        memcpy(server.repl_backlog+server.repl_backlog_idx,p,thislen);
        server.repl_backlog_idx += thislen;
        if (server.repl_backlog_idx == server.repl_backlog_size)
            server.repl_backlog_idx = 0;
        len -= thislen;
        p += thislen;
        server.repl_backlog_histlen += thislen;
    }
    if (server.repl_backlog_histlen &gt; server.repl_backlog_size)
        server.repl_backlog_histlen = server.repl_backlog_size;
    /* Set the offset of the first byte we have in the backlog. */
    server.repl_backlog_off = server.master_repl_offset -
                              server.repl_backlog_histlen + 1;
}
```

对于 `master` 和 `slave` 角色的节点而言，只有在调用 [feedReplicationBacklog](https://github.com/redis/redis/blob/5.0/src/replication.c#L127) 函数时会增长该字段，即只有在向 `backlog` 中添加数据的时候。

不同的是 `master` 是在接收客户端命令时会在 [replicationFeedSlaves](https://github.com/redis/redis/blob/5.0/src/replication.c#L174) 函数中执行 `backlog` 备份，`slave` 是在 [replicationFeedSlavesFromMasterStream](https://github.com/redis/redis/blob/5.0/src/replication.c#L279) 函数中执行备份，用于支持依附在该节点上的  `sub-slaves` 的部分同步需求。

``` C
/* Propagate write commands to slaves, and populate the replication backlog
 * as well. This function is used if the instance is a master: we use
 * the commands received by our clients in order to create the replication
 * stream. Instead if the instance is a slave and has sub-slaves attached,
 * we use replicationFeedSlavesFromMaster() */
void replicationFeedSlaves(list *slaves, int dictid, robj **argv, int argc) {
    ... ...
}

/* This function is used in order to proxy what we receive from our master
 * to our sub-slaves. */
#include &lt;ctype.h&gt;
void replicationFeedSlavesFromMasterStream(list *slaves, char *buf, size_t buflen) {
    ... ...
}
```

backlog的创建是通过 [createReplicationBacklog](https://github.com/redis/redis/blob/5.0/src/replication.c#L78) 函数：

``` C
void createReplicationBacklog(void) {
    serverAssert(server.repl_backlog == NULL);
    server.repl_backlog = zmalloc(server.repl_backlog_size);
    server.repl_backlog_histlen = 0;
    server.repl_backlog_idx = 0;

    /* We don't have any data inside our buffer, but virtually the first
     * byte we have is the next byte that will be generated for the
     * replication stream. */
    server.repl_backlog_off = server.master_repl_offset+1;
}
```

其调用的场景如下：

1. `syncCommand`函数：执行 `PSYNC` 命令，当前`backlog ==NULL &amp;&amp; listLength(server.slaves) == 1`，则创建 `backlog`：

    ``` C
    // 此时进入 FULLRESYNC 后续流程
    c-&gt;flags |= CLIENT_SLAVE;
    listAddNodeTail(server.slaves,c);

    /* Create the replication backlog if needed. */
    // 当 backlog 为空，且当前 client 是唯一的 slave，初始化 backlog；
    if (listLength(server.slaves) == 1 &amp;&amp; server.repl_backlog == NULL) {
        /* 因为新的backlog没有与过去相关的历史记录，
         * 所以在创建新的backlog的同时去更新replid并清理replid2 */
        changeReplicationId();
        clearReplicationId2();
        createReplicationBacklog();
    }
    ```

2. `readSyncBulkPayload`函数：执行完全量同步后开启`backlog`：

    ``` C
    if (eof_reached) {
        ... ...
        /* slave node 是否需要创建 backlog 
         * 与是否有 sub-slaves node 无关，开启 
         * backlog 主要是为了在切换为 master 时可以
         * 支持 psync. */
        if (server.repl_backlog == NULL) createReplicationBacklog();
        ... ...
    }
    ```

3. `slaveTryPartialResynchronization` 函数：尝试执行部分同步，在返回`+CONTINUE` 后初始化 `backlog`：

    ``` C
    if (!strncmp(reply,&quot;+CONTINUE&quot;,9)) {
        ... ...
         /* 即使实例重启后从RDB文件中恢复了PSYNC metadata，
          * 也无法初始化 backlog，需要创建它*/
        if (server.repl_backlog == NULL) createReplicationBacklog();
        return PSYNC_CONTINUE;
    }
    ```

综上对于 `master` 而言，某些情况可能会不开启 `backlog`；对于 `slave` 而言无论是否有`sub-slaves` 都会开启 `backlog`（在支持PSYNC 协议的前提下）。

`backlog`的释放是通过 [freeReplicationBacklog](https://github.com/redis/redis/blob/5.0/src/replication.c#L117) 函数：

``` C
void freeReplicationBacklog(void) {
    serverAssert(listLength(server.slaves) == 0);
    zfree(server.repl_backlog);
    server.repl_backlog = NULL;
}
```

其调用场景如下：

1. `replicationCron` 函数：当master没有slave且超过一段时间（**由配置指定，如果为0则不会被释放**）后，会释放`backlog`空间：

    ``` C
    ... ...
    /* 如果一个master 没有 slaves，
     * 为了节省内存可以在一定时间过后回释放`backlog`占用的内存，
     * 但是 slave 不可以执行这种操作。 */
    if (listLength(server.slaves) == 0 &amp;&amp; server.repl_backlog_time_limit &amp;&amp;
        server.repl_backlog &amp;&amp; server.masterhost == NULL)
    {
        time_t idle = server.unixtime - server.repl_no_slaves_since;

        if (idle &gt; server.repl_backlog_time_limit) {
            /* 当释放 backlog的时候，需要更新 replid 并清理 replid2。
             * 当backlog不存在时 master_repl_offset 无法更新，
             * 如果此时仍然保持replid不变可能会造成以下的问题：
             * 1. 当前角色为 master
             * 2. 某个 slave 被切换为 new master，
             * 它的 replid2 与 当前 replid 相同
             * 3. 此时仍然是 master 未切换为 slave，并接收了写入命令，
             * 但是 master_repl_offset 没有更新
             * 4. 切换为 slave，由于接受了写入此时数据不一致，
             * 此时向 new master 发起 PSYNC 请求会被接受，
             * 返回 +CONTINUE new_replid。
            */
            changeReplicationId();
            clearReplicationId2();
            freeReplicationBacklog();
            serverLog(LL_NOTICE,
                &quot;Replication backlog freed after %d seconds &quot;
                &quot;without connected replicas.&quot;,
                (int) server.repl_backlog_time_limit);
        }
    }
    ```

2. `syncWithMaster`函数：当 `slave` 执行 PSYNC 失败（由于master不支持，或者校验未通过），需要清理 `backlog`，强制 sub-slaves 执行 resync。

    ``` C
        ... ...
        if (psync_result == PSYNC_CONTINUE) {
            serverLog(LL_NOTICE, &quot;MASTER &lt;-&gt; REPLICA sync: Master accepted a Partial Resynchronization.&quot;);
            return;
        }

        /* 当 PSYNC 失败或不支持时，如果当前节点有 sub-slaves，
         * 则希望 sub-slaves 与 当前节点一起执行 resync。
         * master 可能传输给当前节点完全不同的data set，
         * 此时无法再去为sub-slaves 提供增量同步的服务。 */
        disconnectSlaves(); /* 强制 sub-slaves 执行 resync. */
        freeReplicationBacklog(); /* 释放 backlog，使得  sub-slaves 无法执行 PSYNC. */
        ... ...
    ```

`backlog` 的size调整是通过 [resizeReplicationBacklog](https://github.com/redis/redis/blob/5.0/src/replication.c#L96) 函数：

``` C
/* This function is called when the user modifies the replication backlog
 * size at runtime. It is up to the function to both update the
 * server.repl_backlog_size and to resize the buffer and setup it so that
 * it contains the same data as the previous one (possibly less data, but
 * the most recent bytes, or the same data and more free space in case the
 * buffer is enlarged). */
void resizeReplicationBacklog(long long newsize) {
    if (newsize &lt; CONFIG_REPL_BACKLOG_MIN_SIZE)
        newsize = CONFIG_REPL_BACKLOG_MIN_SIZE; // 默认大小为 16*1024
    if (server.repl_backlog_size == newsize) return;

    server.repl_backlog_size = newsize;
    if (server.repl_backlog != NULL) {
        /* What we actually do is to flush the old buffer and realloc a new
         * empty one. It will refill with new data incrementally.
         * The reason is that copying a few gigabytes adds latency and even
         * worse often we need to alloc additional space before freeing the
         * old buffer. */
        zfree(server.repl_backlog);
        server.repl_backlog = zmalloc(server.repl_backlog_size);
        server.repl_backlog_histlen = 0;
        server.repl_backlog_idx = 0;
        /* Next byte we have is... the next since the buffer is empty. */
        server.repl_backlog_off = server.master_repl_offset+1;
    }
}
```

其调用场景如下：

1. `loadServerConfigFromString`函数：启动时加载配置项 repl-backlog-size；

2. `configSetCommand`函数：通过 config set repl-backlog-size size 命令；

到此可以回答上面提出的第3个问题：
3. backlog的作用是什么？节点在 slave 角色下是否会开启 backlog？

`backlog` 用于缓存 `client` 下发命令去支持 `PSYNC` 协议；`slave` 也会开启 `backlog`，除了用于服务 `sub-slaves`，还用于切换为主时提供`PSYNC`。

## failover

在这一小节主要专注于`failover`过程中，与 `replications` 相关的操作，failover的触发时机和选举机制会在其他章节中陈述。

当一个 `slave` 切换为 `master` 时会执行 [replicationUnsetMaster](https://github.com/redis/redis/blob/5.0/src/replication.c#L2018) 函数来清理 `old master` 信息并生成自己成为 `new master` 的相关信息：

``` C

/* 取消replications，设置自己为master. */
void replicationUnsetMaster(void) {
    if (server.masterhost == NULL) return; /* Nothing to do. */
    sdsfree(server.masterhost);
    server.masterhost = NULL;
    /* 当一个 slave 切换为master时，
     * 继承自前一任master的 replid 
     * 赋值给 sencondary ID，同时为
     * 新的 replication history 
     * 创建一个 new replid。 */
    shiftReplicationId();
    if (server.master) freeClient(server.master);
    replicationDiscardCachedMaster();
    cancelReplicationHandshake();
    /* 因为需要将当前 replid的变更传播给每一
     * sub-slaves，所以与所有 sub-slaves
     * 断开连接是必须的。
     * 由于可以执行 `PSYNC`重新开启复制流，
     * sub-slaves可快速重连. */
    disconnectSlaves();
    server.repl_state = REPL_STATE_NONE;

    /*需要确保 new master 以一个 SELECT 语句重启复制流*/
    server.slaveseldb = -1;
    
    /* 将当前时间视为没有 slaves 存在的起始时间，
     * 这个时间点用于统计backlog 的生存时间。
     * 如果不设置，当 slave 没有立即链接进来时，
     * backlog 可能会在 replicationCron 中被释放掉。 */
    server.repl_no_slaves_since = server.unixtime;
}
```

综上，在`slave` 提升为 `master` 时执行的步骤如下：

1. 清理 master_host，备份并生成新的 replid；
2. 释放 master 和 cached master client；
3. 取消 replication shake；
4. 与所有的 sub-slaves 断开链接；
5. 清理 slaveseldb 状态，更新 slave 不存在的时间为当前时间；

该函数调用的时机如下：

1. clusterFailoverReplaceYourMaster：执行 `cluster failover` 命令，或者执行自动故障转移（slave对应的master处于 FAIL 状态，发起自动故障转移）；
2. clusterReset：执行 `cluster reset` 命令；
3. replicaofCommand：执行`replicaof no one` 命令；

当 `master` 切换为 `slave` 时要执行[replicationSetMaster](https://github.com/redis/redis/blob/5.0/src/replication.c#L1993)函数，清理`relications`的状态并利用自身信息创建`cached_master`：

``` C
/* Set replication to the specified master address and port. */
void replicationSetMaster(char *ip, int port) {
    int was_master = server.masterhost == NULL;

    // 赋值 master ip、port
    sdsfree(server.masterhost);
    server.masterhost = sdsnew(ip);
    server.masterport = port;
    if (server.master) { 
        // 清理master client
        freeClient(server.master);
    }

    /* 断开所有阻塞. */
    disconnectAllBlockedClients(); 

    /* 强迫slave 重新建立 replications stream，
     *同步 replid 的更改。 */
    // 断开所有slaves；
    disconnectSlaves();
    // 取消 replication handshake；
    cancelReplicationHandshake();
    if (was_master) {
        // 清理 cached_master
        replicationDiscardCachedMaster();
        // 利用自身信息创建 cached master；
        replicationCacheMasterUsingMyself();
    }
    // 重置当前 repl_state 为 REPL_STATE_CONNECT；
    server.repl_state = REPL_STATE_CONNECT;
}
```

综上 `master` 切换为 `slave` 执行的步骤如下：

1. 保存 new master 的 host 信息
2. 断开所有处于`CLIENT_BLOCKED`状态的 `client` ， 向它们发送一个`-UNBLOCKED`的错误；
3. 断开所有的`slaves`，强迫其重新执行同步流程，切换到新的 `replication story`；
4. 清理 cached_master，利用自身信息创建 cached_master 用于 `PSYNC`；
5. 重置当前的 `repl_state` 状态为 `REPL_STATE_CONNECT`；

该函数的调用时机如下：

1. clusterUpdateSlotsConfigWith：更新槽的配置，当一个`master node`没有对应槽去服务时，会被迁移为其他 `master node`的`slave`；当一个 `slave node` 的`master node`没有槽分配时，会被切换到其他的 `slots owner`；
2. replicaofCommand：`master node`执行`slaveof ip port`命令(在 &gt;=redis 5.0 的版本，使用`replicaof`替换；在cluster中不允许执行 `slaveof`和`replicaof`命令)，迁移为指定节点的`slave`；

到此可以回答上面提出的第4个问题：

4. 在发生 failover 后如何继续 psync ？
 
    在发生 failover 后一个 `slave node` 会提升为 `master node`(在哨兵模式下由哨兵选择一个`slave node`切换为`master node`，以优先级高的优先，优先级相同的情况下以 offset 高的优先，offset 相同的情况下以节点号小的优先；在集群模式下由同复制集中的`slave nodes`推举一个`slave node`，以 offset 大的优先，再由其他`master nodes` 去认证，在取得大多数 `master nodes` 认证后，完成切换。)，同时记录上一轮的 `replid` 为`replid2` 和 `master_repl_offset+1` 为 `second_replid_offset`（详情见[replicationUnsetMaster](https://github.com/redis/redis/blob/5.0/src/replication.c#L2018)函数）。
    当其他 `slave nodes` 去向 `new master node` 发送`PSYNC cache_master-&gt;replid cache_master-&gt;reloff+1` 去完成 `handshake` 流程时，可以满足 `server.replid2 == master_replid &amp;&amp; psync_offset &lt;=  server.second_replid_offset` 的判断条件。

## 参考文档

[^1]: 陈雷. Redis 5设计与源码分析, 北京, 机械工业出版社, July 2020

[^2]: Redis Ltd. Replication. Aguest 25 2020, https://redis.io/topics/replication

[^3]: Souvik. Redis diskless replication: What, how, why and the caveats. March 8 2020, https://deepsource.io/blog/redis-diskless-replication/</content><author><name>Zhipeng Wang</name><email>wangzhipenghyc@163.com</email></author><summary type="html"></summary></entry><entry><title type="html">Design_pattern</title><link href="http://localhost:4000/design_pattern/" rel="alternate" type="text/html" title="Design_pattern" /><published>2021-06-15T00:00:00+08:00</published><updated>2021-06-15T00:00:00+08:00</updated><id>http://localhost:4000/design_pattern</id><content type="html" xml:base="http://localhost:4000/design_pattern/">&lt;style type=&quot;text/css&quot;&gt;
@import url(&quot;/assets/css/simple.css&quot;)
&lt;/style&gt;

# Design Pattern

 23 classification and list

## Creational patterns

### Abstract factory
抽象工厂模式，提供一个用于创建相关或依赖对象族的接口，而无需指定其具体类。

**使用场景**：当一个场景需要引入多个接口，尤其是针对不同资源要绑定不同的对象时。例如：一个`player`加入战场的时候要分配装备，而且不同`阵营`的`player` 分配的装备型号不同。
```mermaid!
classDiagram
class Factory {
    CreateTank() Tank
    CreateGun() Gun
}
&lt;&lt;interface&gt;&gt; Factory

class Tank{
    Run()
    Fire()
}
&lt;&lt;interface&gt;&gt; Tank

class Gun{
    Shoot()
}
&lt;&lt;interface&gt;&gt; Gun

class sovietFactory{
    CreateTank() Tank
    CreateGun() Gun
}

class t34 {
    -uint bullet
    -uint gas
    -string engine
    Run()
    Fire()
}

class ak47 {
    -uint clip
    Shoot()
}

class germanyFactory{
    CreateTank() Tank
    CreateGun() Gun
}

class tigerTank{
    -uint bullet
    -uint gas
    -string engine
    Run()
    Fire()
}

class mp40 {
    -uint clip
    Shoot()
}

Factory ..&gt; Tank
Factory ..&gt; Gun

sovietFactory ..|&gt; Factory
sovietFactory ..&gt; ak47
sovietFactory ..&gt; t34
t34 ..|&gt; Tank
ak47 ..|&gt; Gun

germanyFactory ..|&gt; Factory
germanyFactory ..&gt; mp40
germanyFactory ..&gt; tigerTank
tigerTank ..|&gt; Tank
mp40 ..|&gt; Gun
```

调用方不需要了解实例化过程和接口对应的实现类，只要能`Tank`可以开、`Gun`可以射击，他只需要找到正确的`factory`。golang的示例代码如下：
``` go
type Factory interface{
    CreateTank() Tank
    CreateGun() Gun
}

type Tank interface{
    Fire()
    Run()
}

type Gun()interface{
    Shot()
}

type sovietFactory struct{
}

func (sf *sovietFactory) CreateTank() Car{
    return &amp;t34{
        gas: 700 
        engine: &quot;t34&quot;
        bullet: 50
    }
}

func (sf *sovietFactory) CreateGun() Gun {
    return &amp;ak47{
        clip: 10000
    }
} 

type t34 struct {
    gas uint64
    bullet uint64
    engine string
}

func (t *t34)Run(){
    t.gas--
}

func (t *t34)Fire(){
    t.bullet--
}

type ak47 struct{
    uint64 clip
}

func (a *akf7)Shot(){
    a.clip--
}

type germanyFactory struct{
}

func (gf *germanyFactory) CreateTank() Car{
    return &amp;tigerTank{
        gas: 1000
        engine: &quot;tiger&quot;
        bullet: 60
    }
}

func (gf *germanyFactory) CreateGun() Gun {
    return &amp;mp40{
        clip: 8000
    }
} 

type tigerTank struct {
    gas uint64
    bullet uint64
    engine string
}

func (t *tigerTank)Run(){
    t.gas--
}

func (t *tigerTank)Fire(){
    t.bullet--
}

type mp40 struct{
    uint64 clip
}

func (a *mp40)Shot(){
    a.clip--
}

func openFactory(power string) (f Factory) {
    switch power {
        case `soviet`:
            f = &amp;sovietFactory{}
        case `germany`:
            f = &amp;germanyFactory{}
        default:
            f = nil
    }

    return f
}

// caller
if f := openFactory(player.power); f != nil {
    tank := f.CreateTank()
    player.vehicle = tank
    player.weapon.cannon = tank
    player.weapon.gun = f.CreateGun()
}
```

这种设计模式的优缺点显而易见：
+ 优点： 将对象的创建的具体流程与调用方完全屏蔽，调用方无需做额外工作，只要保证调用正确的factory就好了；
+ 缺点： 引入额外的封装代码，尤其当生产类型组合过多的时候，需要实现多种 factory。对调用方约束性较强，当调用方需要某些特性未暴露时，无法通过自己封装创建方法实现；

### Builder
构建器模式，将复杂对象的构造与其表示分开，从而允许相同的构造过程创建各种表示。

**使用场景**：适用于构造一些初始化可变选项特别多的对象，而其中一些选项对某个资源是相同的。例如：`player`在注册游戏时创建了一个`role`，同时还可以选择不同的职业加入战场。
```mermaid!
classDiagram
class RoleBuilder{
    Name()
    Power()
    Gender()
    HeaderImage()
}
&lt;&lt;interface&gt;&gt; RoleBuilder

class Role{
    name
    power
    gender
    headImage
    skill
    speed
    lifeValue
    Show()
}

class SpecialSoldier{
    Name()
    Power()
    Gender()
    HeaderImage()
    GetRole() Role
}

class ArmoredSoldier{
    Name()
    Power()
    Gender()
    HeaderImage()
    GetRole() Role
}

class Director{
    builder RoleBuilder
    Construct()
}

Director ..o RoleBuilder
SpecialSoldier ..&gt; Role
ArmoredSoldier ..&gt; Role
ArmoredSoldier ..|&gt; RoleBuilder
SpecialSoldier ..|&gt; RoleBuilder
```

golang 的示例代码如下：
``` go
type Builder interface {
    Name(name string)
    Power(power string)
    Gender(gender string)
    HeaderImage(headImage string)
}

type Role struct {
    name string
    power string
    gender string
    headImage string
    skill string
    speed uint32
    lifeValue uint64
}

func (r *Role)Show(){
    fmt.Println(*r)
}

type builder struct{
    Role
}

func (b *builder) Name(name string){
    b.name = name
}

func (b *builder)Power(power string){
    b.power = power
}

func (b *builder)Gender(gender string){
    b.gender = gender
}
 
func (b *builder)HeaderImage(headImage string){
    b.headImage = headImage
}

type SpecialSoldier struct{
    builder
}

func (ss *SpecialSoldier)GetRole()(r Role) {
    r = ss.role
    r.skill = &quot;everything&quot;
    r.speed = 50
    r.lifeValue = 150
}

type ArmoredSoldier struct{
    builder
}

func (as *ArmoredSoldier)GetRole()(r Role) {
    r = as.role
    r.skill = &quot;driving tanks&quot;
    r.speed = 30
    r.lifeValue = 100
}

type Director struct{
    builder RoleBuilder
}

func (d *Director)Construct(){
    d.builder.Name(&quot;panda&quot;)
    d.builder.Gender(&quot;male&quot;)
    d.builder.Power(&quot;China&quot;)
    d.builder.HeadImage(&quot;panda.icon&quot;)
}

func NewDirector(b builder) *Director{
    return &amp;Director{
        builder: builder
    }
}

// caller
specialSoldier := &amp;SpecialSoldier{}
NewDirector(specialSoldier).Construct()
pandaSpecSoldier := specialSoldier.GetRole()

armoredSoldier := &amp;ArmoredSoldier{}
NewDirector(armoredSoldier).Construct()
pandaArmSoldier := armoredSoldier.GetRole()
```
在上面的示例代码中利用`golang的组合特性`（*）减少重复性的代码。对于一个职业（`Special Soldier`、`Armored Soldier`等），无论是哪个用户，都可以获得相同属性的角色。

该设计模式的优缺点如下：
+ 优点：调用方可以去按需修改对象内部的成员，控制构造流程；
+ 缺点：每一个种 Product 都要构造一个对应的 Builder，且必须是可变的。

在`golang`中或其他语言中`Builder`还有一种常见的，也十分‘有趣的’使用方法：
``` go
type Role struct {
    name string
    power string
    gender string
    headImage string
}

type builder struct{
    Role
}

func (b *builder)Name(name string)*builder{
    b.name = name
    return b
}

func (b *builder)Power(power string)*builder{
    b.power = power
    return b
}

func (b *builder)Gender(gender string)*builder{
    b.gender = gender
    return b
}
 
func (b *builder)HeaderImage(headImage string)*builder{
    b.headImage = headImage
    return b
}

func (b *builder)GetRole()Role{
    return b.Role
}

func (b*builder)Builder() *builder{
    new := *b
    return &amp;new
}

// caller
builder := &amp;builder{}.Gender(&quot;male&quot;).Power(&quot;china&quot;)
panda := builder.Builder().Name(&quot;panda&quot;).HeadrImage(&quot;panda.icon&quot;).GetRole()
long := builder.Builder().Name(&quot;long&quot;).HeadrImage(&quot;long.icon&quot;).GetRole()
```
这种使用方法一定要注意，在`GetRole()`时返回一个结构体而不是结构体指针，否则构建新对象时容易对已经构造的对象造成污染。使用这种方法的好处是，可以通过‘继承’前一个`builder`，减少大量重复的成员变量赋值，而且也是并发安全的。

### Dependency Injection
依赖注入模式，类通过一个注入器来代替直接创建依赖的对象，是增加代码可扩展性常用的手段。

**使用场景**：简单的说调用方的实现依赖于某个类或服务，该类或服务是它实现上的一个有效组成部分，需要在初始化或者某个流程步骤将依赖类的对象传递给调用方。例如：`player`在一个游戏中需要切换不同的游戏模式。

```mermaid!
classDiagram
class Player{
    service GameService
    Login()
    SetService(GameService)
}

class GameServiceSetter{
    SetService(GameService)
}
&lt;&lt;interface&gt;&gt; GameServiceSetter

class GameService{
    Auth() 
    Start()
}
&lt;&lt;interface&gt;&gt; GameService

class LocalService{
    Auth() 
    Start()
}

class OnlineService{
    Auth() 
    Start()
}
LocalService ..|&gt; GameService
OnlineService ..|&gt; GameService
Player--&gt;GameService
GameServiceSetter..&gt;GameService
Player..|&gt;GameServiceSetter
```

golang的示例代码如下：

``` go
type GameService interface{
    Auth(name, passwd string) bool
    Start()
}

type GameServiceSetter interface{
    SetService(srv GameService)   
}

type srvSetter struct{
    srv GameService
}

func (ss *srvSetter)SetService(srv GameService){
    ss.srv = srv
}

type player Struct{
    name string
    passwd string
    srvSetter
}

func (p *player)Login(){
    if p.srv != nil{
       if p.srv.Auth(p.name, p.passwd){
           p.srv.Start()
       }
    }
}

type LocalService struct {
}

func (s *LocalService) Auth(name, passwd string) bool{
    // TODO
    return true
}

func (s *LocalService) Start() {
    // TODO
    return
}

type OnlineService struct {
}

func (s *OnlineService) Auth(name, passwd string) bool{
    // TODO
    return true
}

func (s *OnlineService) Start() {
    // TODO
    return
}

func SelectService(mode string,s GameServiceSetter){
    switch mode{
    case `online`:
        s.SetService(&amp;OnlineService{})
    case ``: 
        s.SetService(&amp;LocalService{})
    default:
        // TODO
    }
}

// caller
panda := &amp;Player{name:&quot;panda&quot;, passwd:&quot;passwd&quot;}
SelectService(&quot;online&quot;, panda)
panda.Login()
```

通常情况下`GameServiceSetter`这一层抽象是被省略的，经常被以下的模式替代：

``` go
func NewPalyer(..., srv GameService)(p *Player){
    ...
    p.srv = srv
}

// OR

func (p *Player)SetGameService(srv GameService){
    p.srv = srv
}

// OR

type Option func(*Player) 

func WithService(srv GameService){
    return func(p *Player){
        p.srv = srv
    }
}

func NewPlayer(..., ops ... Option)(p *Player){
    ...
    for _, op := range ops{
        op(p)
    }
}
```
将 `GameServiceSetter` 单独的实现，通过组合（*）的方式放在结构体里面也不是必须的，多加一层抽象可以使代码迭代和扩展性上更好，但同时也会引起维护和理解上的困难。
以上替代方案中，第三种方案在 `golang package` 中对于接口的兼容和维护十分有效。

优缺点如下：
+ 优点：在调用方对象的生命周期内，可以通过更换依赖的服务对象来进行模式切换，而不用关心底层具体实现；
+ 缺点：调用方要保障注入时机的可控性，否则可能调用一个未注册的空对象；

### Factory method
工厂模式，定义用于创建单个对象的接口，将实例化创建部分放在子类实现，让子类决定实例化哪个类。

**使用场景**：当获取的资源对象可以通过另一个对象构造和管理时，适合使用工厂模式。例如：`player`在`Game`中创建一个 `Room`，可以选择不同的对战模式。

```mermaid!
classDiagram
class Room{
    +Join()
}
&lt;&lt;interface&gt;&gt; Room

class pvpRoom{
    -int capacity
    +Join()
}

class pvcRoom{
    -int capacity
    +Join()
}

%% makeRoom is &lt;&lt;interface&gt;&gt; function
class Game{
    -List~Room~ rooms
    +OpenRoom() Room
    makeRoom()* Room 
}

class pvcGame{
    makeRoom() Room
}

class pvpGame{
    makeRoom() Room
}

pvcRoom ..|&gt; Room
pvcGame..&gt;pvcRoom
pvcGame--|&gt;Game
pvpRoom ..|&gt; Room
pvpGame..&gt;pvpRoom
pvpGame--|&gt;Game
Game..&gt;Room
```

golang的示例代码如下：
``` go
type Room interface{
    Join()
}

type pvcRoom struct{
    capacity int
}

func (p* pvcRoom)Join(){
    // TODO
}

type pvpRoom struct{
    capacity int
}

func (p* pvpRoom)Join(){
    // TODO
}

type roomMaker interface {
    makeRoom() Room
}

type Game struct{
    rooms []Room
    roomMaker
}

func (g *Game)OpenRoom() Room{
    r := g.makeRoom()
    g.rooms = append(g.rooms, r)
    return r
}

type pvpGame struct{
}

func (pvp *pvpGame)makeRoom(){
    return &amp;pvpRoom{capacity: 10}
}

type pvcGame struct{
}

func (pvc *pvcGame)makeRoom(){
    return &amp;pvcRoom{capacity: 5}
}

func NewPVPGame() *Game{
    return &amp;Gmame{
        roomMaker: &amp;pvpGame
    }
}

func NewPVCGame() *Game{
    return &amp;Gmame{
        roomMaker: &amp;pvcGame
    }
}

// caller
pvpGame := NewPVPGame()
room := pvpGame.OpenRoom()
```
由于golang没有类似虚基类的语法特性，无法实现类图中所展示的关系，只能通过类似`Dependency Injection`的方式来模拟。

其优缺点如下：
+ 优点：将类的构造与调用方隔离，实现解耦合。将底层具体类型的构造实现交给继承的子类，通过拓展子类就可以实现更多工厂类型；
+ 缺点：额外的封装增加开发成本，如果只是简单的几种类型，建议使用`switch`代替。

### Lazy Initialization
懒汉模式：将对象的创建，值的计算或其他昂贵的过程延迟到第一次使用时的策略。此模式在GoF目录中显示为“虚拟代理”，这是代理模式的一种实施策略。

**使用场景**：当访问的资源比较昂贵，而资源又未必一定会被访问的情况下可以使用懒汉模式。例如：`player`在`Game`中请求开启了超清画质模式，该模式需要启动超高清计算引擎。

golang示例代码如下：
``` go

var (
    lowGraphEngine = NewEngine(`lowlevel`)
    commonGraphEngine = NewEngine(`common`)
    highGraphEngine Engine
    once sync.Once
    nowEngine Engine
)

func SwitchEngine(engine string){
    switch engine{
        case `lowlevel`:
            nowEngine=lowGraphEngine
        case `commonlevel`:
            nowEngine=commonGraphEngine
        case `highlevel`:
            // TODO: add lock
            if highGraphEngine == nil {
                highGraphEngine = NewEngine(`highlevel`)
            }
                
            nowEngine=highGraphEngine
        default: 
            // TODO
    }
}  
```

如上示例代码所示，只有在选择开启`highlevel`时候才会去初始化 `highGraphEngine`，示例代码中没有加锁进行资源保护，并非多线程安全的，实际使用时要注意。

### Multiion

### Object pool
对象池模式，经常在各种sdk中见到，主要作用是将资源池化，通过回收再利用的模式，避免频繁创建对象引入的消耗，同时也能减轻对象访问资源的压力。

**使用场景**: 通过socket访问存储服务，调用远程http/https服务等。例如：游戏`client`与`server`频繁的进行数据交换。

```mermaid!
classDiagram
class Pool {
    -List~Client~ pools
    +GetClient() Client
    +RecyleClient(Client) 
}

class Client{
    +Send()
    +Recv()
}
&lt;&lt;interface&gt;&gt; Client

class grpcClient{
    +Send()
    +Recv()
}

grpcClient..|&gt;Client
Pool..&gt;Client
```

golang的示例代码如下：

``` go

type Client interface{
    Send([]byte)
    Recv([]byte)
}

type grpcClient struct{
}

func ( *grpcClient)Send(data []byte){
    //TODO
}

func ( *grpcClient)Recv(data []byte){
    //TODO
}

type ClientPool struct {
    pools chan Client
    conn func() (Client, error)
}

func (p *ClientPool)GetClient(ctx context.Context)(c Client, err error){
    tCtx, cancel := context.WithTimeout(ctx, p.timeOut)
    defer cancel()
    select {
    case c = &lt;-p.pools:
    case &lt;-tCtx.Done():
        err = tCtx.Error()
    }
    return 
}

func (p *ClientPool)RecyleClient(c Client){
    select{
    case p.pools &lt;- c:
    default:
    }
}

// callers:
c, _ := pool.GetClient(context.Background())
c.Send([]byte(&quot;hello world&quot;))
pool.RecycleClient(c)
```

资源池化模式是在各种driver sdk中必定出现的一种模式，主要的目的就是尽量复用与远程服务之间的tcp链接，这样做的好处 ：一、减少重复建立链接带来的时间延迟（三次握手、四次挥手）；二、减少服务端维持链接的开销（mysql社区版中每条链接都会起一个服务线程）。
上面的示例属于比较*简陋*的链接池实现，仅仅实现了复用的功能，不具备弹性扩容、空闲统计、心跳检测的功能。

### Prototype 
原型模式，指定使用原型实例创建的对象类型，并从现有对象的“骨架”创建新对象，从而提高性能并将内存占用量降至最低。简单的来讲就是“复制-粘贴”模式。

```mermaid!
classDiagram

class Gun{
    &lt;&lt;interface&gt;&gt;
    +Shoot()
}

class Prototype{
    +Clone() Gun
}
&lt;&lt;interface&gt;&gt; Prototype

class ak47{
    -int bullet
    +Shoot()
    +Clone() Gun
}

class caller{
    Operation()
}

ak47 ..|&gt; Gun
ak47 ..|&gt; Prototype
caller ..&gt; Prototype
```

golang 示例代码如下：
``` go
type Gun interface {
    Shoot()
}

type Prototype interface {
    Clone() Gun
}

type ak47 struct {
    bullet int
}

func (a *ak47)Shoot(){
    //TODO
}

func (a *ak47)Clone()Gun{
    cp := *a
    return &amp;cp
}

// caller
var proto Prototype = &amp;ak47{bullet:10000}
gun := proto.Clone()
```
这种写法与上面的`builder`示例很相似，实际上也确实有一个偷懒的做法，即将对象本身作为自己的`builder`：
``` go
type ak47 struct {
    bullet int
}

func (a *ak47)Bullet(b int) *ak47{
    a.bullet = b
    return a
}

func (a *ak47)Shoot(){
    //TODO
}

func (a *ak47)Clone()Gun{
    cp := *a
    return &amp;cp
}

// caller
gun := &amp;ak47{}.Bullet(10000).Clone()
```
对于这种写法，作者并不持推荐态度。虽然它节省了工作量，但是会对使用方造成很大的麻烦，使用方要时刻谨小慎微，以保证对象不会被重用，而且一旦出现问题很难排查。

同时作者认为设计模式是灵活多变的，有时一种设计模式可以成为另一种模式的实现，比如抽象工厂模式就可以通过原型模式来实现，而原型模式又可以通过单例模式来实现。

### Resource acquisition is initialization

### Singleton
单例模式，确保一个类只有一个对象，提供一个全局的变量去访问它。
这种模式在`golang`开发的常驻服务类型的软件工程中几乎都能见到，但这种模式也被称之为反对象模式，原因有以下几点：
+ 无法继承：当添加新功能的时候，无法通过一个新类降级为包含该功能，打破了关联分离；
+ 无法控制创建：饮用无法感知是新创建的实例，还是已经存在的；
+ 无法依赖注入：如果通过依赖注入修改属性，所有依赖该实例的对象都会受到影响；
+ 对TDD(Test-driven development)很不友好[3]：每一个单独的测试case都很难单独依赖一个“干净”的实例；

针对以上的问题都有很多对应的编程技巧，在使用单例模式时候以下的用法是一定要避免的：
``` go

type global interface{
    Increment()
}

type globalImpl struct{
}

func (g *globalImpl)Increment(){
}

var GlobalInstance global = &amp;globalImpl{}

// call
GlobalInstance.Increment()
```
直接在代码中调用全局实例，首先，导致代码紧耦合；其次，如果不小心篡改了`GlobalInstance` 指向的内存地址，将是灾难性的。
常用的方法是这样的：
``` go
var globalInstance global

func GetGlobalIns() global {
    return globalInstance
}

// call
GetGlobalIns().Increment()
```
通过一层函数的包装，把全局变量*保护*起来，既可以防止**篡改**的发生，还可以达到**延迟初始化**的效果：
``` go

var (
    globalInstance global 
    once sync.Once
)

func GetGlobalIns() global {
    once.Do(func(){
        if globaleInstance == nil {
            globalInstance = &amp;globalImpl{}
        }
    })
    return globalInstance
}
```
使用 `sync.Once`既可以避免引用未初始化变量的悲剧，也可以避免重复初始化的问题。
单例模式对于测试case的不友好可以通过以下的小技巧解决：
``` go

func MockGlobalInject(mockIns global){
    globalInstance = mockIns
}

// callers
MockGlobalInject(mockIns)
// TODO
```

## Structural patterns

### Adapter, Wrapper, or Translator
适配器模式，主要作用是让一个接口或类型可以支持另一个不适配的接口的功能。
主要的适配模式有两种：
+ 对象适配
```mermaid!
classDiagram

class Tagert{
    &lt;&lt;interface&gt;&gt;
    +Operation()
}

class Adapter {
    -Adaptee apdatee
    +Operation()
}

class Adaptee {
    SpecialOperation()
}

Adapter ..|&gt; Tagert
Adapter --&gt; Adaptee
```

golang 示例代码如下：
``` go
type Target interface {
    Operation()
}

type Adaptee struct {
}

func (a *Adaptee)SpecialOperation(){
}

type Adapter struct {
    at *Adaptee
}

func (a* Adapter)Operation(){
    a.at.SpecialOperation()
}

func Adaptee2Target(at *Adaptee) Target{
    return &amp;Adapter{at: at}
}
```

+ 类型适配
```mermaid!
classDiagram

class Tagert{
    &lt;&lt;interface&gt;&gt;
    +Operation()
}

class Adapter {
    -Adaptee apdatee
    +Operation()
}

class Adaptee {
    SpecialOperation()
}

Adapter ..|&gt; Tagert
Adapter --|&gt; Adaptee
```

golang 示例代码如下：
``` go
type Target interface {
    Operation()
}

type Adaptee struct {
}

func (a *Adaptee)SpecialOperation(){
}

type Adapter struct {
    Adaptee
}

func (a* Adapter)Operation(){
    a.SpecialOperation()
}

func Adaptee2Target() Target{
    return &amp;Adapter{}
}
```
通过附加一层包装，来避免重复性的代码开发，这是非常常用的一种手段，但是也容易造成“层层包装”的现象，尤其是在使用子类多态的情况下。在`C/C++`中使用虚成员函数实现多态，在编译阶段无法确定确切调用的函数（静态联编），只有在运行时才能确认（动态联编），而这会增加调用的耗时。

### Bridge
桥模式：将抽象与其实现分离，从而允许两者独立变化。使用桥模式可是使得抽象和实现在运行时进行绑定选择。

```mermaid!
classDiagram
class Abstraction{
    -Implementor impl
    +function()
}

class Implementor {
    +operationImp()
}

class Implementor1{
    +operationImp()
}

class Implementor2{
    +operationImp()
}

class Abstraction1{
    +function()
}

Abstraction1 --|&gt; Abstraction
Implementor1 ..|&gt; Implementor
Implementor2 ..|&gt; Implementor
Abstraction &quot;0..1&quot; o-- &quot;1&quot; Implementor
```

golang 示例代码如下：
``` go
type Abstraction interface {
    Print()
}

type Implementor interface{
    OperationPrint(int)
}

type Implementor1 struct {
}

func (i1 *Implementor1)OperationPrint(x int){
    fmt.Printf(&quot;%d&quot;, x)
}

type Implementor2 struct {
}

func (i2 *Implementor2)OperationPrint(x int){
    fmt.Printf(&quot;%x&quot;, x)
}

type Abstraction1 struct{
    impl Implementor
    x int
}

func (a1 *Abstraction1)Print(){
    a1.impl.OperationPrint(a1.x)
}

func CreateAbstract(x int, impl Implementor) Abstraction{
    return &amp;Abstraction1{
        x: x,
        impl: impl,
    }
}

// callers
CreateAbstract(1, &amp;Implementor2{}).Print()
CreateAbstract(100, &amp;Implementor1{}).Print()
```
由于 golang 不具备继承的语法特性，上面的示例看起来与`Adapter`模式有些类似，不过两者的侧重点不一样：`Adapter`侧重于不同接口之间的兼容，`Bridge`侧重于抽象和实现的分离。

golang 具有第一公民函数的特性，可以将上面的示例进行简化：
``` go
type Abstraction interface {
    Print()
}

type Abstraction1 struct{
    impl func(int)
    x int
}

func (a1 *Abstraction1)Print(){
    a1.impl(a1.x)
}

func CreateAbstract(x int, impl func(int)) Abstraction{
    return &amp;Abstraction1{
        x: x,
        impl: impl,
    }
}

// callers
CreateAbstract(1, func(x int){fmt.Printf(&quot;%d&quot;, x)}).Print()
CreateAbstract(100, func(x int){fmt.Printf(&quot;%x&quot;, x)}).Print()
```
这是种比较偏`C/C++`的写法，虽然一定程度上破坏了抽象性，但是可以减少代码数量。

### Composite

组合模式，将多个对象以树结构组合，形成部分-整体的层次结构。组合模式可以使客户统一对待单个对象和对象集。

**使用场景**：调用方忽略部分和整体的细节，抽象出整体和部分操作的共性部分提供给调用方。

有两种组合模式：
+ 集合模式（Design for uniformly）
```mermaid!
classDiagram
class Component{
    + operation()
    + add(child)
    + remove(child)
    + getChild()
}

class Leaf{
    + operation()
}

class Composite{
    + operation()
    + add(child)
    + remove(child)
    + getChild()
}

Component &quot;1..*&quot; --o &quot;0..1&quot; Composite
Leaf --|&gt; Component
Composite --|&gt; Component

```

+ 类型安全模式(Design for Type Safety)

```mermaid!
classDiagram
class Component{
    + operation()
}

class Leaf{
    + operation()
}

class Composite{
    + operation()
    + add(child)
    + remove(child)
    + getChild()
}

Leaf --|&gt; Component
Composite --|&gt; Component
Component &quot;1..*&quot; --o &quot;0..1&quot; Composite
```

golang 示例代码如下：

``` go

type Component interface {
    Print()
}

type Leaf struct {
    name string
}

func (l *Leaf)Print(){
    fmt.Println(l.name)
}

func NewLeaf(name string) *Leaf{
    return &amp;Leaf{name: name}
}

type Composite struct{
    childs []Component
}

func (c *Composite)Add(ch Component){
    c.childs = append(c.childs, ch)
}

func (c *Composite)Remove(ch Component){
    for i, c := range c.childs{
        if c == ch {
            c.childs = append(c.childs[:i], c.childs[i+1:]...)
            break
        }
    }
}

func (c *Composite)GetChild(index int) Component{
    if index &lt; len(c.childs){
        return c.childs[index]
    }

    return nil
}

func (c *Composite)Print(){
    for _, c := range c.childs{
        c.Print()
    }
}

// callers
com := &amp;Composite{}
com.Add(NewLeaf(&quot;leaf1&quot;))
com.Add(NewLeaf(&quot;leaf2&quot;))
com.Add(NewLeaf(&quot;leaf3&quot;))


com1 := &amp;Composite{}
com1.Add(com)
com1.Add(NewLeaf(&quot;leaf4&quot;))
com1.Print()
com1.GetChild(1).Print()

// output:
// leaf1
// leaf2
// leaf3
// leaf4
// leaf4
```

上面的示例通过组合模式形成一个n叉树，调用根结点的`Print`方法就可以按照从左到右的顺序打印出所有叶子节点的`name`。

### Decorator
修饰器模式：动态地将附加的责任附加到对象上，以保持相同的接口。 装饰器为子类提供了灵活的替代方案，以扩展功能。
**使用场景**：在原有类成员函数的基础上，扩展功能并且不去修改该类，这样既可以维持存量调用方式不变，也能满足新需求。

```mermaid!
classDiagram
class Component{
    &lt;&lt;interface&gt;&gt;
    +operation()
}

class Concrete{
    +operation()
}

class Decorator{
    -com: Component
    +operation()
}

class Decorator1 {
    -com: Component
    +operation()
}

class Decorator2 {
    -com: Component
    +operation()
}


Component --* Decorator
Concrete ..|&gt; Component
Decorator ..|&gt; Component
Decorator1 --|&gt; Decorator
Decorator2 --|&gt; Decorator
```

golang 示例代码如下：

``` go
type Component interface{
    Print()
}

type Concrete struct{
}

func (*Concrete)Print(){
    fmt.Println(&quot;concreate&quot;)
}

type Decorator struct{
   com Component
}

func (d *Decorator)Print(){
    d.com.Print()
    fmt.Println(&quot;decorator&quot;)
}

func NewDecorator(com Component)Decorator{

}

// callers
var com Component = NewDecorator(&amp;Concrete{})
com.Print()
com = NewDecorator(com)
com.Print()

// output:
// concreate
// decorator
// concreate
// decorator
// decorator
```
以上的示例代码省去了继承的中间步骤，还可以像`Bridge`模式一样，利用`第一公民函数`和`闭包`特性继续简化如下：

``` go
func Decorator(print func())func(){
    return func(){
        print()
        fmt.Println(&quot;decorator&quot;)
    }
}

// callers
Decorator(func(){
    fmt.Println(&quot;concreate&quot;)
})()
// output:
// concreate
// decorator
```

### Extension object

### Facade
门面模式（外观模式）：把一组复杂的接口整合起来，形成几个简单的接口提供给调用方。使得调用放对子系统的依赖最小化、简单化。

**使用场景**：子系统的多个接口可以组成一个资源类供调用方使用。

```mermaid!
classDiagram
class Facade{
    +operation()
}

class Component1{
    +specialOperation1()
}

class Component2{
    +specialOperation2()
}

class Component3{
    +specialOperation3()
}

Facade --&gt; Component1
Facade --&gt; Component2
Facade --&gt; Component3

```

golang的示例代码如下：
``` go
type CPU struct{
}

func (*CPU)Freezy(){
    // TODO
}

func (*CPU)Jump(position uint64){
    // TODO
}

func (*CPU)Execute(){
    // TODO
}

type HardDrive struct{
}

func(hd *HardDrive)Read(lba uint64, size int) []byte{
    // TODO
}

type Memory struct{
}

func (Memory)Load(position uint64, data []byte){
    // TODO
}

type ComputerFacade struct {
    hd  HardDrive
    mem Memory
    cpu Cpu

    kBootAddress uint64
    kBootSector uint64
    kSectorSize int
}

func (cf *ComputerFacade)Start() {
    cf.cpu.Freezy()
    cf.mem.load(cf.kBootAddress, cf.hd(cf.kBootSector, cf.kSectorSize))
    cf.cpu.Jump(cf.kBootAddress)
    cf.cpu.Execute()
}

// caller
cf := ComputerFacade{}
cf.Start()
```
在上面的示例中模拟了电脑启动的过程，可以敏锐的发现`cpu`、`memory`、`hard drive` 都是电脑的有机组成部分，它们可以组成一个`computer`资源类供调用方使用。

### Flyweight
享元模式：多个调用方之间尽量共享依赖单元，从而减少内存的开销。
**使用场景**：在各类算法引擎中或着算法库（openssl）中非常常见，将可变的输入输出与不变的运算逻辑和参数抽离。

```mermaid!
classDiagram
class Flyweight{
    &lt;&lt;interface&gt;&gt;
    +operation(extinsicState)
}

class Flyweight1{
    intrinsicState
    +operation(extinsicState)
}

class UnsharedFlyweight1{
    operation(extinsicState)
}

class FlyweightFactory{
    getFlyweight(key)Flyweight
}

FlyweightFactory ..&gt; Flyweight1 : create and share
Flyweight1 ..|&gt;  Flyweight
UnsharedFlyweight1 ..|&gt;  Flyweight
```

golang的示例代码如下：
``` go
type Color struct {
    red int
    blue int
    yellow int
}

func (c *Color)Set(red, blue, yellow int){
    c.red = red
    c.blue = blue
    c.yellow = yellow
}

type ColorPalette struct {
    colors map[string]Color
}

func (cp *ColorPalette)findByName(name string) *Color {
    c, ok := cp.colors[name]
    if ok {
        return c
    }

    return nil
}

func (cp *ColorPalette)addColor(name string, c *Color){
    cp.colors[name]=c
}

func (cp *ColorPalatte)Palatte(name string)*Color{
    c := cp.findByName(name)
    if c == nil{
        c = &amp;Color{}
        cp.addColor(name, c)
    }

    return c
}

func NewColorPalette()*ColorPalette{
    cp := &amp;ColorPalette{
        colors: map[string]Color{}
    }

    cp.addColor(&quot;red&quot;, &amp;Color{red:255})
    cp.addColor(&quot;yellow&quot;, &amp;Color{yellow:255})
    cp.addColor(&quot;blue&quot;, &amp;Color{blue:255})
}

type Brush struct {
    color *Color
    palette *ColorPalette
}

func (b *Brush)Color(name string) {
    b.color = palette.findByName(name)
}

func (b* Brush) Draw(x, y, x1, y1 int){
    // TODO
}

func NewBrush(cp *ColorPalette) *Brush{
    return &amp;Brush{
        palette: cp,
    }
}

// callers
palette := NewColorPalette()
brush := NewBrush(palette)
palette.Palatte(&quot;green&quot;).Set(0, 255, 255)
brush.Color(&quot;green&quot;)
brush.Draw(100,100,200,300)
```
上面以调色板为例，展示了享元模式的基本用法，享元模式的一个难点在于：共享单元的生命周期的管理。当调用方决定去释放该单元时，一定要保证该单元没有被其他地方占用。通常采用计数器的方式，被引用时计数加一，结束引用时计数减一，当计数为负时释放该单元。

计数器有两种方式，一种方式是与共享单元绑定，另一种方式是由管理共享单元的对象统计。
``` go
type Unit struct{
    factor int
}

func (*Unit)Caculate(x int)int{
    return x * factor
}

func (u *Unit)SetFactor(f int){
    u.factor = f
}

type counter struct {
    count int
}

func (c *counter)decr() int {
    c.count--
    return c.count
}

func (c *counter)incr() int {
    c.count++
    return c.count
}

type UnitEx struct {
    Unit
    counter
    name string
}

type UnitFactory struct{
    units map[string]*UnitEx
}

func (uf *UnitFactory)Release(u *UnitEx) {
    if u.decr() &lt; 0 {
        // TODO: release unit
    }
}

func (uf *UnitFactory)Delete(name string){
    u, ok := uf.units[u.name]
    if ok {
        delete(uf.units, u.name)
        uf.Release(u)
    }
}

func (uf *UnitFactory)ADD(name string, factor int){
    u := &amp;UnitEx{name:name}
    u.SetFactor(factor)
    uf.units[name] = u
}

func (uf *UnitFactory)Get(name string) *UnitEx {
    u, ok := uf.units[name]
    if ok {
        u.incr()
    }

    return u
}

// callers
// init unit
unitFactory.ADD(&quot;first&quot;, 10)

// use unit
u := unitFactory.Get(&quot;first&quot;) // count = 1
if u != nil{
    u.Caculate(10) // return 100
    unitFactory.Release(u) // count = 0
}

// delete unit
unitFactory.Delete(&quot;first&quot;) // count = -1, delete unit from sets.
```

上面的示例就是第一种方案，把计数器与共享单元绑定。任何调用方用`Get()`获取到共享单元后，都不会因为`Delete()`导致资源不可用，只要保证对应的`Release()`会被调用，也不会出现资源泄漏的问题。

实际上面的示例并不需要计数器，因为`golang`中有`gc`机制，当`UnitEx`对象没有被引用的时候会自动被回收掉。主要是预防在`Relese()`阶段有主动释资源放动作（如：关闭socket、关闭 channel等）的情况，由于`gc`并非实时具有一定延迟，可能会因为资源短时间大量泄漏（如：积累特别多client socket）导致不可用，所以这种资源保护还是有一定必要的。

### Front controller

### Marker

### Module

### Proxy
代理模式：为另一个对象提供代理或占位符，以控制对其的访问。
**使用场景**：对某个对象的访问需要是可控制的，在访问时要执行一些附加的动作。
```mermaid!
classDiagram

class Subject{
    &lt;&lt;interface&gt;&gt;
    +operation()
}

class Proxy{
    +operation()
}

class RealSubject{
    +operation()
}

Proxy ..|&gt; Subject
RealSubject ..|&gt; Subject
Proxy --&gt; RealSubject
```

golang示例代码如下：

``` go
type ICar interface {
    DriveCar()
}

type Car struct {
}

func ( *Car)DriveCar(){
    fmt.Println(&quot;running ....!!!&quot;)
}

type ProxyCar struct {
    driverAge int
    realCar Car
}

func (p *ProxyCar)DriveCar(){
    if p.driverAge &lt; 16 {
        fmt.Println(&quot;Sorry, the driver is too young to drive.&quot;)
    }else{
        p.realCar.DriveCar()
    }
}
```
以上示例与`Decorator`模式相似，不过两者的侧重点不同，`Proxy`模式侧重于访问控制，而`Decorator`测重于功能的拓展。

### Twin

## Behavioural patterns

### Blackboard

### Chain-of-responsibility
链式应答模式：通过给一个以上的对象一个处理请求的机会，避免将请求的发送者耦合到其接收者。 链接接收对象，并沿着链传递请求，直到对象处理该请求为止。

**使用场景**：一个request需要对应多个应答方，且各个应答方的判断条件不一致。

```mermaid!
classDiagram

class Handler{
    &lt;&lt;interface&gt;&gt;
    +handleRequest()
}

class Reciver1{
    +handlerRequest()
}

class Reciver2{
    +handlerRequest()
}

class Reciver3{
    +handlerRequest()
}

Reciver1 ..|&gt; Handler
Reciver2 ..|&gt; Handler
Reciver3 ..|&gt; Handler
Handler --&gt; Handler : successor
```

golang 示例代码如下：
``` go

type DebugLevel int
const (
    panic DebugLevel = 1 &lt;&lt; itoa
    error
    warning
    info 
    debug 
    function_error
    function_msg
    all = 1023
)

type Logger interface {
    Write(DebugLevel, string)
}

type ConsoleLogger struct{
    next Logger
    mask DebugLevel
}

func (cl *ConsoleLogger)Write(level DebugLevel, msg string){
    if cl.mask &amp; level {
        fmt.Println(&quot;Console: &quot;, msg)
    }

    if cl.next != nil {
        cl.next.Write(level, msg)
    }
}

func NewConsoleLogger(next Logger) Logger{
    return &amp;ConsoleLogger{
        next: next,
        mask: all,
    }
}

type FileLogger struct{
    next Logger
    mask DebugLevel
}

func (fl *FileLogger)Write(level DebugLevel, msg string){
    if fl.mask &amp; level {
        fmt.Println(&quot;File: &quot;, msg)
    }

    if fl.next != nil {
        fl.next.Write(level, msg)
    }
}

func NewFileLogger(next Logger) Logger{
    return &amp;FileLogger{
        next: next,
        mask: all^debug,
    }
}

type EmailLogger struct {
    next Logger
    mask DebugLevel
}

func (el *EmailLogger)Write(level DebugLevel, msg string){
    if el.mask &amp; level {
        fmt.Println(&quot;Email: &quot;, msg)
    }

    if el.next != nil {
        el.next.Write(level, msg)
    }
}

func NewEmailLogger(next Logger) Logger{
    return &amp;EmailLogger{
        next: next,
        mask:function_error| function_msg,
    }
}

// callers
logger := NewEmailLogger(nil)
logger = NewFileLogger(logger)
logger = NewConsoleLogger(logger)

logger.Write(debug, &quot;debug message&quot;)
logger.Write(info, &quot;info message&quot;)
logger.Write(function_msg, &quot;function message&quot;)

// output:
// Console: debug message
// Console: info message
// File: info message
// Console: function message
// File: function message
// Email: function message
```
以上的示例中把多个输出`logger`组成一个`loggers chain`，可以由调用方自由组合，又避免了发送方和接收方耦合。以上的写法和`Composite`模式有些类似，但并非层级关系，而且每个节点都会对`request`进行判断和处理，并非单纯的转发。

### Command
命令模式：将请求封装为对象，从而可以对具有不同请求的客户端进行参数化，以及对请求进行排队或记录。 它还允许支持不可撤消的操作。

**使用场景**：

```mermaid!
classDiagram
class Invoker{
    +invoke()
}

class Command{
    &lt;&lt;interafce&gt;&gt;
    +execute()
}

class Command1{
    +execute()
}

class Reciver{
    +action()
}

Invoker --&gt; Command: command
Command1 ..|&gt; Command
Command1 --&gt; Reciver
```

golang 示例代码如下：
``` go
type ForceUnit interface {
    Attack()
    Defense()
}

type solider struct {
}

func (*solider)Attack(){
}

func (*solider)Defense(){
}

func NewSolider() *solider {
    return &amp;solider{}
}

type ICommand interface{
    Execute()
}

type AttackCommand struct {
    f ForceUnit
}

func (ac *AttackCommand)Execute(){
    ac.f.Attack()
}

func CreateAttackCommand(f ForceUnit) *AttackCommand{
    return &amp;AttackCommand{f:f}
}

type DefenseCommand struct{
    f ForceUnit
}

func (dc *DefenseCommand)Execute(){
    dc.f.Defense()
}

func CreateDefenceCommand(f ForceUnit) *DefenseCommand{
    return &amp;AttackCommand{f:f}
}

type Invoker struct{
    startCmd ICommand
    stopCmd ICommand
}

func (i *Invoker)Start(){
    i.startCmd.Execute()
}

func (i *Invoker)Stop(){
    i.stopCmd.Execute()
}

func NewInvoker(startCmd, stopCmd ICommand) *Invoker{
    return &amp;Invoker{startCmd:startCmd, stopCmd:stopCmd}
}

// callers
s := NewSolider()
invoker := NewInvoker(CreateAttackCommand(s),CreateDefenceCommand(s))
invoker.Start() // solider start attacking.
invoker.Stop() // solider start defensing.
```

上面的示例是通过一个`invoker`去控制一个`solider`去`attack`或`defense`，多层的嵌套显得有些冗余。关键在于`invoker`这一层可以适应任何`start`和 `stop`二元模式的`command group`。开始的不仅仅可以是`attack command`也可以是`building command`，这样可以制定一系列的组合在一个“游戏回合”内执行。

### Interpreter
解释器模式：给定一种语言并定义其语法的表示形式，使用该表示形式来解释该语言中的句子。

```mermaid!
classDiagram

class Context{
    data
}

class Expression{
    &lt;&lt;interface&gt;&gt; 
    + Interpret(Context)
}

class TerminalExpression {
    + interpret(Context)
}

class Expression1 {
    + interpret(Context)
}

Expression ..&gt; Context
Expression1 ..|&gt; Expression
TerminalExpression ..|&gt; Expression
Expression &quot;1..*&quot; --* Expression1
```

golang 的示例如下：

``` go

```

### Iterator
遍历器模式：在不了解一个聚合对象的底层实现情况下，顺序遍历其中所有元素。
**使用场景**：遍历数组、Map、链表等。

```mermaid!
classDiagram

class Iterator{
    &lt;&lt;interface&gt;&gt;
    +next()
    +hasNext()
}

class Aggregate {
    &lt;&lt;interface&gt;&gt;
    +createIterator() Iterator
}

class ConcreteAggregate{
    +createIterator() Iterator
}

class ConcreteIterator{
    +next()
    +hasNext()
}

ConcreteAggregate ..|&gt; Aggregate
ConcreteIterator ..|&gt; Iterator
ConcreteAggregate ..&gt; ConcreteIterator: create
ConcreteIterator *-- ConcreteAggregate
```

golang的示例代码如下：
``` go
type Iterator interface {
    Next() interface{}
    HasNext() bool
}

type Repository interface {
    Iterator() Iterator
}

type ConceteRepository struct {
    elements []interface{}
}

func (cr * ConceteRepository)Iterator()Iterator{
    iter := &amp;ConceteIterator{}
    // copy elements
    iter.cr.elements = append(iter.cr.elements, cr.elements...)
} 

type ConceteIterator struct {
    index int
    end int
    cr ConceteRepository
}

func (ci *ConceteIterator)HasNext() bool{
    return index &lt; end
}

func (ci *ConceteIterator)Next()interface{} {
    if ci.HasNext() {
        defer ci.index++
        return cr.elements[ci.index]
    }
    
    return nil
}

// callers
for iter := repository.Iterator() ; iter.HasNext(); {
    fmt.Printf(&quot;%#v&quot;, iter.Next())
}
```

以上的示例代码是展示了一个简单的遍历对象内数组元素的过程，在实现过程中有一点需要注意：拷贝 `elements` 数组而不是直接的引用，对当时的状态做一个快照。

遍历模式在`C++`的 `std`库中是十分关键的模式，在`golang`中的一些`database driver`包中也十分常见，主要用途是遍历查询的结果。

### Mediator 
中介者模式：定义一个对象，该对象封装了一组对象之间的交互方式。中介类通过阻止对象之间显式地相互引用来促进松散耦合，并且它允许它们的交互独立地变化。

**使用场景**：两个具体类之间可能发生互相引用的情况，例如：用户要通过邮箱发邮件，邮箱要把邮件投递给每个用户。

```mermaid!
classDiagram
class Mediator{
    &lt;&lt;interface&gt;&gt;
    +mediate()
}

class Colleague{
    &lt;&lt;interface&gt;&gt;
    +getState()
}

class ConceteMediator {
    +mediate()
}

class ConceteColleague1 {
    +getState()
    +action1()
}

class ConceteColleague2 {
    +getState()
    +action2()
}

Mediator &lt;-- Colleague
ConceteMediator --&gt; ConceteColleague1
ConceteColleague1 ..|&gt; Colleague
ConceteMediator --&gt; ConceteColleague2
ConceteColleague2 ..|&gt; Colleague
ConceteMediator ..|&gt; Mediator
```

golang 示例代码如下：
``` go

type Email struct {
    From string
    To string
    Message string
}

type Mediator interface{
    SendMessage(from, to, message string)
    ReceiveMessage(from, to, message string)
}

type User struct {
    md Mediator
    name string
}

func NewUser(name string, md *ConceteMediator) *User{
    user := &amp;{name:name, md:md}
    md.addUser(user)
    return user
}

func (u *User) Name() string {
    return u.name
}

func (u *user)Receive(sender, message string){
    if sender != u.name {
        fmt.Printf(&quot;from %s to %s: %s&quot;, sender, name, message)
    } 
}

func (u *user)Send(rc, message string){
    u.md.SendMessage(u.name, rc, message)
}

type EMailBox struct {
    md Mediator
}

func NewEmailBox(md *ConceteMediator)*EMailBox{
    eb := &amp;EMailBox{
        md : md
    }

    md.setEmailBox(eb)
    return md
}

func (eb *EMailBox)push(email Email){
    // TODO
}

func (eb *EMailBox)pop()Email{
    // TODO
}

func (eb *EMailBox)number() int {
    // TODO
}

func (eb *EMailBox)HasFree() bool {
    // TODO
}

func (eb *EMailBox)Delivery(){
    for eb.number() &gt; 0 {
        email := eb.pop() 
        md.ReceiveMessage(email.from, email.to, email.message)
    }
}

type ConceteMediator struct {
    emailBox *EMailBox
    receivers map[string]*User
}

func (m *ConceteMediator)setEmailBox(eb *EMailBox){
    m.emailBox = eb
    eb.md = m
}

func (m *ConceteMediator)addUser(user *User){
    m.receivers[user.Name()] = user
    user.md = m
}


func (m *ConceteMediator)SendMessage(from, to, message string){
   sender := user.Name()
   if m.emialBox.HasFree() {
       m.emialBox.push(Email{From: from, To: to, Message: message})
   }
}

func (m *ConceteMediator)ReceiveMessage(from, to, message string){
        rc, ok := m.receivers[to]
        if ok {
            rc.Receive(from, message)
        }
    }
}

// callers
emailBox := NewEmailBox(mediator)
alice := NewUser(&quot;alice&quot;, mediator)
bob := NewUser(&quot;bob&quot;, mediator)

alice.Send(&quot;bob&quot;, &quot;nice to meet you.&quot;)
emailBox.Delivery()

// output:
// from alice to bob: nice to meet you.
```
上面的示例展示了邮箱发送邮件的过程，用户把邮件通过`Mediator`放到邮箱中，邮箱在通过`Mediator` 投递到每个用户。如果不添加`Mediator`类，则会出现`User` 和`EmailBox`互相调用的情况，导致强耦合。

### Memento
备忘录模式：在不打破对象封装的情况下，备份对象的状态，帮助对象回滚到前一状态。
**使用模式**：对于需要版本记录的对象，适用于备忘录模式。例如：执行了一个错误的提交，需要回滚到前一个版本。

```mermaid!
classDiagram
class Originator{
    - state
    + createMemento() 
    + restoreMemento(memento)
}

class Memento {
    - state
    + getState()
    + setState()
}

Originator ..&gt; Memento: &lt;&lt;create&gt;&gt;
Originator --&gt; Memento
```

golang的示例代码如下：
``` go
type Memento struct{
    state int
}

func (m *Memento)setState(state int){
    m.state = state
}

func (m *Memento)getState() int {
    return m.state
}

type EWallet struct{
    balance int
}

func NewEWallet(balance int)*EWallet{
    return &amp;EWallet{balance: balance}
}

func (ew *EWallet)CreateMemento() *Memento{
    m := new(Memento)
    m.setState(ew.balance)
    return m
}

func (ew *EWallet)RestoreMemento(m *Memento){
    ew.balance = m.getState()
}

func (ew *EWallet)Pay(pay int){
    ew.balance -= pay
}

// callers
ewallet := NewEWallet(100)
mem := ewallet.CreateMemento()

ewallet.Pay(10) // 90
ewallet.RestoreMemento(mem) // back to 100
```
对于开发人员来讲，每一个函数的调用，都是一次状态的流转。对状态进行备份，当触发回滚时再恢复状态，此时就需要用到`Memento`模式。

### observer
观察者模式：定义对象之间的一对多依赖关系，其中一个对象的状态变化导致其所有依赖关系都被通知并自动更新。

```mermaid!
classDiagram
class observer {
    &lt;&lt;interface&gt;&gt;
    +update()
}

class subject {
    +attech(observer)
    +dettech(observer)
    +notify()
}

class conteteObserver1{
    +update()
}

class conteteObserver2{
    +update()
}

subject --&gt; observer : obeservers
conteteObserver1 ..|&gt; observer
conteteObserver2 ..|&gt; observer

```

golang 示例代码如下：
``` go
type Observer interface{
    Update()
}

type conteteObserver struct{
    state int
}

func (co *conteteObserver)Update(){
    fmt.Println(&quot;Check if receive pay.&quot;)
}

func NewObserver() Observer{
    return &amp;conteteObserver{}
}

type subject struct {
   obs map[observer]struct{}
}

func (s *subject)Attech(o observer){
    s.obs[o] = struct{}
}

func (s *subject)Dettech(o observer){
    delete(s.obs, o)
}

func (s *subject)Notify(){
    for o := range s.obs {
        o.Update()
    }
}

type Wallet struct {
    sub subject
    balance int
}

func (w *Wallet)Pay(pay int){
    if w.balance &gt;= pay {
        w.balance -= pay
        w.sub.Notify()
    }
}

func (w *Wallet)PaySubject() *subject{
    return &amp;w.sub
}

// callers
wallet.PaySubject().Attech(NewObserver())
wallet.Pay(1)
// output:
// Check if receive pay.
```

上面给出了一个检测转账事件的示例，`Wallet`类直接关联到`Subject`是有些强耦合性的，可以利用`Mediator`模式、`Flywegiht`模式或`Singelton`模式进行事件转发。
这里需要注意的是，观察者模式是GoF提到的23种模式的一种，只是一个基本概念。并没有解决如下问题：1.消除对观察到变化主题的兴趣；2.在通知观察者之前或之后，对被观察到的主题进行特殊的逻辑处理。

该模式不记录通知发送，也不保证观察者已收到更改。这些问题通常在消息队列系统中处理，观察者模式是消息队列系统中的一小部分。`publish-subscribe`是一种消息模式，而不是一种设计模式，两者不要混淆。

### state
状态模式：当对象的内部状态更改时，允许其更改其行为。该对象似乎将更改其类。

**使用场景**：对象内部包含一个有限状态机时，不同的状态对应不同处理类。

```mermaid!
classDiagram
class Context{
    -state State
    +Request() 
}

class State{
    &lt;&lt;interface&gt;&gt;
    +Handle()
}

class ConceteStateA{
    +Handle()
}

class ConceteStateB{
    +Handle()
}

State --* Context : state.Handle()
State &lt;|.. ConceteStateA
State &lt;|.. ConceteStateB

``` 

golang的示例代码如下：
``` go
type State interface{
    Check(num int) bool
}

type normalState struct{
}

func (normalState)Check(num int) bool {
    // TODO
}

type wariningState struct {
}

func (wariningState)Check(num int) bool {
    // TODO
}

type panicState struct{
}

func (panicState)Check(num int) bool {
    // TODO
}

type recoverState struct{
}

func (recoverState)Check(num int) bool {
    // TODO
}

type Context struct {
    state State
}

func (c *Context)Request(num int) {
    if !c.state.Check(num) {
        return
    }

    // TODO
}

func (c *Context)SwitchState(state State){
    c.state = state
}

// callers
// normal
context.Request(10086)

// warning
context.SwitchState(wariningState)
context.Request(10086)

// recover
context.SwitchState(recoverState)
context.Request(10086)
```

状态模式可以将状态切换和处理逻辑分割开，在包含多个状态的流程中可以降低代码的复杂度，增加可读性。需要注意的是，状态切换动作尽量不要放在逻辑处理单元中来执行，这样会增加耦合性，也不利于代码维护。

### Strategy

策略模式：定义一系列算法，封装每个算法，并使它们可互换。 策略使算法可以独立于使用该算法的客户端而变化。

```mermaid!
classDiagram
class Context{
    -strategy Strategy
    +Request() 
}

class Strategy{
    &lt;&lt;interface&gt;&gt;
    +Excute()
}

class ConceteStrategyA{
    +Excute()
}

class ConceteStrategyB{
    +Excute()
}

Strategy --* Context : state.Handle()
Strategy &lt;|.. ConceteStrategyA
Strategy &lt;|.. ConceteStrategyB
```

golang的示例代码如下：
``` go
struct Price interface {
    GetActPrice(float, int) float
}

struct discountedPrice struct {
}

func (discountedPrice) GetActPrice(rawPrice float, num int) float{
    sum := rawPrice * float(num)
    switch(num){
        case 1:
            sum *= 0.95
        case 2:
            sum *= 0.85
        default:
            sum *= 0.75
    }

    return sum
}

struct normalPrice struct {
}

func (normalPrice) GetActPrice(rawPrice float, num int) float{
    return rawPrice*float(num)
}

type CustomerBill struct {
    price Price
    bills []float
}

func (c *CustomerBill)Add(rawPrice float, num int){
    c.bills = append(c.bills, price.GetActPrice(rawPrice, num))
}

func (c *CustomerBill)Set(price Price){
    c.price = price
}

// callers

// start discounting
customerBill.Set(discountedPrice)
customerBill.Add(10, 4) // 40 * 0.75
customerBill.Add(300, 1) // 300 * 0.95
customerBill.Add(50, 2) // 50 *2 * 0.85 

// stop discounting
customerBill.Set(normalPrice)
customerBill.Add(50, 2) // 50 *2 
```

上面的类图和示例都与`State`模式十分相似，但是两者有一个最显著的不同：触发切换时机，`State`模式的切换是由处理逻辑触发的（根据其返回结果，或者直接在处理逻辑单元中执行状态切换），而`Strategy`的切换与逻辑单元无关。

### Template 
模板模式：在操作中定义算法的框架，将某些步骤推迟到子类。 模板方法允许子类重新定义算法的某些步骤，而无需更改算法的结构。

**使用场景**：在一些上层的逻辑流程框架相同，但底层处理函数有细微差别的情况下。

```mermaid!
classDiagram

class Algorithm{
    + primitive1()
    + primitive2()
    + primitive3()
    + run()
}

class ContectAlgorithm1{
    + primitive1()
    + primitive2()
    + primitive3()
    + run()
}

ContectAlgorithm1 --|&gt; Algorithm
Algorithm --&gt; Algorithm: run(){ this-&gt;primitive1() ... }
```

``` golang
type Primitive interface {
    primitive1()
    primitive2()
}

type Template interface {
    Primitive
    Run()
}

type templateBase struct{
    p Primitive
}

func (t *templateBase)Run(){
    t.p.primitive1()
    t.p.primitive2()
}

type template1 struct {
    templateBase
}

func (*template1)primitive1(){
    fmt.Println(&quot;primitive1&quot;)
}

func (*template1)primitive2(){
    fmt.Println(&quot;primitive2&quot;)
}

func NewTemplate()Template{
    t1 := &amp;template1{}
    t1.p = t1
    return t1
}

// callers
t := NewTemplate()
t.Run()

// output:
// primitive1
// primitive2
```
上面的示例中模仿了继承基类的过程，由于`golang`没有虚函数特性，只能把底层函数抽象为接口`Primitive`，在*基础模板*中调用。在具体实现类中包含*基础模板*，同时实现`Primitive`接口，再将具体实现类的对象赋值给*基础模板*引用。

### vistor
访客模式：表示要在对象结构的元素上执行的操作。访客可以定义新操作，而无需更改其所操作元素的类。

**使用模式**：这种模式在需要访问多个类型状态，又不想打破被访问对象结构的情况下使用很是方便。

```mermaid!
classDiagram
class Element{
    &lt;&lt;interface&gt;&gt;
    +accept(vistor)
}

class Element1 {
    +accept(vistor)
}

class Element2{
    +accept(vistor)
}

class vistor{
    &lt;&lt;interface&gt;&gt;
    +vistElement1(Element1)
    +vistElement2(Element2)
}

class conteteVistor{
    +vistElement1(Element1)
    +vistElement2(Element2)
}

conteteVistor ..|&gt; vistor
vistor ..&gt;  Element2
vistor ..&gt;  Element1
Element1 ..|&gt; Element
Element2 ..|&gt; Element
Element ..&gt; vistor
```
golang 示例代码如下：
``` go
type vistor interface{
    VistCar(*Car)
    VistDriver(*Driver)
}

type element interface{
    Accept(vistor)
}

type Car struct{
}

func (c *Car)Accept(vistor){
    vistor.VistCar(c)
}

type Driver struct{
}

func (d *Driver)Accept(vistor){
    vistor.VistDriver(d)
}
```
上面的示例代码展示了`vistor`模式的简单应用。

## Concurrency Pattern

### Active Object
主动对象模式：使方法执行与驻留在其自己的控制线程中的方法调用脱钩。 目标是通过使用异步方法调用和用于处理请求的调度程序来引入并发。

主动对象模式的实现多种多样，常见的实现方式如下：

``` go
package main

import (
	&quot;fmt&quot;
	&quot;time&quot;
	&quot;sync&quot;
	&quot;runtime&quot;
	&quot;context&quot;
)

type Runnable func()

type ActiveObject struct {
	list   chan Runnable
	ctx    context.Context
	cancel context.CancelFunc
}

func (a *ActiveObject) Run(r Runnable) {
	a.list &lt;- r
}

func (a *ActiveObject) worker() {
	for running := true; running; {
		select {
		case &lt;-a.ctx.Done():
			running = false
		case r, ok := &lt;-a.list:
			if ok {
				r()
			} else {
				running = false
			}
		}
	}

	return
}

func (a *ActiveObject) Stop() {
	a.cancel()
	close(a.list)
}

func CreateActiveObject(ctx context.Context) *ActiveObject {
	a := ActiveObject{}
	a.ctx, a.cancel = context.WithCancel(ctx)
	a.list = make(chan Runnable, 1024)
	for i := 0; i &lt; runtime.NumCPU(); i++ {
		go a.worker()
	}
	return &amp;a
}

// callers
var obj = CreateActiveObject(context.TODO())

func useActive(r Runnable, n int) {
	for i := 0; i &lt; n; i++ {
		obj.Run(r)
	}
}

func unUseActive(r Runnable, n int) {
	for i := 0; i &lt; n; i++ {
		go r()
	}
}

func main() {
	n := 1024*1024*10
	wg := sync.WaitGroup{}
	wg.Add(n)
	
	start := time.Now().UnixNano()
	// useActive(func() {
	unUseActive(func() {
		j := 0
		for i := 0; i &lt; 1024; i++ {
			j = j*i
		} 
		wg.Done()
	}, n)

	wg.Wait()
	start = time.Now().UnixNano() - start
	fmt.Println(start)
}

```
如上面的示例所示，启动了多个worker routines 监听 `list channel` 等待 `Runnable` 下发。goroutines 的 `GMP` 调度器类似上述的active模式，`runnable` 和 `worker` 对应 `goroutine` 和 `thread`，只是缺少了`processors`这一层，也是golang调度器模型中最复杂的一层。那在 golang中基于`goroutine`创建并发调度模型是否有意义？

下面对比下使用主动对象模式和不使用对象模式的效率：

|NUM|使用active pattern(ns)| 不使用active pattern(ns)|
|--|--|--|
|1|4873583000|3095207000|
|2|4909422000|3160784000|
|3|5156796000|3034738000|
|4|5206848000|3373471000|
|5|4918898000|3279373000|

上面的结果显而易见，使用`goroutines pool`的并发模型比使用原生的`GMP`模型总耗时要多40%，在golang中使用并发模型似乎并不能提高效率？

上面的测试代码中使用了“乘法+加法”的纯cpu运算，以此为基准调整一下单个`Runnable`的计算量：
``` go
func main() {
	n := 1024*1024*10
	wg := sync.WaitGroup{}
	wg.Add(n)
	
	start := time.Now().UnixNano()
	useActive(func() {
	// unUseActive(func() {
		j := 0
		for i := 0; i &lt; 1024*100; i++ {
			j = j*i
		} 
		wg.Done()
	}, n)

	wg.Wait()
	start = time.Now().UnixNano() - start
	fmt.Println(start)
}
```
再次执行测试对比：

|NUM|使用active pattern(ns)| 不使用active pattern(ns)|
|--|--|--|
|1|36332021000|40188496000|
|2|37933490000|43609256000|
|3|34902684000|44315337000|
|4|35616489000|37804656000|
|5|35273675000|42551568000|

在计算量提高了`10`倍之后，运行测试的主机cpu跑満后，使用`goroutines pool`要优于原生的`GMP`。

具体的内部耗时可以用golang原生的profile去分析一下，首先我们在代码里加入可以生成`cpu profile` 的代码：
``` go
func main() {
    n := 1024 * 1024 * 10
    wg := sync.WaitGroup{}
    wg.Add(n)

    start := time.Now().UnixNano()

    if enable := true; enable {
        w, err := os.OpenFile(&quot;parrel.pprof&quot;, os.O_CREATE|os.O_RDWR, 0600)
        if err != nil {
            panic(err)
        }
        defer w.Close()

        pprof.StartCPUProfile(w)
        defer pprof.StopCPUProfile()
    }

    useActive(func() {
        // unUseActive(func() {
        j := 0
        for i := 0; i &lt; 1024*100; i++ {
            j = j * i
        }
        wg.Done()
    }, n)

    wg.Wait()
    start = time.Now().UnixNano() - start
    fmt.Println(start)
}
```
针对两种情况进行`cpu profile`分析，首先来看使用了`goroutines pool`的：

```
ype: cpu
Time: Jun 15, 2021 at 8:07pm (CST)
Duration: 38.02s, Total samples = 7.19mins (1134.18%)
Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)
(pprof) top 10
Showing nodes accounting for 426.48s, 98.91% of 431.18s total
Dropped 88 nodes (cum &lt;= 2.16s)
Showing top 10 nodes out of 21
      flat  flat%   sum%        cum   cum%
   329.75s 76.48% 76.48%    352.28s 81.70%  main.main.func1
    67.35s 15.62% 92.10%     67.35s 15.62%  runtime.usleep
    21.81s  5.06% 97.15%     21.81s  5.06%  runtime.asyncPreempt
     3.41s  0.79% 97.95%      3.41s  0.79%  runtime.pthread_cond_wait
     2.94s  0.68% 98.63%    372.65s 86.43%  main.(*ActiveObject).worker
     0.40s 0.093% 98.72%     15.27s  3.54%  runtime.lock2
     0.38s 0.088% 98.81%     17.27s  4.01%  runtime.selectgo
     0.26s  0.06% 98.87%     15.38s  3.57%  runtime.sellock
     0.10s 0.023% 98.89%     56.95s 13.21%  runtime.findrunnable
     0.08s 0.019% 98.91%     53.27s 12.35%  runtime.runqgrab
(pprof) tree
Showing nodes accounting for 426.53s, 98.92% of 431.18s total
Dropped 88 nodes (cum &lt;= 2.16s)
----------------------------------------------------------+-------------
      flat  flat%   sum%        cum   cum%   calls calls% + context              
----------------------------------------------------------+-------------
                                           352.28s   100% |   main.(*ActiveObject).worker
   329.75s 76.48% 76.48%    352.28s 81.70%                | main.main.func1
                                            21.81s  6.19% |   runtime.asyncPreempt
----------------------------------------------------------+-------------
                                            53.19s 78.98% |   runtime.runqgrab
                                            14.16s 21.02% |   runtime.osyield
    67.35s 15.62% 92.10%     67.35s 15.62%                | runtime.usleep
----------------------------------------------------------+-------------
                                            21.81s   100% |   main.main.func1
    21.81s  5.06% 97.15%     21.81s  5.06%                | runtime.asyncPreempt
----------------------------------------------------------+-------------
                                             3.41s   100% |   runtime.semasleep
     3.41s  0.79% 97.95%      3.41s  0.79%                | runtime.pthread_cond_wait
----------------------------------------------------------+-------------
     2.94s  0.68% 98.63%    372.65s 86.43%                | main.(*ActiveObject).worker
                                           352.28s 94.53% |   main.main.func1
                                            17.27s  4.63% |   runtime.selectgo
----------------------------------------------------------+-------------
                                            15.27s   100% |   runtime.lockWithRank
     0.40s 0.093% 98.72%     15.27s  3.54%                | runtime.lock2
                                            14.16s 92.73% |   runtime.osyield (inline)
----------------------------------------------------------+-------------
                                            17.27s   100% |   main.(*ActiveObject).worker
     0.38s 0.088% 98.81%     17.27s  4.01%                | runtime.selectgo
                                            15.38s 89.06% |   runtime.sellock
----------------------------------------------------------+-------------
                                            15.38s   100% |   runtime.selectgo
     0.26s  0.06% 98.87%     15.38s  3.57%                | runtime.sellock
                                            15.12s 98.31% |   runtime.lock (inline)
----------------------------------------------------------+-------------
                                            56.95s   100% |   runtime.schedule
     0.10s 0.023% 98.89%     56.95s 13.21%                | runtime.findrunnable
                                            53.28s 93.56% |   runtime.runqsteal
                                             3.43s  6.02% |   runtime.stopm
----------------------------------------------------------+-------------
                                            53.27s   100% |   runtime.runqsteal
     0.08s 0.019% 98.91%     53.27s 12.35%                | runtime.runqgrab
                                            53.19s 99.85% |   runtime.usleep
----------------------------------------------------------+-------------
                                            57.13s 99.81% |   runtime.park_m
     0.02s 0.0046% 98.91%     57.24s 13.28%                | runtime.schedule
                                            56.95s 99.49% |   runtime.findrunnable
----------------------------------------------------------+-------------
                                            57.27s   100% |   runtime.mcall
     0.01s 0.0023% 98.92%     57.27s 13.28%                | runtime.park_m
                                            57.13s 99.76% |   runtime.schedule
----------------------------------------------------------+-------------
                                            53.28s   100% |   runtime.findrunnable
     0.01s 0.0023% 98.92%     53.28s 12.36%                | runtime.runqsteal
                                            53.27s   100% |   runtime.runqgrab
----------------------------------------------------------+-------------
                                             3.43s   100% |   runtime.findrunnable
     0.01s 0.0023% 98.92%      3.43s   0.8%                | runtime.stopm
                                             3.41s 99.42% |   runtime.mPark
----------------------------------------------------------+-------------
                                            15.12s 99.08% |   runtime.sellock (inline)
         0     0% 98.92%     15.26s  3.54%                | runtime.lock
                                            15.26s   100% |   runtime.lockWithRank (inline)
----------------------------------------------------------+-------------
                                            15.26s 99.93% |   runtime.lock (inline)
         0     0% 98.92%     15.27s  3.54%                | runtime.lockWithRank
                                            15.27s   100% |   runtime.lock2
----------------------------------------------------------+-------------
                                             3.41s   100% |   runtime.stopm
         0     0% 98.92%      3.41s  0.79%                | runtime.mPark
                                             3.41s   100% |   runtime.notesleep
----------------------------------------------------------+-------------
         0     0% 98.92%     57.27s 13.28%                | runtime.mcall
                                            57.27s   100% |   runtime.park_m
----------------------------------------------------------+-------------
                                             3.41s   100% |   runtime.mPark
         0     0% 98.92%      3.41s  0.79%                | runtime.notesleep
                                             3.41s   100% |   runtime.semasleep
----------------------------------------------------------+-------------
                                            14.16s   100% |   runtime.lock2 (inline)
         0     0% 98.92%     14.16s  3.28%                | runtime.osyield
                                            14.16s   100% |   runtime.usleep
----------------------------------------------------------+-------------
                                             3.41s   100% |   runtime.notesleep
         0     0% 98.92%      3.41s  0.79%                | runtime.semasleep
                                             3.41s   100% |   runtime.pthread_cond_wait
----------------------------------------------------------+-------------
(pprof) list main.main.func1
Total: 7.19mins
ROUTINE ======================== main.main.func1 in /Users/wangzhipeng/workspace/newbee/golang/tmp/design_pattern/parral.go
  5.50mins   5.87mins (flat, cum) 81.70% of Total
         .          .     85:
         .          .     86:           pprof.StartCPUProfile(w)
         .          .     87:           defer pprof.StopCPUProfile()
         .          .     88:   }
         .          .     89:
         .       20ms     90:   // useActive(func() {
         .          .     91:   unUseActive(func() {
         .          .     92:           j := 0
  5.50mins   5.86mins     93:           for i := 0; i &lt; 1024*100; i++ {
         .          .     94:                   j = j * i
         .          .     95:           }
         .      710ms     96:           wg.Done()
         .          .     97:   }, n)
         .          .     98:
         .          .     99:   wg.Wait()
         .          .    100:   start = time.Now().UnixNano() - start
         .          .    101:   fmt.Println(start)
```

然后再来看一下不使用`goroutines pool`的：
```
Type: cpu
Time: Jun 15, 2021 at 8:13pm (CST)
Duration: 52.76s, Total samples = 10.53mins (1197.83%)
Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)
(pprof) top 10
Showing nodes accounting for 10.29mins, 97.64% of 10.53mins total
Dropped 172 nodes (cum &lt;= 0.05mins)
Showing top 10 nodes out of 39
      flat  flat%   sum%        cum   cum%
  7.75mins 73.54% 73.54%   7.76mins 73.66%  main.main.func1
  1.52mins 14.44% 87.98%   1.52mins 14.44%  runtime/pprof.lostProfileEvent
  0.27mins  2.52% 90.50%   0.32mins  3.00%  runtime.execute
  0.17mins  1.57% 92.07%   0.17mins  1.60%  runtime.gentraceback
  0.16mins  1.55% 93.62%   0.17mins  1.62%  runtime.findnull
  0.14mins  1.36% 94.99%   0.14mins  1.36%  runtime.(*gQueue).pop (inline)
  0.10mins  0.92% 95.91%   0.10mins  0.92%  runtime.usleep
  0.08mins  0.72% 96.63%   0.85mins  8.06%  runtime.goexit0
  0.06mins  0.53% 97.17%   0.07mins  0.62%  runtime.gcWriteBarrier
  0.05mins  0.48% 97.64%   0.07mins  0.66%  runtime.stackpoolalloc
(pprof) tree
Showing nodes accounting for 621.92s, 98.41% of 632s total
Dropped 172 nodes (cum &lt;= 3.16s)
----------------------------------------------------------+-------------
      flat  flat%   sum%        cum   cum%   calls calls% + context              
----------------------------------------------------------+-------------
   464.76s 73.54% 73.54%    465.52s 73.66%                | main.main.func1
----------------------------------------------------------+-------------
    91.28s 14.44% 87.98%     91.28s 14.44%                | runtime/pprof.lostProfileEvent
----------------------------------------------------------+-------------
                                            18.93s   100% |   runtime.schedule
    15.92s  2.52% 90.50%     18.93s  3.00%                | runtime.execute
                                             2.89s 15.27% |   runtime.gcWriteBarrier
----------------------------------------------------------+-------------
                                            10.14s   100% |   runtime.scanstack
     9.93s  1.57% 92.07%     10.14s  1.60%                | runtime.gentraceback
----------------------------------------------------------+-------------
                                            10.22s   100% |   runtime.gostringnocopy
     9.82s  1.55% 93.62%     10.22s  1.62%                | runtime.findnull
----------------------------------------------------------+-------------
                                             8.62s   100% |   runtime.globrunqget (inline)
     8.62s  1.36% 94.99%      8.62s  1.36%                | runtime.(*gQueue).pop
----------------------------------------------------------+-------------
                                             5.84s   100% |   runtime.osyield
     5.84s  0.92% 95.91%      5.84s  0.92%                | runtime.usleep
----------------------------------------------------------+-------------
                                            50.91s   100% |   runtime.mcall
     4.55s  0.72% 96.63%     50.91s  8.06%                | runtime.goexit0
                                            31.37s 61.62% |   runtime.schedule
                                            10.78s 21.17% |   runtime.isSystemGoroutine
                                             2.86s  5.62% |   runtime.lock (inline)
                                             1.01s  1.98% |   runtime.gcWriteBarrier
----------------------------------------------------------+-------------
                                             2.89s 74.10% |   runtime.execute
                                             1.01s 25.90% |   runtime.goexit0
     3.37s  0.53% 97.17%      3.90s  0.62%                | runtime.gcWriteBarrier
----------------------------------------------------------+-------------
                                             4.19s   100% |   runtime.stackcacherefill
     3.01s  0.48% 97.64%      4.19s  0.66%                | runtime.stackpoolalloc
----------------------------------------------------------+-------------
                                             3.83s   100% |   runtime.gcDrain
     2.16s  0.34% 97.98%      3.83s  0.61%                | runtime.scanobject
----------------------------------------------------------+-------------
                                             8.32s   100% |   runtime.newproc.func1
     0.75s  0.12% 98.10%      8.32s  1.32%                | runtime.newproc1
                                             5.60s 67.31% |   runtime.malg
----------------------------------------------------------+-------------
                                            10.78s 96.77% |   runtime.goexit0
     0.38s  0.06% 98.16%     11.14s  1.76%                | runtime.isSystemGoroutine
                                            10.40s 93.36% |   runtime.funcname
----------------------------------------------------------+-------------
                                             4.31s   100% |   runtime.malg
     0.35s 0.055% 98.22%      4.31s  0.68%                | runtime.malg.func1
                                             3.96s 91.88% |   runtime.stackalloc
----------------------------------------------------------+-------------
                                            11.03s   100% |   runtime.gcDrain
     0.33s 0.052% 98.27%     11.03s  1.75%                | runtime.markroot
                                            10.37s 94.02% |   runtime.markroot.func1
----------------------------------------------------------+-------------
                                            31.37s   100% |   runtime.goexit0
     0.23s 0.036% 98.31%     31.38s  4.97%                | runtime.schedule
                                            18.93s 60.33% |   runtime.execute
                                             9.49s 30.24% |   runtime.findrunnable
                                             1.97s  6.28% |   runtime.lock (inline)
----------------------------------------------------------+-------------
                                            15.03s   100% |   runtime.gcBgMarkWorker.func2
     0.16s 0.025% 98.33%     15.03s  2.38%                | runtime.gcDrain
                                            11.03s 73.39% |   runtime.markroot
                                             3.83s 25.48% |   runtime.scanobject
----------------------------------------------------------+-------------
     0.10s 0.016% 98.35%     51.02s  8.07%                | runtime.mcall
                                            50.91s 99.78% |   runtime.goexit0
----------------------------------------------------------+-------------
                                             6.71s   100% |   main.unUseActive
     0.06s 0.0095% 98.36%      6.71s  1.06%                | runtime.newproc
                                             6.65s 99.11% |   runtime.systemstack
----------------------------------------------------------+-------------
                                            10.27s   100% |   runtime.funcname (inline)
     0.05s 0.0079% 98.37%     10.27s  1.62%                | runtime.gostringnocopy
                                            10.22s 99.51% |   runtime.findnull
----------------------------------------------------------+-------------
                                             6.01s   100% |   runtime.lockWithRank
     0.05s 0.0079% 98.37%      6.01s  0.95%                | runtime.lock2
                                             5.84s 97.17% |   runtime.osyield (inline)
----------------------------------------------------------+-------------
                                            15.04s 62.88% |   runtime.gcBgMarkWorker
                                             6.65s 27.80% |   runtime.newproc
     0.04s 0.0063% 98.38%     23.92s  3.78%                | runtime.systemstack
                                            15.03s 62.83% |   runtime.gcBgMarkWorker.func2
                                             8.47s 35.41% |   runtime.newproc.func1
----------------------------------------------------------+-------------
                                            10.40s   100% |   runtime.isSystemGoroutine
     0.03s 0.0047% 98.38%     10.40s  1.65%                | runtime.funcname
                                            10.27s 98.75% |   runtime.gostringnocopy (inline)
----------------------------------------------------------+-------------
                                            10.37s   100% |   runtime.markroot
     0.02s 0.0032% 98.39%     10.37s  1.64%                | runtime.markroot.func1
                                            10.23s 98.65% |   runtime.scanstack
----------------------------------------------------------+-------------
                                            10.23s   100% |   runtime.markroot.func1
     0.02s 0.0032% 98.39%     10.23s  1.62%                | runtime.scanstack
                                            10.14s 99.12% |   runtime.gentraceback
----------------------------------------------------------+-------------
                                             3.96s 93.40% |   runtime.malg.func1
     0.02s 0.0032% 98.39%      4.24s  0.67%                | runtime.stackalloc
                                             4.22s 99.53% |   runtime.stackcacherefill
----------------------------------------------------------+-------------
                                             4.22s   100% |   runtime.stackalloc
     0.02s 0.0032% 98.40%      4.22s  0.67%                | runtime.stackcacherefill
                                             4.19s 99.29% |   runtime.stackpoolalloc
----------------------------------------------------------+-------------
                                             9.49s   100% |   runtime.schedule
     0.01s 0.0016% 98.40%      9.49s  1.50%                | runtime.findrunnable
                                             8.37s 88.20% |   runtime.globrunqget
                                             0.98s 10.33% |   runtime.lock (inline)
----------------------------------------------------------+-------------
                                             8.37s 95.99% |   runtime.findrunnable
     0.01s 0.0016% 98.40%      8.72s  1.38%                | runtime.globrunqget
                                             8.62s 98.85% |   runtime.(*gQueue).pop (inline)
----------------------------------------------------------+-------------
                                             6.02s   100% |   runtime.lock (inline)
     0.01s 0.0016% 98.40%      6.02s  0.95%                | runtime.lockWithRank
                                             6.01s 99.83% |   runtime.lock2
----------------------------------------------------------+-------------
                                             5.60s   100% |   runtime.newproc1
     0.01s 0.0016% 98.40%      5.60s  0.89%                | runtime.malg
                                             4.31s 76.96% |   runtime.malg.func1
----------------------------------------------------------+-------------
                                             8.47s   100% |   runtime.systemstack
     0.01s 0.0016% 98.41%      8.47s  1.34%                | runtime.newproc.func1
                                             8.32s 98.23% |   runtime.newproc1
----------------------------------------------------------+-------------
                                             6.71s   100% |   runtime.main
         0     0% 98.41%      6.71s  1.06%                | main.main
                                             6.71s   100% |   main.unUseActive
----------------------------------------------------------+-------------
                                             6.71s   100% |   main.main
         0     0% 98.41%      6.71s  1.06%                | main.unUseActive
                                             6.71s   100% |   runtime.newproc
----------------------------------------------------------+-------------
         0     0% 98.41%     15.05s  2.38%                | runtime.gcBgMarkWorker
                                            15.04s 99.93% |   runtime.systemstack
----------------------------------------------------------+-------------
                                            15.03s   100% |   runtime.systemstack
         0     0% 98.41%     15.03s  2.38%                | runtime.gcBgMarkWorker.func2
                                            15.03s   100% |   runtime.gcDrain
----------------------------------------------------------+-------------
                                             2.86s 47.51% |   runtime.goexit0 (inline)
                                             1.97s 32.72% |   runtime.schedule (inline)
                                             0.98s 16.28% |   runtime.findrunnable (inline)
         0     0% 98.41%      6.02s  0.95%                | runtime.lock
                                             6.02s   100% |   runtime.lockWithRank (inline)
----------------------------------------------------------+-------------
         0     0% 98.41%      6.71s  1.06%                | runtime.main
                                             6.71s   100% |   main.main
----------------------------------------------------------+-------------
                                             5.84s   100% |   runtime.lock2 (inline)
         0     0% 98.41%      5.84s  0.92%                | runtime.osyield
                                             5.84s   100% |   runtime.usleep
----------------------------------------------------------+-------------
(pprof) list main.main.func1
Total: 10.53mins
ROUTINE ======================== main.main.func1 in /Users/wangzhipeng/workspace/newbee/golang/tmp/design_pattern/parral.go
  7.75mins   7.76mins (flat, cum) 73.66% of Total
         .          .     86:           pprof.StartCPUProfile(w)
         .          .     87:           defer pprof.StopCPUProfile()
         .          .     88:   }
         .          .     89:
         .          .     90:   // useActive(func() {
  1.73mins   1.73mins     91:   unUseActive(func() {
         .          .     92:           j := 0
  6.02mins   6.02mins     93:           for i := 0; i &lt; 1024*100; i++ {
         .          .     94:                   j = j * i
         .          .     95:           }
         .      720ms     96:           wg.Done()
         .          .     97:   }, n)
         .          .     98:
         .          .     99:   wg.Wait()
         .          .    100:   start = time.Now().UnixNano() - start
         .          .    101:   fmt.Println(start)
```
由开启了`profile`会对协程调度的开销有所影响， 对于后者来讲带来的损耗会更大，因为后者要创建大量的协程。最消耗`cpu`的明显是`f`，其余部分的抛开`runtime/pprof.lostProfileEvent`，前者集中在因为`selectgo`(4.01%)和`runtime.park_m`(13.28%)引起的开销，后者主要集中在`go.exit()`(8.06%)和`runtime.newproc`引起的`runtime.systemstack`的调用（1.06%）。需要注意的是上面是没有扣除`profile`引入的额外损耗下的百分比，在实际情况下后者的耗时占比会上升。

golang中创建`goroutine`的开销要远小于创建`thread`的开销，但也是需要一定开销的：
1. go 命令创建 `goroutine` 可能会触发调度器进行调度。要选择一个 processor加入其队列中，如果所有processor的队列都已经充满，它会加入到全局队列中等待执行；
2. `goroutine`最起码要分配2k栈空间（golang 1.12 版本中，最大空间取决于操作系统位数，32位系统中最大 250M，64位操作系统中最大 1G），需要内存开销。虽然golang中采取了一些优化措施，在`goroutine end-of-life`时会将其加入一个`internel pool`中，`new goroutine`会从池中捞取进行重用，但那只针对栈空间没有被拓展过的`goroutine`(验证代码未起到对应效果)；
3. 采用一个类似`goroutines pool`的并发模型，可以任务抑制并发执行数量，因为同时只有`n`个`worker`在执行。对于高并发且会执行大量内存申请操作的场景下，采用`goroutines pool`的模型可以有效抑制内存和cpu。

Note: 需要注意的是，当申请一个过大的连续内存空间（例如 make([]int, 10240)）时，会被分配在堆上，及时它没有脱离申请函数的生命周期。

### Double-checked locking	

双重检查加锁优化：在加锁前进行检查判断是否需要加锁保护，避免不必要加锁带来的额外开销。
**使用场景**：它通常用于在多线程环境中实现“延迟初始化”时减少锁定开销，尤其是作为单例模式的一部分。延迟初始化避免在第一次访问值之前对其进行初始化。

``` golang
package main

import &quot;sync&quot;

var arrOnce sync.Once
var arr []int

// getArr retrieves arr, lazily initializing on first call. Double-checked
// locking is implemented with the sync.Once library function. The first
// goroutine to win the race to call Do() will initialize the array, while
// others will block until Do() has completed. After Do has run, only a
// single atomic comparison will be required to get the array.
func getArr() []int {
	arrOnce.Do(func() {
		arr = []int{0, 1, 2}
	})
	return arr
}

func main() {
	// thanks to double-checked locking, two goroutines attempting to getArr()
	// will not cause double-initialization
	go getArr()
	go getArr()
}
```
在上面`lazy initial`的模式时已经讲过这种使用方法了，这里使用`sync.Once`有两个好处：
1. 只会执行一次，如果执行过了，下次调用不会重复执行；
2. 并发调用的时候有锁保护，保证不会重复执行；
综上即只会在第一次调用的时候执行初始化，且只执行一次。

下面简单讲一下`sync.Once`是如何实现的该功能，后面会单独对其进行详细的解析。
``` golang
type Once struct {
	// done indicates whether the action has been performed.
	// It is first in the struct because it is used in the hot path.
	// The hot path is inlined at every call site.
	// Placing done first allows more compact instructions on some architectures (amd64/386),
	// and fewer instructions (to calculate offset) on other architectures.
	done uint32
	m    Mutex
}

func (o *Once) Do(f func()) {
	if atomic.LoadUint32(&amp;o.done) == 0 {
		o.doSlow(f)
	}
}

func (o *Once) doSlow(f func()) {
	o.m.Lock()
	defer o.m.Unlock()
	if o.done == 0 {
		defer atomic.StoreUint32(&amp;o.done, 1)
		f()
	}
}
```
结构体中成员变量：
+ `done`是用来标记是否已经执行过，它保证了上述第一个好处；
+ `m`是用来做并发保护的`sync.Mutext`，它保证了上述的第一个好处；

`once.Done(f func())`的流程如下：

```mermaid!
graph TD
    Start[start];
    IsFirst{load done in atomic, done ==0 ?};
    Lock[m.lock];
    Unlock[m.unlock];
    IsNotDone{ done==0? };
    Func[run function];
    Done[ set done = 1 in atomic];
    End[end];

    Start--&gt;IsFirst;
    IsFirst--&gt;|No|End;
    IsFirst--&gt;|Yes|Lock;
    Lock--&gt;IsNotDone;
    IsNotDone--&gt;|Yes|Func;Func--&gt;Done;Done--&gt;Unlock;
    IsNotDone--&gt;|Not|Unlock;
    Unlock--&gt;End;
```

这里面有几个关键的点需要注意一下：
1. 在`m`锁保护的情况下使用`atomic.StoreUint32(&amp;o.done, 1)`修改值，这是为了使`Done`状态立刻生效，虽然此时`m`还未被释放，但是`f`已经执行完毕了。
2. 拆分出`doSlow`函数是为了`Done`函数可以在编译时内联优化，这样执行起来速度更快的判断`Done`的状态。

### Monitor

监视器模式：监视器模式是并发编程中一种资源保护模式，它通常由`mutex`和`condition`共同组成，它允许调用者放弃对`mutex`的占用，等待满足特定条件下才会触发对资源的独占访问，同时也可以发出信号唤醒一些等待条件的调用方（thread、goroutine、process等等）。

在golang中实现起来有多种方案，可以直接使用`sync.Cond`和`sync.Mutex`来实现一个简单版本监视器模式：

``` golang
type Queue struct {
	buff  []int
	read  int
	write int

	m    sync.Mutex
	cond *sync.Cond
}

func (q *Queue) isFull() bool {
	return (q.write+1)%len(q.buff) == q.read
}

func (q *Queue) isEmpty() bool {
	return q.write == q.read
}

func (q *Queue) Push(e int) {
	q.m.Lock()
	for q.isFull() {
		q.cond.Wait()
	}

	q.buff[q.write] = e
	q.write = (q.write + 1) % len(q.buff)
	q.cond.Signal()

	q.m.Unlock()
}

func (q *Queue) Pop() (res int) {
	q.m.Lock()
	for q.isEmpty() {
		q.cond.Wait()
	}

	res = q.buff[q.read]
	q.read = (q.read + 1) % len(q.buff)
	q.cond.Signal()
	q.m.Unlock()
	return
}
```
上面是一个简单的`ring buffer`实现，主要是依赖于`sync.Cond`的强大功能，这里对它的实现做一个简单的解析：

``` golang
type Cond struct {
    noCopy noCopy

    // L is held while observing or changing the condition
    L Locker // 资源保护锁，可以为sync.Mutex或sync.RWMutex

    notify  notifyList // 通知列表
    checker copyChecker
}

// 等待信号
func (c *Cond) Wait() {
    c.checker.check()
    // 将当前的goroutine加入通知列表，返回一个ticket number；
    t := runtime_notifyListAdd(&amp;c.notify)
    // 释放独占锁；
    c.L.Unlock()
    // 根据ticket number 等待被通知，此处在接收信号前会blokced；
    runtime_notifyListWait(&amp;c.notify, t)
    // 获取独占锁；
    c.L.Lock()
}

// 发送信号并唤醒一个等待队列中的 g
func (c *Cond) Signal() {
    c.checker.check()
    runtime_notifyListNotifyOne(&amp;c.notify)
}

// 唤醒等待队列中所有的g
func (c *Cond) Broadcast() {
    c.checker.check()
    runtime_notifyListNotifyAll(&amp;c.notify)
}

// notifyList 基于ticket实现的一个通知链表，它被 sync.Cond 依赖.
//
// It must be kept in sync with the sync package.
type notifyList struct {
    // 下一waiter的tickt number，在lock保护外原子累加
    wait uint32

    // 下一个被通知的waiter的ticket number，
    // 它可以在lock保护外读取，但只能在lock保护内写入。
    // wait 和 notify都可能“越界”（即累加超过2^32-1），
    // 只要它们的真实“差距”（wait - notify）不超过（2^31），
    // 这在目前是不可能的，因为同时存在的g不可能超过 2^31。
    notify uint32

    // List of parked waiters，它是一个单向指针链表.
    lock mutex // 互斥保护
    head *sudog  // waiters list 头指针，sudo是用于在等待列表中表示一个 g (goroutine)，它也被channel实现依赖
    tail *sudog // waiters list 尾指针
}

// runtime_notifyListAdd 实现，在runtime/sema.go中，
// 通过linkname进行连接
//go:linkname notifyListAdd sync.runtime_notifyListAdd
func notifyListAdd(l *notifyList) uint32 {
    // 当sync.Cond.Wait 以 read 模式占用一个RWMutex时，
    // 此函数可能会被并发调用.
    // Note: 返回当前l.wait的值，并将l.wait+1
    return atomic.Xadd(&amp;l.wait, 1) - 1
}

// runtime_notifyListWait 实现，在runtime/sema.go中，
// 过linkname进行连接
// notifyListWait 会等待一个通知，如果一个通知在已经在notifyListAdd
// 前发送它会立即返回，否则它会block.
//go:linkname notifyListWait sync.runtime_notifyListWait
func notifyListWait(l *notifyList, t uint32) {
    lockWithRank(&amp;l.lock, lockRankNotifyList)

    // Return right away if this ticket has already been notified.
    // int32(t-l.notify) &lt; 0
    // 考虑到两者都可能越界，但是它们的差异不会超过`2^31`
    // 如果小于notify则立即返回
    if less(t, l.notify) {
        unlock(&amp;l.lock)
        return
    }

    // Enqueue itself.
    // sudog 绑定 ticket number 与 g，压入 waiter list.
    s := acquireSudog()
    s.g = getg()
    s.ticket = t
    s.releasetime = 0
    t0 := int64(0)
    if blockprofilerate &gt; 0 {
        t0 = cputicks()
        s.releasetime = -1
    }
    if l.tail == nil {
        l.head = s
    } else {
        l.tail.next = s
    }
    l.tail = s
    // 挂起 g并释放l.lock，可以通过 goready(g)唤醒
    // 此处goroutine会进入挂起状态
    goparkunlock(&amp;l.lock, waitReasonSyncCondWait, traceEvGoBlockCond, 3)
    if t0 != 0 {
        blockevent(s.releasetime-t0, 2)
    }
    // 释放对象 s
    releaseSudog(s)
}

// runtime_notifyListNotifyOne 实现，在runtime/sema.go中，
// 过linkname进行连接
// notifyListNotifyOne 唤醒一个在waiter list 的 g
//go:linkname notifyListNotifyOne sync.runtime_notifyListNotifyOne
func notifyListNotifyOne(l *notifyList) {
    // Fast-path: 自从上一次通知发送后，没有新的waiter进入队列；
    // 直接返回，不进入唤醒流程。
    if atomic.Load(&amp;l.wait) == atomic.Load(&amp;l.notify) {
        return
    }

    lockWithRank(&amp;l.lock, lockRankNotifyList)

    // 进行二次检查.
    t := l.notify
    // 此处使用atomic.Load 是因为l.wait在notifyListAdd
    // 中并未使用l.lock保护自增，而是通过原子操作。
    if t == atomic.Load(&amp;l.wait) {
        unlock(&amp;l.lock)
        return
    }

    // 更新通知编号.
    // 此处使用原子操作因为l.notify的读取有不在l.lock保护的情况
    // 如上面的FastPath 
    atomic.Store(&amp;l.notify, t+1)

    // 寻找需要唤醒的g，有可能它刚刚调用notifyListAdd() 还未来得及
    // 调用notifyListWait()将自己加入到 waiter list，但是当它
    // 的 ticket - l.notify &lt; 0 时，它会立即触发条件而无需 
    // 调用gopark 挂起自己.
    //
    // 在链表中找到与 t (此处一定要注意，是t而不是l.notfiy)
    // 相等的sudog唤醒，并将其移除链表
    // 需要说明的是，链表中的sudog顺序并不与ticket 顺序一致，
    // 因为获取 ticket number与加入队列是割裂的。但总能保证
    // 被唤醒的 g 是 ticket number == t 的，无论其此时是否
    // 已经加入队列。
    for p, s := (*sudog)(nil), l.head; s != nil; p, s = s, s.next {
        if s.ticket == t {
            n := s.next
            if p != nil {
                p.next = n
            } else {
                l.head = n
            }
            if n == nil {
                l.tail = p
            }
            unlock(&amp;l.lock)
            s.next = nil // 取消next引用
            readyWithTime(s, 4) // 唤醒对应的 g
            return
        }
    }
    unlock(&amp;l.lock)
}

// runtime_notifyListNotifyOne 实现，在runtime/sema.go中，
// 过linkname进行连接
// runtime_notifyListNotifyAll 唤醒链表中所有的g.
//go:linkname notifyListNotifyAll sync.runtime_notifyListNotifyAll
func notifyListNotifyAll(l *notifyList) {
    // Fast-path: 自从上一次通知发送后，没有新的waiter进入队列；
    // 直接返回，不进入唤醒流程。
    if atomic.Load(&amp;l.wait) == atomic.Load(&amp;l.notify) {
        return
    }

    // 复制当前链表，并清空队列
    lockWithRank(&amp;l.lock, lockRankNotifyList)
    s := l.head
    l.head = nil
    l.tail = nil

    // 直接将l.wait赋值给l.notify
    // 因为cond的机制总能保证 持有ticker number
    // 的g 接收到对应通知，无论其是否在队列中
    atomic.Store(&amp;l.notify, atomic.Load(&amp;l.wait))
    unlock(&amp;l.lock)

    // 遍历列表，唤醒所有g
    for s != nil {
        next := s.next
        s.next = nil
        readyWithTime(s, 4)
        s = next
    }
}
```

golang的`sync.Cond`与`sync.Once`一样，它们的源码都很简单易读，可以说是模范代码。通过各种 `FastPath` 进行提前检查（Double Check 模式），从而避免过多的锁竞争，减少cpu的开销。

同时也应该注意到：所有在锁保护内外都有读写操作的变量，如果想使写入立即更新（无论操作是否在锁保护范围内）和读取最新的值（在锁保护范围外），最好通过atomic方法执行。

### Reactor

反应堆模式：反应堆模式是一个事件驱动的并发模型，可以同时监听多个服务请求，通过`同步多路复用器（Synchronous Event Demultiplexer）`进行多路分解，将请求调度到对应的`Request Handler`。

**使用场景**：多用于服务器程序中，用来监听外部链接请求和已建立链接发送的请求。

它主要由：

+ Resource：任何可以提供输入和消费输出的资源；
+ Synchronous Event Demultiplexer：通过`event loop`的形式监听所有资源的`event`，当监听到资源的事件时候，它会将资源交给`Dispathcer`去调度执行对应的`Rquest Handler`；
+ Dispatcher：管理`Request Handler`的注册与注销，从多路复用器处获取资源，并调用关联的`Rquest Handler`；
+ Request Handler：包含用户定义的处理流程和它关联的资源；

如下类图参照Pattern In C - Part 5:REACTOR的内容[4]：
```mermaid!
classDiagram

class Handle

class Reactor {
    +Register(*RequestHandler)
    +Unregister(*RequestHandler)
    +HandleEvents()
}

class RequestHandler{
    &lt;&lt;interface&gt;&gt;
    + HandlerEvent()
    + GetHandle() Handle
}

class ContactServerHandler {
    + HandlerEvent()
    + GetHandle() Handle
}

class ContactClientHandler {
    + HandlerEvent()
    + GetHandle() Handle
}


ContactClientHandler ..|&gt; RequestHandler
ContactServerHandler ..|&gt; RequestHandler
Reactor &quot;1&quot; *-- &quot;0..*&quot; RequestHandler: dispatches to 

Reactor ..&gt; Handle
ContactServerHandler ..&gt; Handle
ContactClientHandler ..&gt; Handle
```

上面的`Handle`就是`Resources`，它通常是一个系统资源的标识符（如：socket、file、devices等），可以通过poll、select、epoll 来监听事件（READ、WRITE、EXCEPTION等），基于event驱动的服务模型十分常见而且高效。

在golang中使用的`os.File`、`os.Connection`都是基于事件模型，它们都是基于一个关键结构`poller.FD`的封装。[5]

这里简单介绍一下golang中封装的基于事件的文件描述符操作模型(**以linux版本为主**)：

``` golang
// FD 文件描述符. 在net和os包中引用这个类型
// 用于实现一个网络链接或系统文件类型 
type FD struct {
    // 互斥锁，用于序列化读写方法调用；
    fdmu fdMutex

    // 真正的系统分配的文件描述符，不可变
    Sysfd int

    // I/O poller.
    pd pollDesc

    // Writev cache.
    iovecs *[]syscall.Iovec

    // 文件关闭时接收到的信号
    csema uint32

    // 是否开启了block模式（非0表示开启）.
    isBlocking uint32

    // 是否为一个 streaming 描述符（如tcp），
    // False 表示为一个 packet-based描述符（如udp）
    // 不可变 
    IsStream bool

    // 读取返回零字节表示EOF
    // Flase 表示当前为一个 message-based socket。
    ZeroReadIsEOF bool

    // 当前对应系统资源是否为 File 而不是 网络 socket。
    isFile bool
}

// 初始化FD，此时Sysfd 应该已经被设置了.
// net 参数可以传入一种网络链接协议（如 tcp、udp等）或者 &quot;file&quot;.
// pollable 为 true的时候 fd应该由 runtime netpoll来管理.
func (fd *FD) Init(net string, pollable bool) error {
    // 此处不关心真正的网络协议类型.
    if net == &quot;file&quot; {
        fd.isFile = true
    }
    if !pollable {
        fd.isBlocking = 1
        return nil
    }
    // 初始化 pollDesc，有可能会触发runtime.netpoll初始化
    err := fd.pd.init(fd)
    if err != nil {
        // 如果没办法初始化runtime poller，就使用block模式.
        fd.isBlocking = 1
    }
    return err
}

const maxRW = 1 &lt;&lt; 30

// Read 实现 了io.Reader接口.
func (fd *FD) Read(p []byte) (int, error) {
    if err := fd.readLock(); err != nil {
        return 0, err
    }
    defer fd.readUnlock()
    if len(p) == 0 {
        // 如果传入的read buffer为0此处立即返回
        // 否则 返回 0时， err = nil 等同于 io.EOF
        // TODO(bradfitz): make it wait for readability? (Issue 15735)
        return 0, nil
    }
    // 准备读取
    if err := fd.pd.prepareRead(fd.isFile); err != nil {
        return 0, err
    }
    // 对于 streaming fd，限制读取大小 1GB
    if fd.IsStream &amp;&amp; len(p) &gt; maxRW {
        p = p[:maxRW]
    }
    for {
        // 从 fd中读取数据，忽略EINTR 错误
        n, err := ignoringEINTRIO(syscall.Read, fd.Sysfd, p)
        if err != nil {
            n = 0
            // syscall.EAGAIN 资源暂时无效需要重试，
            // 且pollDesc.runtimeCtx != 0
            if err == syscall.EAGAIN &amp;&amp; fd.pd.pollable() {
                // 等待文件可读
                if err = fd.pd.waitRead(fd.isFile); err == nil {
                    continue
                }
            }
        }
        err = fd.eofError(n, err)
        return n, err
    }
}

type pollDesc struct {
	runtimeCtx uintptr // runtime.pollDesc 指针
}

var serverInit sync.Once

func (pd *pollDesc) init(fd *FD) error {
    // 此处使用了 lazyInitial 的模式 
    serverInit.Do(runtime_pollServerInit)
    // 注册sysfd事件监听到 netpoll
    ctx, errno := runtime_pollOpen(uintptr(fd.Sysfd))
    if errno != 0 {
        if ctx != 0 {
            runtime_pollUnblock(ctx)
            runtime_pollClose(ctx)
        }
        return errnoErr(syscall.Errno(errno))
    }
    pd.runtimeCtx = ctx
    return nil
}

// 准备读取
func (pd *pollDesc) prepareRead(isFile bool) error {
    return pd.prepare('r', isFile)
}

func (pd *pollDesc) prepare(mode int, isFile bool) error {
    if pd.runtimeCtx == 0 {
        return nil
    }
    // 检查并重置当前的runtimeCtx状态
    // 如对应模式是否超时、是否发生错误(主要是针对rw）、
    // 是否正在被关闭
    res := runtime_pollReset(pd.runtimeCtx, mode)
    return convertErr(res, isFile)
}

func (pd *pollDesc) waitRead(isFile bool) error {
    return pd.wait('r', isFile)
}

func (pd *pollDesc) wait(mode int, isFile bool) error {
    if pd.runtimeCtx == 0 {
        return errors.New(&quot;waiting for unsupported file type&quot;)
    }
    res := runtime_pollWait(pd.runtimeCtx, mode)
    return convertErr(res, isFile)
}
```

上述代码是fd的read过程，其依赖的runtime实现如下：

``` golang
// runtime_pollServerInit 的具体实现，位于runtime/netpoll.go
//go:linkname poll_runtime_pollServerInit internal/poll.runtime_pollServerInit
func poll_runtime_pollServerInit() {
    netpollGenericInit()
}

// runtime_pollOpen 的具体实现，位于runtime/netpoll.go
//go:linkname poll_runtime_pollOpen internal/poll.runtime_pollOpen
func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int) {
    pd := pollcache.alloc() // pollcache 是一个单向链表
    lock(&amp;pd.lock)
    // 检查读状态
    if pd.wg != 0 &amp;&amp; pd.wg != pdReady {
        throw(&quot;runtime: blocked write on free polldesc&quot;)
    }
    // 检查写状态
    if pd.rg != 0 &amp;&amp; pd.rg != pdReady {
        throw(&quot;runtime: blocked read on free polldesc&quot;)
    }
    pd.fd = fd
    pd.closing = false
    pd.everr = false
    pd.rseq++
    pd.rg = 0
    pd.rd = 0
    pd.wseq++
    pd.wg = 0
    pd.wd = 0
    pd.self = pd
    unlock(&amp;pd.lock)

    var errno int32
    // 添加事件监听
    errno = netpollopen(fd, pd)
    return pd, int(errno)
}

// runtime_pollReset 的具体实现，位于runtime/netpoll.go 
// 对一个描述符的`r` 或 `w` 模式做准备
//go:linkname poll_runtime_pollReset internal/poll.runtime_pollReset
func poll_runtime_pollReset(pd *pollDesc, mode int) int {
    // 检查当前的状态
    errcode := netpollcheckerr(pd, int32(mode))
    if errcode != pollNoError {
        return errcode
    }
    if mode == 'r' {
        pd.rg = 0
    } else if mode == 'w' {
        pd.wg = 0
    }
    return pollNoError
}

func netpollGenericInit() {
    if atomic.Load(&amp;netpollInited) == 0 {
        lockInit(&amp;netpollInitLock, lockRankNetpollInit)
        lock(&amp;netpollInitLock)
        if netpollInited == 0 {
            // 执行对应操作平台的初始化
            // linux 中是通过 epoll 实现
            // darwin 中是通过 kqueue 实现
            // windows 中是通过 Iocp（完成端口） 实现
            netpollinit() 
            atomic.Store(&amp;netpollInited, 1)
        }
        unlock(&amp;netpollInitLock)
    }
}

// runtime_pollWait, 的具体实现，位于runtime/netpoll.go,
// 根据模式（r 或 w）,等待描述符可读或可写;
//go:linkname poll_runtime_pollWait internal/poll.runtime_pollWait
func poll_runtime_pollWait(pd *pollDesc, mode int) int {
    // 检查当前的状态
    errcode := netpollcheckerr(pd, int32(mode))
    if errcode != pollNoError {
        return errcode
    }
    // 目前只有 Solaris, illumos, and AIX 使用 level-triggered IO.
    if GOOS == &quot;solaris&quot; || GOOS == &quot;illumos&quot; || GOOS == &quot;aix&quot; {
        netpollarm(pd, mode)
    }
    for !netpollblock(pd, int32(mode), false) {
        errcode = netpollcheckerr(pd, int32(mode))
        if errcode != pollNoError {
            return errcode
        }
        // Can happen if timeout has fired and unblocked us,
        // but before we had a chance to run, timeout has been reset.
        // Pretend it has not happened and retry.
    }
    return pollNoError
}

// netpoll 初始化，linux 实现
// 位于runtime/net_epoll.go
func netpollinit() {
    // 创建一个epoll资源，
    // 通过调用 syscall 291 epoll_create1(int flags)
    // 取消子进程继承对该epoll实例的继承
    epfd = epollcreate1(_EPOLL_CLOEXEC)
    if epfd &lt; 0 {
        // 如果创建epoll实例创建不成功，
        // 则 syscall 213 epoll_create(int size)，
        // 从linux 2.6.8 后的内核对于size已经忽略了，
        // 但是传入值必须大于0
        epfd = epollcreate(1024)
        if epfd &lt; 0 {
            println(&quot;runtime: epollcreate failed with&quot;, -epfd)
            throw(&quot;runtime: netpollinit failed&quot;)
        }
        // 取消子进程继承对该epoll实例的继承
        // fcntl(fd, F_SETFD, FD_CLOEXEC)
        closeonexec(epfd)
    }
    // 创建一个非阻塞的读写管道，
    // syscall 293 pipe2
    r, w, errno := nonblockingPipe()
    if errno != 0 {
        println(&quot;runtime: pipe failed with&quot;, -errno)
        throw(&quot;runtime: pipe failed&quot;)
    }
    // 在epoll中注册监听 pipe Read 端的 read event
    ev := epollevent{
        events: _EPOLLIN,
    }
    *(**uintptr)(unsafe.Pointer(&amp;ev.data)) = &amp;netpollBreakRd
    errno = epollctl(epfd, _EPOLL_CTL_ADD, r, &amp;ev)
    if errno != 0 {
        println(&quot;runtime: epollctl failed with&quot;, -errno)
        throw(&quot;runtime: epollctl failed&quot;)
    }
    netpollBreakRd = uintptr(r)
    netpollBreakWr = uintptr(w)
}

func netpollopen(fd uintptr, pd *pollDesc) int32 {
    var ev epollevent
    // 监听 可读、可写、对端关闭连接或者shut down writing helf、
    // 边缘触发通知（默认为 level-triggerd）  
    ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET
    *(**pollDesc)(unsafe.Pointer(&amp;ev.data)) = pd
    // 注册监听事件到 netpoll
    return -epollctl(epfd, _EPOLL_CTL_ADD, int32(fd), &amp;ev)
}

func netpollcheckerr(pd *pollDesc, mode int32) int {
    if pd.closing {
        return pollErrClosing
    }
    // 检查是否读写超时
    if (mode == 'r' &amp;&amp; pd.rd &lt; 0) || (mode == 'w' &amp;&amp; pd.wd &lt; 0) {
        return pollErrTimeout
    }

    // 只有在读模式下会上报一个事件扫描错误
    // 写事件错误会被随后的写入调用获取，
    // 而且会上报更具体的错误
    if mode == 'r' &amp;&amp; pd.everr {
        return pollErrNotPollable
    }
    return pollNoError
}

// 如果IO处于 ready状态则返回 true，如果 timeout或者 closed 则返回false
// waitio - 忽略错误，仅仅等待IO完成
func netpollblock(pd *pollDesc, mode int32, waitio bool) bool {
    gpp := &amp;pd.rg
    if mode == 'w' {
        gpp = &amp;pd.wg
    }

    // 将pd对应的状态设置为 wait
    for {
        old := *gpp
        if old == pdReady {
            *gpp = 0
            return true
        }
        if old != 0 {
            throw(&quot;runtime: double wait&quot;)
        }
        if atomic.Casuintptr(gpp, 0, pdWait) {
            break
        }
    }

    // 需要在将 gpp 设置为 pdWait 后重新检查错误状态，这是必要的，
    // 因为 runtime_pollUnblock/runtime_pollSetDeadline/deadlineimpl 
    // 做相反的事情：写数据到 closing/rd/wd 成员变量，membarrier，加载 rg/wg
    if waitio || netpollcheckerr(pd, mode) == 0 {
        // 挂起 g，并把 g 的地址赋值给 wg 或 rg。
        gopark(netpollblockcommit, unsafe.Pointer(gpp), waitReasonIOWait, traceEvGoBlockNet, 5)
    }
    // be careful to not lose concurrent pdReady notification
    old := atomic.Xchguintptr(gpp, 0)
    if old &gt; pdWait {
        throw(&quot;runtime: corrupted polldesc&quot;)
    }
    return old == pdReady
}
```

下面对`runtime.pollDesc`结构体进行一下简单的阐述：

``` golang
// 位于runtime/netpoll.go 不要与上面的混淆 
// Network poller 的描述.
//go:notinheap
type pollDesc struct {
    link *pollDesc // in pollcache, protected by pollcache.lock

    // The lock protects pollOpen, pollSetDeadline, pollUnblock and deadlineimpl operations.
    // This fully covers seq, rt and wt variables. fd is constant throughout the PollDesc lifetime.
    // pollReset, pollWait, pollWaitCanceled and runtime·netpollready (IO readiness notification)
    // proceed w/o taking the lock. So closing, everr, rg, rd, wg and wd are manipulated
    // in a lock-free way by all operations.
    // NOTE(dvyukov): the following code uses uintptr to store *g (rg/wg),
    // that will blow up when GC starts moving objects.
    lock    mutex // 互斥锁，用于保护下面的字段
    fd      uintptr // 真正的文件描述符(由系统分配)，贯穿整个desc生命周期
    closing bool    // 正在关闭
    everr   bool      // epoll 监听到EPOLLERR事件
    user    uint32    // user settable cookie
    rseq    uintptr   // 防止过时的读定时器
    rg      uintptr   // 当前状态 pdReady, pdWait, 等待读取的 G 或者 nil
    rt      timer     // 读取 deadline 定时器 (set if rt.f != nil)
    rd      int64     // 读取 deadline
    wseq    uintptr   // 防止过时的写定时器
    wg      uintptr   // pdReady, pdWait,  等待写入的 G or nil
    wt      timer     // 写入 deadline 定时器
    wd      int64     // 写入 deadline
    self    *pollDesc // storage for indirect interface. See (*pollDesc).makeArg.
}
```

熟悉`epoll`和`edge-tiggerd`的读者可能更容易理解一些，在`edge-triggerd`下只有在状态发生切换的时候才会触发事件，即由`不可读`变为`可读`、`不可写`变为`可写`，而在数据未完全读取前不会再次触发`可读`事件。
所以这就是为什么在`read`的时候先进行循环读取，如果返回`EAGAIN`（无数据可读）时才执行`gopark`挂起 `g`，等待`可读`事件触发再通过`goready`唤醒 `g`。

```mermaid!
flowchart TD
    Start[start];
    End[end];
    SysRead{syscall read data, OK?};
    ErrAgain{Is EAGAIN?};
    NoError{ error == nil?}
    subgraph prepare[netpoll_prepare]
        pre_start[enter]--&gt;pre_chkerr{netpoller no error?};
        pre_chkerr--&gt;|yes|pre_reset[reset ready state];
        pre_chkerr--&gt;|no|pre_return[return];
        pre_reset--&gt;pre_return;
    end
    subgraph wait[netpoll_wait_ready]
        wait_start[enter]--&gt;wait_chkerr;
        wait_chkerr{netpoller no error?}--&gt;|yes|netpollblock{is ready?};
        wait_chkerr--&gt;|no|wait_return[return];
        netpollblock--&gt;|no|wait_chkerr;
        netpollblock--&gt;|Yes|wait_return;
    end
    Start--&gt;prepare;prepare--&gt;NoError;
    NoError--&gt;|yes|SysRead;NoError--&gt;|no|End;
    SysRead--&gt;|no|ErrAgain;SysRead--&gt;|yes|End;
    ErrAgain--&gt;|yes|wait;ErrAgain--&gt;|no|End;
    wait--&gt;NoError;
```

如上面流程图所示，当`syscall read`返回`EAGAIN`的时候就会进入`netpoll_wait_ready`，有可能会导致挂起，那什么时候会唤醒呢？

``` golang
// netpoll 检查准备好的网络链接.
// 返回可以执行的 g 列表
// delay &lt; 0: 阻塞
// delay == 0: 不阻塞
// delay &gt; 0: 阻塞超时 delay ns
func netpoll(delay int64) gList {
    if epfd == -1 {
        return gList{}
    }
    var waitms int32
    if delay &lt; 0 {
        waitms = -1
    } else if delay == 0 {
        waitms = 0
    } else if delay &lt; 1e6 {
        waitms = 1
    } else if delay &lt; 1e15 {
        waitms = int32(delay / 1e6)
    } else {
        // An arbitrary cap on how long to wait for a timer.
        // 1e9 ms == ~11.5 days.
        waitms = 1e9
    }
    var events [128]epollevent
retry:
    n := epollwait(epfd, &amp;events[0], int32(len(events)), waitms)
    if n &lt; 0 {
        if n != -_EINTR {
            println(&quot;runtime: epollwait on fd&quot;, epfd, &quot;failed with&quot;, -n)
            throw(&quot;runtime: netpoll failed&quot;)
        }
        // 如果 sleep 被中断，立即返回重新统计需要sleep的时间
        if waitms &gt; 0 {
            return gList{}
        }
        goto retry
    }
    var toRun gList
    // 遍历接收的事件
    for i := int32(0); i &lt; n; i++ {
        ev := &amp;events[i]
        if ev.events == 0 {
            continue
        }
        // 过滤netpollBreakRd读事件
        if *(**uintptr)(unsafe.Pointer(&amp;ev.data)) == &amp;netpollBreakRd {
            if ev.events != _EPOLLIN {
                println(&quot;runtime: netpoll: break fd ready for&quot;, ev.events)
                throw(&quot;runtime: netpoll: break fd ready for something unexpected&quot;)
            }
            if delay != 0 {
                // netpollBreak 用于打破poll的阻塞
                // 通过向监听的 netpollBreakWr(pipe 写入端)
                // 写入一byte数据，触发监听事件
                var tmp [16]byte
                read(int32(netpollBreakRd), noescape(unsafe.Pointer(&amp;tmp[0])), int32(len(tmp)))
                atomic.Store(&amp;netpollWakeSig, 0)
            }
            continue
        }

        // 判断事件的模式（写入 或 读取）
        var mode int32
        if ev.events&amp;(_EPOLLIN|_EPOLLRDHUP|_EPOLLHUP|_EPOLLERR) != 0 {
            mode += 'r'
        }
        if ev.events&amp;(_EPOLLOUT|_EPOLLHUP|_EPOLLERR) != 0 {
            mode += 'w'
        }
        if mode != 0 {
            pd := *(**pollDesc)(unsafe.Pointer(&amp;ev.data))
            pd.everr = false
            if ev.events == _EPOLLERR { // 判断是否为错误类型事件
                pd.everr = true
            }
            // 根据对应的模式, 将 rg 或 wg 设置为 pReady模式，
            // 将绑定在上面的 g 添加到 toRun 列表中
            netpollready(&amp;toRun, pd, mode)
        }
    }
    return toRun
}
```
可以看到完全是依赖于系统提供的`epoll`能力，那`netpoll`在何时会被调用？主要在以下情况：

+ startTheWorld：启动世界，用于解除stopWorld的效果，此时会从`poll network` 处拉取所有可以运行的`goroutine`分配给处理器去运行；
+ findrunnable：查找一个可运行的`goroutine`去执行，尝试从其他P的本地队列或全局队列窃取，或者从`poll network 处去获取；
+ gcDrain：在执行一个idle模式的标记任务时，会有限检查当前是否有其他可执行的任务，此时会从 `poll network` 拉取所有可以运行的`goroutine`分配给处理器去运行；
+ sysmon：wasm 上还没有线程，所以没有 sysmon，为此启动一个 m 去执行 sysmon （循环调用不会停止），里面会从 `poll network`拉取任务；

上面对 `golang`的`network poll`读取和初始化流程做了一个简要的分析，它与`Rector`并发模式有些相似，实现了`Synchronous Event Demultiplexer`，将`fd`使用`runtime.PollDesc`进行封装，基于事件决定挂起和恢复对应的`g`。主要差别在于没有将响应事件的处理方法和`Resources`封装到一起。


### Thread-local storage

线程本地存储模式：略

## 参考链接

[1] mermaid. Class diagrams. April 28 2021, https://mermaid-js.github.io/mermaid/#/classDiagram

[2] wikipedia. Software design pattern. 18 February 2021,, https://en.wikipedia.org/wiki/Software_design_pattern

[3] wikipedia. Test-driven development. https://en.wikipedia.org/wiki/Test-driven_development

[4] Adam Petersen. Pattern In C - Part 5:REACTOR. 14 Jun 2021, https://www.adamtornhill.com/Patterns%20in%20C%205,%20REACTOR.pdf

[5] golang. poller.FD. 1 Oct 2020, https://github.com/golang/go/blob/go1.16.2/src/internal/poll/fd_unix.go#L17</content><author><name>Zhipeng Wang</name><email>wangzhipenghyc@163.com</email></author><summary type="html"></summary></entry><entry><title type="html">Computer_boundary</title><link href="http://localhost:4000/computer_boundary/" rel="alternate" type="text/html" title="Computer_boundary" /><published>2021-04-18T00:00:00+08:00</published><updated>2021-04-18T00:00:00+08:00</updated><id>http://localhost:4000/computer_boundary</id><content type="html" xml:base="http://localhost:4000/computer_boundary/">&lt;style&gt;
  @import url(&quot;/assets/css/simple.css&quot;)
&lt;/style&gt;

# Computer boundary

自从第三次工业革命后，信息时代的到来使得计算机成为了人们生活中不可或缺的生产工具，从硬件性能上可以划分为以下几种：超大型计算机（超算，多用于大型科研项目进行海量的计算，其主要侧重于计算尤其是浮点数计算）、大型计算机（主机、服务器，多用于公司生产，运行线上服务和数据库，有强大的IO处理能力）、微型计算机（台式机、笔记本、手机，多用于日常办公和娱乐）。单片机也可以划分为微型计算机，它属于一种用于工业控制的特种计算机。

虽然它们的用途不同，但底层的架构和原理都是相同的，都遵循着（广义上的）冯.诺依曼体系结构，运行的过程都符合图灵机的定义。所以从某种意义上来讲，它们拥有相同的能力边界。有些问题用微型计算机解决不了，同样用大型计算机也解决不了。在使用一种工具、一款软件甚至某项学科时，如果充分了解其边界能力，就可以避免我们做很多无用的尝试。突破边界让人兴奋，可从效率的角度上来讲，事先划定好边界更能提高生产。

## Math &amp; CS

数学（math）属于形式科学，是研究数量、结构、变化、空间以及信息等概念的一门学科，它被人们称之为自然科学的皇后。计算机科学（computer sceince，缩写cs）与数学是紧密相关，当开发者在使用计算机去解决一个问题的时候一定会去寻找对应的数学模型，其背后一定符合某种数学原理。即便在与数学很“遥远”的业务开发也是如此，代码中无数的 `if` 和 `else` 就是对数学中逻辑学的简单应用。

### 数学不是万能的

在人们的生产生活中存在着无数个问题，如果把世界上所有问题看作一个集合 S，数学能够覆盖和描述的只是其中一小部分。有很多问题都不属于可计算的问题，例如：为何已经曝光的电信诈骗手法，还是会有人不断的上当受骗？为何同样的礼物送给女友，前一次高兴后一次却生气？

以上的例子给出的界限都比较模糊，而早在1930年的时候，哥德尔就证明了部分数学公理（蕴含皮亚诺公理体系[2]）不可能既是完备的，又是一致的，即：公理范围内存在命题 `P` 为真，但无法证明的情况。哥德尔不完备定理的提出，让人们意识到数学的方法并不是万能的，下面是哥德尔不完备定理的内容：

```
    TheoremVI: For every ω-consistent primitive recursive class κ of formulae, there is aprimitive recursive class-sign r , such that neither forall(v,r) nornot(forall(v,r)) belongs to Conseq(κ) (where v is the free variable of r).
    定理VI：对于任意一个ω一致的原始递归公理集合κ，一定存在一个原始递归的表达式r，使得无论是“r总成立”这个命题，还是“r不总成立”这个命题，都不属于通过κ可推导出来的定理的集合（原文中的Conseq(κ)）。
```

希尔伯特把赋予形式化公理体系以含义后形成的数学称为“元数学”（Metamathematics，被称之为数学的数学）[3]，因此哥德尔自己把论文中这个章节的标题叫做“Expressingmetamathematical concepts”，表达元数学概念，翻译成“有含义概念的表达”[1]。

一个经典语义悖论是说谎者悖论：“我说的每一句话都是谎话”，哥德尔的证明与这个语义悖论类似，他在严格遵守元数学体系的情况下，基于 蕴含皮亚诺公理体系的 PM（Principia Mathematica） 公理体系[12]，列出了 45 个表达式，然后基于这些表达式，构造出了一个 `P` 不可以证明，则 `P` 为真的命题。

以下展示其中几个表达式：

1. y|x表示y能够整除x 
```
    y|x ⇔ ∃z≤x*x=y*z
```

2. isPrime(x)判断x是否为素数
```
    isPrime(x) ⇔ ~(∃z≤x * (z≠1∧z≠x∧z|x)∧ x&gt;1)
```

3. prFactor(n,x)表示x这个整数所包含的第n个（按大小排列）素因子
```
    prFactor(0,x)= 0；
    prFactor(n+1,x) = argmin y≤x * (isPrime(y)∧y|x∧y&gt;prFactor(n,x))
```

其最终构造出的命题 `P` 如下：
```
    p=forall(17, q)（式5）

    r=subst(q,19,number(p))（式6）
```

这个定理对于cs的影响也是巨大的，因为很容易就可以推导出：**基于数学的计算机不是万能的**。一个问题如果从数学层面无法解决，那么使用计算机也是无能为力的。因此我们应该关注于那些可计算的，属于数学范畴的问题。

+ ps1: 皮亚诺公里体系定义了自然数集合和运算规则，皮亚诺本人也是符号逻辑学开创者之一。
+ ps2: 在公理自身范围内不可证明不代表在其他公理范围内不可证明，只要扩充额外的公理，但额外扩充的公理也会存在同样的问题，最终导致公理的无限拓展。
+ ps3: 哥德尔定理中有严格的前提条件，及公理蕴含皮亚诺算数公理体系。
+ ps4: 公理的一致性：对于任何可在一个公理体系内描述的命题，不能同时即为真又为假。[4]
+ ps5: 公理的完备性：对于任何可在一个公理体系内描述的命题，都可以在这个公理体系内得到判定。[4]

### 数学问题不是都有解的

在数学中有一种常见的问题格式：已知 f、y ，给定 y=f(x)，求 x 在 N 范围内是否有解？请写出答案和推导过程。这里面蕴含着一个道理，我们去求解一个问题先要判断这个问题是不是有解，然后再去给出最终的答案。有大量的数学问题是无解的，而且是占多数的。

判定问题`Q`是否有解，可以转化为一个命题：`Pq` = 问题 `Q` 有解，只要论证命题 `Pq` 为真就好了。通过上面哥德尔不完备定理我们可以清楚，数学当中存在着一些命题`Pq`，在某些公理体系下它既不能证明也不能证伪。从而我们可以推导出**存在一些问题 `Q` 无法判定它是否有解**，这样的问题是真实存在的。

1900年，希伯尔特在国际数学大会上提出了23个著名的数学问题，其中第十个问题如下：
```
    任意一个（多项式）不定方程，能否通过有限步的运算，判定它是否存在整数解。
```

不定方程也被称之为丢番图方程（Diophantine equation），就是指有两个或更多的未知数方程，它们的解有无数多个，下面举三个示例：

+ 例一：x^2 + y^2 = z^2 ，每一个直角三角形的三边都是一组解。
+ 例二：x^N + y^N = z^N ，其中 N&gt;2；这些方程都没有正整数解，这就是著名的费马大定理。
+ 例三：x^3 + 5*y^3 = 4*z^3，我们难以直观的判断此方程是否有正整数解，甚至我们都没有办法在有限步骤内判断它是否有解。

马蒂亚塞维奇定理(也可以称之为DPRM或MRDP定理，Davis-Putnam-Robinson-Matiyasevich)对希伯尔特第十个问题给出了一个否定的证明，他严格证明了除了极少数特例外，在一般情况下无法通过有限步的运算，来判断一个不定方程是否存在整数解。其定理详细内容如下[6]：

```
存在一个非负整数的集合 M，当且仅当 a∈M 时，存在一个具有整数系数的多项式P（a，x1，...，xm），使得one-parameter Diophantine 方程 P（a，x1，...，xm）= 0 成立。

该集合 M 被称为 Diophantine，以下表达式: 
    a∈M⟺∃x1…xm {P（a，x1，…，xm）= 0} 
被称为集合 M 的 Diophantine 表示。

该自然数集 M 一定是可有效枚举或可有效列出的，前提是存在一种算法，该算法可能无限长地工作，将以某种顺序输出 M 中的所有元素，并且仅输出集合中的元素[9]。每个Diophantine集都是可列出的，反之亦成立：每个可列出的集合都是 Diophantine。

该定理的证明是建设性的：如果一个可列出的集合 M 可以用（任何）标准方式表示，则存在对应的多项式 P 作为 M 的 Diophantine表示。
```

下面给出几个 Diophantine 示例来展示一下马蒂亚塞维奇定理：

```
    Diophatine equation F： 
    a - 2x = 0，该方程定义了一个非零偶数集合 
    list : a = 2n (n ∈ N*)
    
    a−x^2=0，该方程定义了一个所有整数的平方集合
    list : a = n^2 (n ∈ N*)

    (x^2−a*x−a^2)^2=1，该方程定义了一个斐波那契数列集合 
    list: Fibonacci sequence：F(0)=0，F(1)=1, F(n)=F(n - 1)+F(n - 2)（n ≥ 2，n ∈ N*）

    list 表示列出公式。
```

Diophantine和可列出集的定义可以自然的推广到非负整数的对，三元组，...，n元组的集合，并且该定理很容易扩展到以下情况：

```
    ⟨a1,…,an⟩∈M⟺∃x1…xm{P(a1,…,an,x1,…,xm)=0}
```

上式表示每个n元非负整数的可列出集合，其中 P(a1,…,an,x1,…,xm)带有整数系数。

马丁.戴维斯(迈出对定理证明第一步的数学家)表示，他的猜想是（可能）证明希尔伯特第十个问题的不确定性，而这要归功于一个基本事实：**存在不可确定的可列出集合**。如果 M 是不确定的可列出集合，而 a∈ M ⟺∃x1…xm {P（a，x1，…，xm）= 0}是其 Diophantine 表示，则人们得到以下希尔伯特第十个问题的不确定性的强有力证明：对于给定的参数a值，没有算法可以来确定Diophantine equation P（a，x1，…，xm）= 0是否具有整数解。

该定理通过给出了一个不可证明的陈述（相当于 *特定 Diophantine equation 没有解* 的断言），从而实现对哥德尔不完备定理的补全，该证明后来成为 确立某些决策问题不可判定性 的有力工具。

ps6: 马蒂亚塞维奇定理的确立不仅仅是其一个人的功劳，它是建立在多位数学家成果之上的，其中包括马丁·戴维斯（Martin Davis）、希拉里·普特南（Hilary Putnam）和茱莉亚·罗宾逊（Julia Robinson），尤其是朱莉-逻宾逊，她的论文给予马蒂亚塞维奇重大的启发，使得马蒂亚塞维齐能完成证明的最后一步[5]。

最后我们作出总结：许多数学问题是无解的，此外还有一些数学问题是无法有效判断是否有解的。对于那些可以明确是否有解的问题，本文称之为**可判定问题**，至此可以得出一下集合关系：

![问题集合关系](/assets/images/math_boundary_1.svg)

### 图灵的思考：计算机的能力边界

图灵在希伯尔特第十问题被给出否定答案之前，就凭直觉意识到有很多数学问题不是可判定问题，这一点他也是受到了哥德尔不完备定理的影响和启发。因此图灵在设计图灵机数学模型时，没有像那些沉迷于“构造一个大一统的公理体系”的数学家一样，企图构造一个能解决一切数学问题的机器，而是提前的预设好了边界：**专注于那些可以在有限步骤内解决的问题（本文称之为有答案的问题），即使不能解决也可以在有限步骤内证明其无解**。

图灵想构造一种机器（图灵机）：给定一个可计算的问题，输入计算模型和初始条件，机器开始运转，当机器停止运转的时候输出结果。对于*可计算问题集* 等于 还是 包含于 *有答案问题集*？这一点还存在争议，丘奇-图灵论题（Church-Turing Thesis）是对可计算问题的描述（后面会讲到），从理论上讲当一个问题有解且有明确公式，则图灵机可以求解该问题。

这样一看**可计算的问题**就是计算机能力的边界，然而现实世界又对其加上了新的制约--硬件的资源是有限的，一个公式求解的 *有限步骤*（可以理解为算法的时间复杂度） 可能对应的是几百上千年的计算，例如我们常常提到的 NP 难问题：大素数因式分解（RSA非对称加密的基础）。该问题从理论上讲是可以求解的，但实际上目前还无法制造出可以短时间内（人类最长寿命）破解 RSA-2084bit 的计算机，有些人寄希望于量子计算机，因为理论上来讲它具有极强的并行计算能力（依赖于量子叠加状态的物理特性），但目前来看量子计算机还是遥遥无期。另外需要指出的是：即便是量子计算机，只要它遵循图灵机的数学模型，仍然打不破马蒂亚塞维奇定理划下的边界。

对于这种时间复杂度巨大的问题，在工程上来讲是很难实现的，有生之年都无法看到最终结果，而百年之后求解出来真实秘钥又有什么意义呢？只有在当前硬件条件下可以实现解决方案的才是有意义的。本文以解决实际问题的角度出发，从上至下以递进关系阐述了计算机的边界，到此为止可以用下图来概括：

![计算机能力边界](/assets/images/math_boundary_2.svg)

## 图灵机（Turing Machine）

图灵机是一个数学模型，是一个强大的逻辑机器，从理论上来讲，任何一个能给出算法的问题都可以用图灵机计算。那么**可计算的**评价标准是什么？“可计算”这个词是模糊不清的，多位数学家对“可计算性”进行研究，其中就包含上文我们提到了“丘奇-图灵论题”[10]，其内容如下：

```
A method, or procedure, M, for achieving some desired result is called ‘effective’ (or ‘systematic’ or ‘mechanical’) just in case:

1. M is set out in terms of a finite number of exact instructions (each instruction being expressed by means of a finite number of symbols);
2. M will, if carried out without error, produce the desired result in a finite number of steps;
3. M can (in practice or in principle) be carried out by a human being unaided by any machinery except paper and pencil;
4. M demands no insight, intuition, or ingenuity, on the part of the human being carrying out the method.

在如下的情况下，一个包含确定结果的方法或过程 M 可以称之为 “有效的”（也可称之为机械方法执行的）：
1. M以有限数量的精确指令（每个指令通过有限数量的符号表示）表示； 
2. 如果执行无误，M将在有限的步骤中产生期望的结果；
3. 任何人执行M只需要纸和笔，无需其他机器辅助（实际上或原则上）；
4. 对于执行该方法的人员，M不需要任何洞察力，直觉或独创性。
```
此类方法 `M` 的一个范例便是用于确定两个自然数的最大公约数的欧基里德算法。

丘奇和图灵做了同一件事情，即：如何陈述`M`，即如何定义 `M`，他提出了通过λ演算的方法来定义函数，实际上与图灵定义了同一类函数，即两个陈述是等价的。但是其中的前提假设--“能够有效计算”是一个模糊的定义。因此，虽然这个假说已接近完全，但仍然不能由公式证明，所以称之为论题。

简单来说，邱奇-图灵论题认为“任何在算法上可计算的问题同样可由图灵机计算”。

### 图灵机模型

图灵机的核心思想就是用机器模拟人类计算的过程（仅仅使用纸和笔），如果我们对人做数学题的过程进行简化，就发现这个过程其实是不断的重复两个动作：
1. 在纸上写下或擦除一些符号；
2. 不断移动笔在纸上的书写位置。

为了模拟人的这种运算过程，图灵构造出一台假想的机器，该机器由以下几个部分组成：

一条无限长的纸带TAPE。纸带被划分为一个接一个的小格子，每个格子上包含一个来自有限字母表的符号，字母表中有一个特殊的符号来表示空白。纸带上的格子从左到右依次被编号为0, 1, 2, ...，纸带的右端可以无限伸展。
一个读写头HEAD。该读写头可以在纸带上左右移动，它能读出当前所指的格子上的符号，并能改变当前格子上的符号。

一套控制规则TABLE。它根据当前机器所处的状态以及当前读写头所指的格子上的符号来确定读写头下一步的动作，并改变状态寄存器的值，令机器进入一个新的状态，按照以下顺序告知图灵机命令：

1. 写入（替换）或擦除当前符号；
2. 移动 HEAD， 'L'向左， 'R'向右或者'N'不移动；
3. 保持当前状态或者转到另一状态；
4. 一个状态寄存器。它用来保存图灵机当前所处的状态。图灵机的所有可能状态的数目是有限的，并且有一个特殊的状态，称为停机状态。参见停机问题。

![图灵机示意图 a](/assets/images/Turing_machine_2a.svg)

![图灵机示意图 b](/assets/images/Turing_machine_2b.svg)

注意这个机器的每一部分都是有限的，但它有一个潜在的无限长的纸带，因此这种机器只是一个理想的设备。图灵认为这样的一台机器就能模拟人类所能进行的任何计算过程。


### 图灵机的数学表达

一台图灵机是一个七元有序组 `(Q, A, I, b, q0, F, t)`，其中 `Q、A、I、F、t` 都是有限集合，且满足：
+ Q: 非空有限状态集合；
+ A: 非空有限字母表（不包含特殊的空白符号）；
+ I: 非空有限输入字母表，且 I 包含于 A；
+ b: 空白符号，唯一可以无限出现的符号；
+ q0: 起始状态，且 `q0` 属于 `Q`；
+ F: 终止状态集合，其中包含多种状态（成功或失败）；
+ t: 状态转移函数，它是 `{ Q/F.A}` 到 `{Q/F.{Left, Right}}` 的映射，其中 `Q/F` 表示不包含终止状态的状态集合，`Left` 和 `Right` 表示指针的左右合法移动。[11]


图灵机的执行过程如下：

1. 开始的时候将输入符号串 `W = w0w1...w(n-1)` 从左到右依此填在纸带的第 `0, 1... , n-1` 号格子上，其他格子保持空白（即填以空白符 `b` )；
2. `M` 的读写头指向第0号格子， `M` 处于状态 `q0`；
3. 机器开始运行后，按照转移函数 `t` 所描述的规则进行计算。

例如，若当前机器的状态为 `q`，读写头所指的格子中的符号为 `x`，t(q,x)=(q',x',L)，则机器进入新状态 `q'`，将读写头所指的格子中的符号改为 `x'`，然后将读写头向左移动一个格子。若在某一时刻，读写头所指的是第0号格子，但根据转移函数它下一步将继续向左移，这时它停在原地不动。

如果 `q'` 属于 `F`，则停止计算，成功则返回结果，失败则返回失败状态。如果运行到某一步骤时，发生没有定义转移函数的情况（因为 `t` 是一个有限集合 ），无法继续运算，立即停机。

## 小结

本文实际上是对《数学之美（第3版）》中 34 章节 -- 数学的极限的复述，在吴军博士的基础上进行了一些整理和引申。同时吴军博士在文章中提到了一个观点：**AI 的能力是有限的，只能解决世界上一小部分问题**，因为其受到数学和计算机能力的制约。

我们无需担心 AI 过于强大，但也不必对 AI 过于悲观。AI 将来的前景还是光明的，即便是在 AI 没有类人意识（所谓强AI）的前提下，它必然可以在某些岗位取代人类。就像人类不需要扇动翅膀也能在天空飞翔，AI 也不需要完全理解人类的思维，也可以作出符合正常人类逻辑的行为。

## 参考文献

[1] 赵昊彤(2017) http://blog.sciencenet.cn/blog-409681-1067025.html 哥德尔不完备定理到底说了什么?（四）

[2] peano(1889) 《用一种新方法陈述的算术原理》皮亚诺公理

[3] wiki https://wikimili.com/en/Metamathematics 元数学

[4] 赵昊彤(2017) http://blog.sciencenet.cn/blog-409681-1067019.html 哥德尔不完备定理到底说了什么?（一）

[5] Jiapu Zhang(2019) http://blog.sciencenet.cn/home.php?mod=space&amp;uid=498408&amp;do=blog&amp;id=1202676 希尔伯特第十问题: 一段数学发现史

[6] http://www.scholarpedia.org/article/Matiyasevich_theorem/Examples_of_Diophantine_sets 马蒂亚塞维奇定理

[7] http://www.scholarpedia.org/article/Matiyasevich_theorem/Examples_of_Diophantine_sets 丢番图集合

[8] https://en.wikipedia.org/wiki/Diophantine_equation 丢番图方程

[9] http://www.scholarpedia.org/article/Matiyasevich_theorem/Examples_of_listable_sets 可列出事例

[10] https://plato.stanford.edu/entries/church-turing/ 丘奇-图灵论题

[11] https://zh.wikipedia.org/wiki/%E5%9B%BE%E7%81%B5%E6%9C%BA 图灵机

[12] 赵昊彤(2017) http://blog.sciencenet.cn/blog-409681-1067021.html 哥德尔不完备定理到底说了什么?（三）</content><author><name>Zhipeng Wang</name><email>wangzhipenghyc@163.com</email></author><summary type="html"></summary></entry></feed>